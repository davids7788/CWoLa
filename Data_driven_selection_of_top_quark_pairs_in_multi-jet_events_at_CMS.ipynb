{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data driven selection of top quark pairs in multi-jet events at CMS\n",
    "\n",
    "## This is a complete description of my Master Thesis work. \n",
    "1. Preselection\n",
    "2. Input Features and normalization <br>\n",
    "    1.1 Input feature distribution <br>\n",
    "    1.2 Normalization of data <br>\n",
    "3. Building the Neural Network\n",
    "4. Tests on MC simulated events <br>\n",
    "    4.1 MC Simulation $t\\bar{t}$ vs. QCD  <br>\n",
    "    4.2 MC Simulation $t\\bar{t}$ vs. QCD CWoLa <br>\n",
    "5. CWoLa on data\n",
    "6. Background estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preselection\n",
    "\n",
    "This Preselection is done to increase and imbalance signal fractions in mixed samples $M_1$ and $M_2$, which will be introduced in the CWoLa section. The preselection cutflow is displayed below. One event may consist of up to 90 different assignments of the 6 leading jets. Here, only the first 6 jets are used, ordered via $P_{GoF}$, a p-value, derived from the kinematic fit. The $\\chi^2$ value, calculated with the kinematic fit is displayed below the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/cut_flow.png)\n",
    "![title](img/kinematic_fit_chi_squared.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Input Features and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One assignment of jets is described through 27 parameters in this thesis. The parameters are listed in the following. For better readability the description along with their corresponding place in the parameter list is stored in a dictionary:\n",
    "\n",
    "0: $P_T$ of b-jet 1 <br>\n",
    "1: $P_T$ of b-jet 2 <br>\n",
    "2: $P_T$ of product 1 of W boson 1 <br>\n",
    "3: $P_T$ of product 2 of W boson 1 <br>\n",
    "4: $P_T$ of product 1 of W boson 2 <br>\n",
    "5: $P_T$ of product 2 of W boson 2 <br>\n",
    "6: $\\Delta R_{b\\bar{b}}$ <br>\n",
    "7: b-tag value of b-jet 1 <br>\n",
    "8: b-tag value of b-jet 2 <br>\n",
    "9: b-tag value of product 1 of W boson 1 <br>\n",
    "10: b-tag value of product 2 of W boson 1 <br>\n",
    "11: b-tag value of product 1 of W boson 2 <br>\n",
    "12: b-tag value of product 2 of W boson 2 <br>\n",
    "13: reconstructed mass of W boson 1: $m_{W1}^{reco}$ <br>\n",
    "14: reconstructed mass of W boson 2: $m_{W2}^{reco}$ <br>\n",
    "15: fitted top mass: $m_t^{fit}$ <br>\n",
    "16: p-value obtained through $\\chi^2$ from the kinematik fit $P_{GoF}$ <br>\n",
    "17: combination type <br>\n",
    "18: decay channel <br>\n",
    "19: Hadronic Activity $H_T$ <br>\n",
    "20: $p_T$ of the 6th jet <br>\n",
    "21: # of b-assigned jets <br>\n",
    "22: lumiblock <br>\n",
    "23: run number <br>\n",
    "24: event number <br>\n",
    "25: combined weight <br>\n",
    "26: combined weight * trigger efficiency correction @dissertation Johannes Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_feature_dictionary = {'top.recoB1.Pt': 0,\n",
    "                               'top.recoB2.Pt': 1,\n",
    "                               'top.recoW1Prod1.Pt': 2,\n",
    "                               'top.recoW1Prod2.Pt': 3,\n",
    "                               'top.recoW2Prod1.Pt': 4,\n",
    "                               'top.recoW2Prod2.Pt': 5,\n",
    "                               'Delta_Rbb': 6,\n",
    "                               'jet.bTag_B1': 7,\n",
    "                               'jet.bTag_B2': 8,\n",
    "                               'jet.bTag_W1P1': 9,\n",
    "                               'jet.bTag_W1P2': 10,\n",
    "                               'jet.bTag_W2P1': 11,\n",
    "                               'jet.bTag_W2P2': 12,\n",
    "                               'top.recoW1.M': 13,\n",
    "                               'top.recoW2.M': 14,\n",
    "                               'top.fitTop1.M': 15,\n",
    "                               'top.fitProb': 16,\n",
    "                               'top.combinationType': 17,\n",
    "                               'top.decayChannel': 18,\n",
    "                               'jet.HT': 19,\n",
    "                               'jet.jet[5].Pt': 20,\n",
    "                               'n_bjets': 21,\n",
    "                               'top.lumiblock': 22,\n",
    "                               'top.run': 23,\n",
    "                               'top.event': 24,\n",
    "                               'weight.combinedWeight': 25,\n",
    "                               'combinedWeight_and_trigger_efficiency_correction': 26}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Input features\n",
    "\n",
    "Only few of this features are used for this project. Some of them are just used to verify the correctness of this analysis, like the unique combination of run number, event number and lumiblock, or for separating data like the assigned b-tag values and of course the weights of the events itself. \n",
    "\n",
    "The input featers which are used for the analysis are:\n",
    "\n",
    "0: $P_T$ of b-jet 1 <br>\n",
    "1: $P_T$ of b-jet 2 <br>\n",
    "2: $P_T$ of product 1 of W boson 1 <br>\n",
    "3: $P_T$ of product 2 of W boson 1 <br>\n",
    "4: $P_T$ of product 1 of W boson 2 <br>\n",
    "5: $P_T$ of product 2 of W boson 2 <br>\n",
    "13: reconstructed mass of W boson 1: $m_{W1}^{reco}$ <br>\n",
    "14: reconstructed mass of W boson 2: $m_{W2}^{reco}$ <br>\n",
    "15: fitted top mass: $m_t^{fit}$ <br>\n",
    "16: p-value obtained through $\\chi^2$ from the kinematik fit $P_{GoF}$ <br>\n",
    "19: Hadronic Activity $H_T$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = ['top.recoB1.Pt',\n",
    "                     'top.recoB2.Pt',\n",
    "                     'top.recoW1Prod1.Pt',\n",
    "                     'top.recoW1Prod2.Pt',\n",
    "                     'top.recoW2Prod1.Pt',\n",
    "                     'top.recoW2Prod2.Pt',\n",
    "                     'top.recoW1.M',\n",
    "                     'top.recoW2.M',\n",
    "                     'top.fitTop1.M',\n",
    "                     'top.fitProb',\n",
    "                     'jet.HT']\n",
    "\n",
    "features_of_interest =['top.recoB1.Pt',\n",
    "                       'top.recoB2.Pt',\n",
    "                       'top.recoW1Prod1.Pt',\n",
    "                       'top.recoW1Prod2.Pt',\n",
    "                       'top.recoW2Prod1.Pt',\n",
    "                       'top.recoW2Prod2.Pt',\n",
    "                       'Delta_Rbb',\n",
    "                       'top.recoW1.M',\n",
    "                       'top.recoW2.M',\n",
    "                       'top.fitTop1.M',\n",
    "                       'top.fitProb',\n",
    "                       'jet.HT',\n",
    "                       'jet.jet[5].Pt']\n",
    "\n",
    "x_label_dictionary = {'top.recoB1.Pt': '$p_T^{recoB1}$ [GeV]',\n",
    "                      'top.recoB2.Pt': '$p_T^{recoB2}$ [GeV]',\n",
    "                      'top.recoW1Prod1.Pt': '$p_T^{recoW1Prod1}$ [GeV]',\n",
    "                      'top.recoW1Prod2.Pt': '$p_T^{recoW1Prod2}$ [GeV]',\n",
    "                      'top.recoW2Prod1.Pt': '$p_T^{recoW2Prod1}$ [GeV]',\n",
    "                      'top.recoW2Prod2.Pt': '$p_T^{recoW2Prod2}$ [GeV]',\n",
    "                      'Delta_Rbb': r'$\\Delta R_{b\\bar{b}}$',\n",
    "                      'top.recoW1.M': '$m_{W1}^{reco}$ [GeV]',\n",
    "                      'top.recoW2.M': '$m_{W2}^{reco}$ [GeV]',\n",
    "                      'top.fitTop1.M': '$m_t^{fit}$ [GeV]',\n",
    "                      'top.fitProb': '$P_{GoF}$',\n",
    "                      'jet.HT': '$H_T$ [GeV]',\n",
    "                      'jet.jet[5].Pt': '$p_T^{jet[5]}$ [GeV]'}\n",
    "\n",
    "# dictionary for values where most of the data lies, mostly normalized data\n",
    "range_dict_input_features = {'top.recoB1.Pt': (45, 300),\n",
    "                             'top.recoB2.Pt': (45, 200),\n",
    "                             'top.recoW1Prod1.Pt': (45, 300),\n",
    "                             'top.recoW1Prod2.Pt': (45, 140),\n",
    "                             'top.recoW2Prod1.Pt': (45, 400),\n",
    "                             'top.recoW2Prod2.Pt': (45, 200),\n",
    "                             'Delta_Rbb': (2, 4),\n",
    "                             'top.recoW1.M': (60, 120),\n",
    "                             'top.recoW2.M': (60, 120),\n",
    "                             'top.fitTop1.M': (100, 300),\n",
    "                             'top.fitProb': (0, 1),\n",
    "                             'jet.HT': (450, 1250),\n",
    "                             'jet.jet[5].Pt': (45, 140)}\n",
    "\n",
    "\n",
    "# number of different bins \n",
    "BIN_NUMBER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "# workspace folder\n",
    "workspace_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and divide into signal background and mixed samples M1 and M2, needs about 4 GB to process these lines, because the \n",
    "# data is in the form of [Event1, Event2, ...] with Event = [Permutation1, Permutation2, ...] and has to be converted into lists,\n",
    "# another approach could be realized if one uses inices and only original files, but since the used memory is not too large for \n",
    "# most computers and computing clusters it's ok at this point I guess... \n",
    "\n",
    "\n",
    "CMS_data_non_flattened = np.load('Thesis_data_after_preselection/CMS_1_b_tag_after_preselection.npy', \n",
    "                                 allow_pickle=True, \n",
    "                                 encoding='latin1')\n",
    "CMS_background_estimation_non_flattened = np.load('Thesis_data_after_preselection/CMS_0_b_tag_after_preselection.npy', \n",
    "                                                  allow_pickle=True, \n",
    "                                                  encoding='latin1')\n",
    "\n",
    "TTbar_non_flattened = np.load('Thesis_data_after_preselection/TTbar_1_b_tag_after_preselection.npy', \n",
    "                              allow_pickle=True, \n",
    "                              encoding='latin1') \n",
    "\n",
    "QCD_non_flattened = np.load('Thesis_data_after_preselection/QCD_1_b_tag_after_preselection.npy', \n",
    "                            allow_pickle=True, \n",
    "                            encoding='latin1')\n",
    "QCD_background_estimation_non_flattened = np.load('Thesis_data_after_preselection/QCD_0_b_tag_after_preselection.npy', \n",
    "                                                  allow_pickle=True, \n",
    "                                                  encoding='latin1')\n",
    "\n",
    "\n",
    "# MC luminosity factor\n",
    "MC_lum = 35.866203\n",
    "\n",
    "# preparing data \n",
    "CMS = []\n",
    "CMS_1_btag = []\n",
    "CMS_2_btag = []\n",
    "\n",
    "TTbar = []\n",
    "TTbar_1_btag = []\n",
    "TTbar_2_btag = []\n",
    "\n",
    "QCD = []\n",
    "QCD_1_btag = []\n",
    "QCD_2_btag = []\n",
    "\n",
    "MC_Simulation_1_btag = []\n",
    "MC_Simulation_2_btag = []\n",
    "\n",
    "# CMS\n",
    "for event in CMS_data_non_flattened:\n",
    "    for permutation in event:\n",
    "        if permutation[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "            CMS_2_btag.append(permutation)\n",
    "        else:\n",
    "            CMS_1_btag.append(permutation)\n",
    "        CMS.append(permutation)\n",
    "            \n",
    "# TTbar\n",
    "for event in TTbar_non_flattened:\n",
    "    for permutation in event:\n",
    "        if permutation[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "            MC_Simulation_2_btag.append(permutation)\n",
    "            TTbar_2_btag.append(permutation)\n",
    "        else:\n",
    "            MC_Simulation_1_btag.append(permutation)\n",
    "            TTbar_1_btag.append(permutation)\n",
    "        TTbar.append(permutation)\n",
    "\n",
    "# QCD\n",
    "for event in QCD_non_flattened:\n",
    "    for permutation in event:\n",
    "        if permutation[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "            MC_Simulation_2_btag.append(permutation)\n",
    "            QCD_2_btag.append(permutation)\n",
    "        else:\n",
    "            MC_Simulation_1_btag.append(permutation)\n",
    "            QCD_1_btag.append(permutation)\n",
    "        QCD.append(permutation)\n",
    "        \n",
    "\n",
    "# Converting lists into numpy arrays\n",
    "CMS_1_btag = np.array(CMS_1_btag)\n",
    "CMS_2_btag = np.array(CMS_2_btag)\n",
    "CMS = np.array(CMS)\n",
    "\n",
    "TTbar = np.array(TTbar)\n",
    "TTbar_1_btag = np.array(TTbar_1_btag)\n",
    "TTbar_2_btag = np.array(TTbar_2_btag)\n",
    "\n",
    "QCD = np.array(QCD)\n",
    "QCD_1_btag = np.array(QCD_1_btag)\n",
    "QCD_2_btag = np.array(QCD_2_btag)\n",
    "\n",
    "MC_Simulation_1_btag = np.array(MC_Simulation_1_btag)\n",
    "MC_Simulation_2_btag = np.array(MC_Simulation_2_btag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the input features are displayed, divided into 1 and 2 b-tag events and $t\\bar{t}$ signal and QCD background. A folder is created and the figures are saved into it, if the folder doesn't already exist. Otherwise this step is skipped. Additionally the input distributions are displayed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# navigate to workspace folder and create new folder if it not existing already\n",
    "os.chdir(workspace_folder)\n",
    "\n",
    "if 'Input distributions' in os.listdir():\n",
    "    pass\n",
    "else:\n",
    "    new_folder = os.mkdir('Input distributions')\n",
    "os.chdir('Input distributions')\n",
    "\n",
    "\n",
    "# compare number of events\n",
    "print('Sum events CMS data: ' \n",
    "      + str(sum(CMS_2_btag[:, complete_feature_dictionary['weight.combinedWeight']])\n",
    "            + sum(CMS_1_btag[:, complete_feature_dictionary['weight.combinedWeight']])))\n",
    "print('Sum events MC simulation: ' \n",
    "      + str(np.around(MC_lum * (sum(TTbar[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']]) \n",
    "                                + sum(QCD[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']])),\n",
    "                      decimals=0)))\n",
    "\n",
    "\n",
    "# comparing CMS and MC simulated events for every chosen feature\n",
    "for plot_number, feature in enumerate(features_of_interest):\n",
    "\n",
    "    fig1 = plt.figure(plot_number)\n",
    "    frame1 = fig1.add_axes((.15, .3, .8, .6))\n",
    "    \n",
    "    n_cms_1b, bins_cms_1b = \\\n",
    "        np.histogram(CMS_1_btag[:, complete_feature_dictionary[feature]],\n",
    "                     bins=BIN_NUMBER,\n",
    "                     range=range_dict_input_features[feature])\n",
    "    n_cms_2b, bins_cms_2b = \\\n",
    "        np.histogram(CMS_2_btag[:, complete_feature_dictionary[feature]],\n",
    "                     bins=BIN_NUMBER,\n",
    "                     range=range_dict_input_features[feature])\n",
    "    \n",
    "    n, bins, patches = \\\n",
    "        frame1.hist([TTbar_2_btag[:, complete_feature_dictionary[feature]],\n",
    "                     QCD_2_btag[:, complete_feature_dictionary[feature]],\n",
    "                     TTbar_1_btag[:, complete_feature_dictionary[feature]],\n",
    "                     QCD_1_btag[:, complete_feature_dictionary[feature]]],\n",
    "                    weights=[TTbar_2_btag[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']] \n",
    "                             * MC_lum,\n",
    "                             QCD_2_btag[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']] \n",
    "                             * MC_lum,\n",
    "                             TTbar_1_btag[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']] \n",
    "                             * MC_lum,\n",
    "                             QCD_1_btag[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']] \n",
    "                             * MC_lum],\n",
    "                    bins=BIN_NUMBER,\n",
    "                    stacked=True,\n",
    "                    alpha=1,\n",
    "                    edgecolor='k',\n",
    "                    range=(range_dict_input_features[feature]),\n",
    "                    color=['lightsteelblue', 'cornflowerblue', 'seagreen', 'darkgreen'],\n",
    "                    label= ['_nolegend_', '_nolegend_', '_nolegend_', '_nolegend_'])\n",
    "\n",
    "    frame1.plot(bins_cms_1b[0:-1],\n",
    "                n_cms_2b,\n",
    "                color='k',\n",
    "                marker='o',\n",
    "                markersize=2,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='')\n",
    "    frame1.plot(bins_cms_1b[0:-1],\n",
    "                n_cms_2b + n_cms_1b,\n",
    "                color='r',\n",
    "                marker='s',\n",
    "                markersize=2,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='')\n",
    "\n",
    "    frame1.set_xticklabels([])  # Remove x-tic labels from the first frame\n",
    "\n",
    "    x_label_string = x_label_dictionary[feature]\n",
    "    y_label = np.around(((range_dict_input_features[feature][1] - range_dict_input_features[feature][0]) /\n",
    "                         BIN_NUMBER), decimals=2)\n",
    "    bin_shift_for_plotting = y_label / 2                                 # schifts residual plots in the\n",
    "\n",
    "    if '[GeV]' in x_label_dictionary[feature]:             # GeV values on x axis need a unit\n",
    "        y_label_string = r'Events / ' + str(y_label) + ' GeV'\n",
    "    else:\n",
    "        y_label_string = r'Events / ' + str(y_label)\n",
    "\n",
    "    plt.title(r'private work'\n",
    "              '                          '\n",
    "              r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "    frame1.set_ylabel(y_label_string, fontsize=13)\n",
    "\n",
    "    if feature == 'top.fitProb':                                          # here log scale, because > 95 % of data\n",
    "        frame1.set_yscale('log')                                          # have P_GoF < 0.1\n",
    "\n",
    "    frame1.legend({r'2 b-tag data': 'k',\n",
    "                   r'1 b-tag data': 'r',\n",
    "                   r'2 b-tag $t\\bar{t}$': 'lightsteelblue',\n",
    "                   r'2 b-tag multijet': 'cornflowerblue',\n",
    "                   r'1 b-tag $t\\bar{t}$': 'seagreen',\n",
    "                   r'1 b-tag multijet': 'darkgreen'},\n",
    "                  fontsize=9,\n",
    "                  loc='best')\n",
    "\n",
    "    frame2 = fig1.add_axes((.15, .11, .8, .175))\n",
    "\n",
    "    # carefully addressing stacked histogram\n",
    "    normalized_ttbar_complete = (n[2] - n[1] + n[0]) / sum(n[2] - n[1] + n[0])\n",
    "    normalized_qcd_complete = (n[3] - n[2] + n[1] - n[0]) / sum(n[3] - n[2] + n[1] - n[0])\n",
    "\n",
    "    normalized_1_btag = (n[3] - n[1]) / sum(n[3] - n[1])\n",
    "    normalized_2_btag = n[1] / sum(n[1])\n",
    "\n",
    "    normalized_ttbar_1_btag = (n[2] - n[1]) / sum(n[2] - n[1])\n",
    "    normalized_ttbar_2_btag = n[0] / sum(n[0])\n",
    "\n",
    "    normalized_qcd_1_tag = (n[3] - n[2]) / sum(n[3] - n[2])\n",
    "    normalized_qcd_2_tag = (n[1] - n[0]) / sum(n[1] - n[0])\n",
    "\n",
    "    normalized_ttbar_qcd_ratio_complete = normalized_ttbar_complete / normalized_qcd_complete\n",
    "    normalized_ttbar_qcd_ratio_1_btag = normalized_ttbar_1_btag / normalized_qcd_1_tag\n",
    "    normalized_ttbar_qcd_ratio_2_btag = normalized_ttbar_2_btag / normalized_qcd_2_tag\n",
    "    normalized_2_btag_1_b_tag_ratio = normalized_2_btag / normalized_1_btag\n",
    "\n",
    "    frame2.plot((bins + bin_shift_for_plotting)[0:-1],\n",
    "                normalized_ttbar_qcd_ratio_complete,\n",
    "                color='k',\n",
    "                marker='x',\n",
    "                markersize=3,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='',\n",
    "                label=r'$t\\bar{t}$ / QCD complete')\n",
    "    frame2.plot((bins + bin_shift_for_plotting)[0:-1],\n",
    "                normalized_2_btag_1_b_tag_ratio,\n",
    "                color='g',\n",
    "                marker='o',\n",
    "                markersize=3,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='',\n",
    "                label='2 b-tags / 1 b-tags')\n",
    "    frame2.plot((bins + bin_shift_for_plotting)[0:-1],\n",
    "                normalized_ttbar_qcd_ratio_1_btag,\n",
    "                color='r',\n",
    "                marker='^',\n",
    "                markersize=3,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='',\n",
    "                label=r'$t\\bar{t}$ / QCD 1 b-tag')\n",
    "    frame2.plot((bins + bin_shift_for_plotting)[0:-1],\n",
    "                normalized_ttbar_qcd_ratio_2_btag,\n",
    "                color='b',\n",
    "                marker='v',\n",
    "                markersize=3,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='',\n",
    "                label=r'$t\\bar{t}$ / QCD 2 b-tags')\n",
    "    \n",
    "    if feature == 'top.fitProb':                                          # here log scale, because > 95 % of data\n",
    "        frame2.set_yscale('log') \n",
    "    frame2.legend(loc='right', fontsize=9)\n",
    "    frame2.set_xlabel(x_label_string, fontsize=9)\n",
    "    plt.savefig('Input_distribution_' + feature.replace('.', '_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features interesting for the CWoLa approach should have the green dots in the ratio plots around the value 1,\n",
    "while the other markers should differ from that. This would mean, that a separation in terms of signal ($t\\bar{t}$) and background (QCD) is possible, while no discrimination for the number of b-tags per example is possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2 Normalization\n",
    "\n",
    "The Normalization is done on $H_T$, the hadronic activity of an event with $H_T = \\sum_{i=0}^{5}p_T^{jet[i]}$. Each value, if \n",
    "measured in $p_T$, is divided by the $H_T$ value, while the $H_T$ value itself is just divided by a factor of 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing input data\n",
    "def normalize_on_ht(data_set, dictionary, feature_list):\n",
    "    \"\"\"This function takes a data set, a dictionary describing the properties of the data and a feature list\n",
    "    and returns a normalization of values given in GeV in parts of HT.\n",
    "    :param data_set: flattened numpy array\n",
    "    :param dictionary: dictionary describing the properties of the values inside the data set\n",
    "    :param feature_list: list of features which are normalized with respect to ht value\n",
    "    :return returns the on ht normalized data set\"\"\"\n",
    "    data_set_return = np.copy(data_set)\n",
    "    for feature in feature_list:\n",
    "        if feature != dictionary['top.fitProb'] and feature != dictionary['jet.HT']:\n",
    "            data_set_return[:, dictionary[feature]] = data_set_return[:, dictionary[feature]] / data_set_return[:, dictionary['jet.HT']]\n",
    "    data_set_return[:, dictionary['jet.HT']] = data_set_return[:, dictionary['jet.HT']] / 5000\n",
    "    return data_set_return\n",
    "\n",
    "\n",
    "# Converting lists into numpy arrays\n",
    "CMS_1_btag = normalize_on_ht(CMS_1_btag, complete_feature_dictionary, training_features)\n",
    "CMS_2_btag = normalize_on_ht(CMS_2_btag, complete_feature_dictionary, training_features)\n",
    "CMS = normalize_on_ht(CMS, complete_feature_dictionary, training_features)\n",
    "\n",
    "TTbar = normalize_on_ht(TTbar, complete_feature_dictionary, training_features)\n",
    "TTbar_1_btag = normalize_on_ht(TTbar_1_btag, complete_feature_dictionary, training_features)\n",
    "TTbar_2_btag = normalize_on_ht(TTbar_2_btag, complete_feature_dictionary, training_features)\n",
    "\n",
    "QCD = normalize_on_ht(QCD, complete_feature_dictionary, training_features)\n",
    "QCD_1_btag = normalize_on_ht(QCD_1_btag, complete_feature_dictionary, training_features)\n",
    "QCD_2_btag = normalize_on_ht(QCD_2_btag, complete_feature_dictionary, training_features)\n",
    "\n",
    "MC_Simulation_1_btag = normalize_on_ht(MC_Simulation_1_btag, complete_feature_dictionary, training_features)\n",
    "MC_Simulation_2_btag = normalize_on_ht(MC_Simulation_2_btag, complete_feature_dictionary, training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Buidling the Neural Network\n",
    "\n",
    "Pease note, that this is a very basic implementation. The following code box can be replaced with a more sophisticated model, but keras gives an easy way to create a very basic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Using adam optimizer with the following parameters:\n",
      "learning rate: 0.001 \n",
      "decay: 0.0005 \n",
      "beta1: 0.6 \n",
      "beta2: 0.85 \n",
      "amsgrad: True\n",
      "############################################################ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer,\n",
    "                 nodes_list=(16, 32, 64, 32, 16, 8),\n",
    "                 batch_normalization=True,\n",
    "                 activation_function='selu',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 bias_initializer='glorot_normal',\n",
    "                 batch_size=1024,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics='accuracy'):\n",
    "\n",
    "    \"\"\" creates a sequential model and writes a file with some of the training history\n",
    "    :param nodes_list list of nodes for each dense layer, has to be the size of num_dense_layers\n",
    "    :param batch_normalization if True batch normalization is used after each layer\n",
    "    :param activation_function the used activation function for all but the last layer\n",
    "    :param kernel_initializer kernel initializer, default 'he_uniform' like the one in the CWoLa paper\n",
    "    :param bias_initializer bias initializer, default 'he_uniform' like the one in the CWoLa paper\n",
    "    :param batch_size size of the used batches for training\n",
    "    :param optimizer self build optimizer or adam with default values\n",
    "    :param loss function of the model\n",
    "    :param metrics tuple of metrics\n",
    "    :return created model\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    for i in range(len(nodes_list)):\n",
    "        model.add(Dense(nodes_list[i],\n",
    "                        activation=activation_function,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        bias_initializer=bias_initializer,\n",
    "                        batch_size=batch_size))\n",
    "        if batch_normalization and i != len(nodes_list) - 1:      # Never add batch normalization before softmax layer\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# specifying parameters for optimizer\n",
    "LR = 1e-3\n",
    "EPOCHS = 2 # default = 50\n",
    "DECAY = LR / EPOCHS\n",
    "BETA_1 = 0.6\n",
    "BETA_2 = 0.85\n",
    "AMSGRAD = True\n",
    "\n",
    "# Preparing k-fold cross validation\n",
    "K_FOLDING = 3 # default = 10\n",
    "\n",
    "adam = optimizers.Adam(lr=LR, decay=DECAY, beta_1=BETA_1, beta_2=BETA_2, amsgrad=AMSGRAD)\n",
    "\n",
    "print('############################################################')\n",
    "print('Using adam optimizer with the following parameters:\\n' +\n",
    "      'learning rate:', LR, '\\n'\n",
    "      'decay:', DECAY,  '\\n'\n",
    "      'beta1:', BETA_1, '\\n'\n",
    "      'beta2:', BETA_2, '\\n'\n",
    "      'amsgrad:', AMSGRAD)\n",
    "print('############################################################', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two useful functions for scaling weights and for implementing a k fold cross validataion with \n",
    "# respect to the weighting parameter of the samples\n",
    "def scaling_weights(data, weight_position, number_normed_events=1e6):\n",
    "    \"\"\" Normalizes input data, so weights of background and signal add up to the same value\n",
    "    :param data data which has to be scaled\n",
    "    :param weight_position position of the weight parameter\n",
    "    :param number_normed_events re-weighting to 1 Million events per sample\n",
    "    :return returns reweighted sample\"\"\"\n",
    "\n",
    "    data_copy = np.copy(data)\n",
    "    scaling_s = number_normed_events / sum(data[:, weight_position])\n",
    "\n",
    "    data_copy[:, weight_position] = data[:, weight_position] * scaling_s\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "def k_fold_cross_validation(k_folding, x_s, x_bg, weight_position):\n",
    "    \"\"\"Divides the data into k_folding equal weighted parts\n",
    "     :param k_folding number of different parts\n",
    "     :param x_s signal data\n",
    "     :param x_bg background data\n",
    "     :param weight_position -1 for MC and -2 for CMS data\n",
    "     :return list of 2 times k_folding fractions\"\"\"\n",
    "    np.random.shuffle(x_s)\n",
    "    np.random.shuffle(x_bg)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    j = 0\n",
    "    k = 0\n",
    "\n",
    "    j_index = 0\n",
    "    k_index = 0\n",
    "\n",
    "    x_s_sum = sum(x_s[:, weight_position])\n",
    "    x_bg_sum = sum(x_bg[:, weight_position])\n",
    "\n",
    "    for k_times in range(1, k_folding + 1):\n",
    "        x_sample = []\n",
    "        y_sample = []\n",
    "        while j < k_times * 1 / k_folding * x_s_sum:\n",
    "            x_sample.append(x_s[j_index, :])\n",
    "            y_sample.append(1)\n",
    "            j += x_s[j_index, weight_position]\n",
    "            j_index += 1\n",
    "        while k < k_times * 1 / k_folding * x_bg_sum:\n",
    "            x_sample.append(x_bg[k_index, :])\n",
    "            y_sample.append(0)\n",
    "            k += x_bg[k_index, weight_position]\n",
    "            k_index += 1\n",
    "\n",
    "        randomize = np.arange(len(y_sample))\n",
    "        np.random.shuffle(randomize)\n",
    "        x_sample = np.array(x_sample)[randomize, :]\n",
    "        y_sample = np.array(y_sample)[randomize]\n",
    "\n",
    "        result.append([x_sample, y_sample])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tests on MC simulated events\n",
    "\n",
    "Before using the CWoLa approach on data, one has to verify if the Neural Network configuration along with \n",
    "the MC simulated samples to get an idea how good the classification might be. The code for the training is not optimzed\n",
    "for runtime or for using gpu, but for good readability and to understand whats going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1  MC Simulation $t\\bar{t}$ vs. QCD\n",
    "\n",
    "First the training and evulation is done on MC simualted events. This will be the benchmark on which the CWoLa training will be measured in terms of AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reweighting samples to match equal weighting\n",
    "x_signal_normalized_reweighted = \\\n",
    "    scaling_weights(TTbar,\n",
    "                    complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "x_background_normalized_reweighted = \\\n",
    "    scaling_weights(QCD,\n",
    "                    complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "\n",
    "# define model output location over date string\n",
    "date_string = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "os.chdir(workspace_folder)\n",
    "os.mkdir('MC_Model_' + date_string)\n",
    "os.chdir('MC_Model_' + date_string)\n",
    "os.mkdir('Model_history')\n",
    "\n",
    "# Take a self written k-folding algorithm, because\n",
    "tenfold_split_data = k_fold_cross_validation(K_FOLDING,\n",
    "                                             x_signal_normalized_reweighted,\n",
    "                                             x_background_normalized_reweighted,\n",
    "                                             complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'])\n",
    "\n",
    "\n",
    "for i in range(K_FOLDING):\n",
    "    print('Run k = ', i, '\\n')\n",
    "    X_tr = []\n",
    "    y_tr = []\n",
    "    for k_fold in range(K_FOLDING):\n",
    "        if k_fold != i:\n",
    "            for event, label in zip(tenfold_split_data[k_fold][0], tenfold_split_data[k_fold][1]):\n",
    "                X_tr.append(event)\n",
    "                y_tr.append(label)\n",
    "\n",
    "    X_tr = np.array(X_tr)\n",
    "    y_tr = np.array(y_tr)\n",
    "\n",
    "    X_val = np.array(tenfold_split_data[i][0])\n",
    "    y_val = np.array(tenfold_split_data[i][1])\n",
    "\n",
    "    file_path_val_loss = 'Run_' + str(i) + '_weights_best_val_loss.h5'\n",
    "    file_path_val_accuracy = 'Run_' + str(i) + '_weights_best_val_accuracy.h5'\n",
    "    \n",
    "    classifier = create_model(adam,\n",
    "                          nodes_list=(16, 32, 64, 32, 16, 8),\n",
    "                          batch_normalization=True,\n",
    "                          activation_function='selu',\n",
    "                          kernel_initializer='glorot_normal',\n",
    "                          bias_initializer='glorot_normal',\n",
    "                          batch_size=1024,\n",
    "                          loss='sparse_categorical_crossentropy',\n",
    "                          metrics='accuracy')\n",
    "    \n",
    "    # checking for lowest val loss and accuracy\n",
    "    checkpoint_val_loss = \\\n",
    "        ModelCheckpoint(file_path_val_loss,\n",
    "                        monitor='val_loss',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode='min')\n",
    "\n",
    "    checkpoint_val_accuracy = \\\n",
    "        ModelCheckpoint(file_path_val_accuracy,\n",
    "                        monitor='val_accuracy',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode='max')\n",
    "\n",
    "    callbacks_list = [checkpoint_val_loss, checkpoint_val_accuracy]\n",
    "    model_history = classifier.fit(X_tr[:, np.array([complete_feature_dictionary[f] \n",
    "                                                     for f in training_features])],\n",
    "                                   y_tr,\n",
    "                                   sample_weight=X_tr[:, complete_feature_dictionary[\n",
    "                                       'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                                   epochs=EPOCHS,\n",
    "                                   verbose=2,\n",
    "                                   validation_data=(X_val[:, np.array([complete_feature_dictionary[f] \n",
    "                                                                       for f in training_features])],\n",
    "                                                    y_val,\n",
    "                                                    X_val[:, complete_feature_dictionary[\n",
    "                                                        'combinedWeight_and_trigger_efficiency_correction']]),\n",
    "                                   callbacks=callbacks_list,\n",
    "                                   shuffle=True)\n",
    "\n",
    "    model_json = classifier.to_json()\n",
    "    with open('model.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    classifier.save_weights('after_last_epoch.h5')\n",
    "    \n",
    "    del classifier\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(i)\n",
    "    plt.plot(model_history.history['accuracy'])\n",
    "    plt.plot(model_history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.savefig('Model_history/Model accuracy_' + str(i))\n",
    "    plt.close(i)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(i + K_FOLDING)\n",
    "    plt.plot(model_history.history['loss'])\n",
    "    plt.plot(model_history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.savefig('Model_history/Model loss_' + str(i))\n",
    "    plt.close(i + K_FOLDING)\n",
    "    \n",
    "\n",
    "# sometimes there are issues with the garbage collector \n",
    "del x_signal_normalized_reweighted\n",
    "del x_background_normalized_reweighted\n",
    "del tenfold_split_data\n",
    "del classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC value\n",
    "Now The AUC value with the standard deviation derived from the k-fold cross validation outcomes is calculated.\n",
    "    In this case, the AUC value is not calculated for a validation set, but for the whole data set. This is in fact not \"good practice\", but we later want to compare this result to the CWoLa trained classifier where only data is used t for the training process. Therefore one can use the whole data set to compare results, especially regarding unbalanced weights of the MC simulated QCD events, which might mess up the evaulation greatly if there aren't many other events to balance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([TTbar, QCD])\n",
    "y = np.concatenate([np.ones(len(TTbar)), np.zeros(len(QCD))])\n",
    "\n",
    "tprs = []\n",
    "tprs_btag = []\n",
    "aucs = []\n",
    "aucs_btag = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_fpr_btag = np.linspace(0, 1, 100)\n",
    "\n",
    "# load json and create model\n",
    "os.chdir(workspace_folder)\n",
    "os.chdir('MC_Model_' + date_string)\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into model\n",
    "for i in range(K_FOLDING):\n",
    "    loaded_model.load_weights('Run_' + str(i) + '_weights_best_val_loss.h5')\n",
    "    print('Loaded model ' + str(i) + ' from disk')\n",
    "\n",
    "    predictions = loaded_model.predict(X[:, np.array([complete_feature_dictionary[f] \n",
    "                                                      for f in training_features])])\n",
    "    fpr, tpr, thresholds = roc_curve(y,\n",
    "                                     predictions[:, 1],\n",
    "                                     sample_weight=X[:, complete_feature_dictionary[\n",
    "                                         'combinedWeight_and_trigger_efficiency_correction']])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_val = auc(mean_fpr, mean_tpr)\n",
    "std_auc_val = np.std(aucs_btag)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'MC $t\\bar{t}$ vs. QCD  (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_val, std_auc_val),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "\n",
    "std_tpr_val = np.std(tprs, axis=0)\n",
    "tprs_upper_val = np.minimum(mean_tpr + std_tpr_val, 1)\n",
    "tprs_lower_val = np.maximum(mean_tpr - std_tpr_val, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower_val, tprs_upper_val, color='grey', alpha=.5,\n",
    "                 label='')\n",
    "\n",
    "\n",
    "plt.xlim([0, 1.0])\n",
    "plt.ylim([0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10-fold cross validation ROC-Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_Curve')\n",
    "\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2  MC Simulation $t\\bar{t}$ vs. QCD CWoLa\n",
    "\n",
    "The data is now separated according to the next figure. The estimated signal fractions are of 10.3% for 1 b-tag samples and 26.9% for 2 b-tag samples. For pairs of fractions $f_1$ and $f_2$ $\\in$ [0.0, 0.40] in steps of 0.25 a training is done and the AUC value is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/MC_CWoLa_separation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data for mixed samples using different fractions\n",
    "def creating_training_data(signal_data,\n",
    "                           background_data,\n",
    "                           f1,\n",
    "                           f2,\n",
    "                           number_of_events,\n",
    "                           lum=MC_lum):\n",
    "    \"\"\" Takes signal and background data and creates training, test and validation samples. The ratio between training\n",
    "    validation and signal samples is always 8:1:1.\n",
    "    :param signal_data data with signal events\n",
    "    :param background_data data with background events\n",
    "    :param f1 signal fraction of training signal sample, has to be greater than f2\n",
    "    :param f2 signal fraction of training background sample\n",
    "    :param number_of_events number of events per returned sample\n",
    "    :param lum luminosity\n",
    "    :return returns training data x,y with fractions\"\"\"\n",
    "\n",
    "    shuffle_signal = np.arange(len(signal_data))\n",
    "    shuffle_background = np.arange(len(background_data))\n",
    "\n",
    "    np.random.shuffle(shuffle_signal)\n",
    "    np.random.shuffle(shuffle_background)\n",
    "\n",
    "    signal_data = signal_data[shuffle_signal]\n",
    "    background_data = background_data[shuffle_background]\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "    signal_index = 0\n",
    "    background_index = 0\n",
    "\n",
    "    # Sample 1\n",
    "    sample_one_counter = 0\n",
    "\n",
    "    while sample_one_counter < f1 * number_of_events:\n",
    "        train_x.append(signal_data[signal_index, :])\n",
    "        train_y.append(1)\n",
    "        signal_index += 1\n",
    "        sample_one_counter += signal_data[signal_index, -1] * lum\n",
    "\n",
    "    while sample_one_counter < number_of_events:\n",
    "        train_x.append(background_data[background_index, :])\n",
    "        train_y.append(1)\n",
    "        background_index += 1\n",
    "        sample_one_counter += background_data[background_index, -1] * lum\n",
    "\n",
    "    # Sample 2\n",
    "    sample_two_counter = 0\n",
    "\n",
    "    while sample_two_counter < f2 * number_of_events:\n",
    "        train_x.append(signal_data[signal_index, :])\n",
    "        train_y.append(0)\n",
    "        signal_index += 1\n",
    "        sample_two_counter += signal_data[signal_index, -1] * lum\n",
    "\n",
    "    while sample_two_counter < number_of_events:\n",
    "        train_x.append(background_data[background_index, :])\n",
    "        train_y.append(0)\n",
    "        background_index += 1\n",
    "        sample_two_counter += background_data[background_index, -1] * lum\n",
    "\n",
    "    # shuffle again. just to make sure there will be no order\n",
    "    shuffle_array_training = np.arange(np.size(train_x, axis=0))\n",
    "    np.random.shuffle(shuffle_array_training)\n",
    "\n",
    "    shuffled_array_training_x = np.array(train_x)[shuffle_array_training]\n",
    "    shuffled_array_training_y = np.array(train_y)[shuffle_array_training]\n",
    "\n",
    "    # print('Weight sum of the training data = ', sum(np.array(shuffled_array_training_x)[:, -1]) * lum)\n",
    "\n",
    "    return shuffled_array_training_x, shuffled_array_training_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting AUC 2D calculation\n",
    "os.chdir(workspace_folder)\n",
    "\n",
    "if 'MC_2D_f1_f2_combinations' in os.listdir():\n",
    "    pass\n",
    "else:\n",
    "    new_folder = os.mkdir('MC_2D_f1_f2_combinations')\n",
    "os.chdir('MC_2D_f1_f2_combinations')\n",
    "\n",
    "x_signal_normalized_reweighted = \\\n",
    "    scaling_weights(MC_Simulation_2_btag,\n",
    "                    complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "x_background_normalized_reweighted = \\\n",
    "    scaling_weights(MC_Simulation_1_btag,\n",
    "                    complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "x_val = np.concatenate([scaling_weights(TTbar,\n",
    "                                        complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                                        number_normed_events=1e6), \n",
    "                        scaling_weights(QCD,\n",
    "                                        complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                                        number_normed_events=1e6)])\n",
    "y_val = np.concatenate([np.ones(len(TTbar)), np.zeros(len(QCD))])\n",
    "\n",
    "\n",
    "N = 400000\n",
    "f = [0.4, 0.375, 0.35, 0.325, 0.3, 0.275, 0.25, 0.225, 0.2, 0.175, 0.15, 0.125, 0.1, 0.075, 0.05, 0.025, 0]\n",
    "\n",
    "\n",
    "auc_array = np.zeros((len(f), len(f)))\n",
    "for i, f1 in enumerate(f):\n",
    "    for k, f2 in enumerate(f):\n",
    "        if f2 > f1:\n",
    "            continue\n",
    "        else:\n",
    "            print('f1 = ' + str(f1) + ', f2 = ' + str(f2))\n",
    "\n",
    "            X_tr, y_tr = creating_training_data(x_signal_normalized_reweighted,\n",
    "                                                x_background_normalized_reweighted,\n",
    "                                                f1,\n",
    "                                                f2,\n",
    "                                                N,\n",
    "                                                MC_lum)\n",
    "\n",
    "            classifier = create_model(adam,\n",
    "                                      nodes_list=(16, 32, 64, 32, 16, 8),\n",
    "                                      batch_normalization=True,\n",
    "                                      activation_function='selu',\n",
    "                                      kernel_initializer='glorot_normal',\n",
    "                                      bias_initializer='glorot_normal',\n",
    "                                      batch_size=1024,\n",
    "                                      loss='sparse_categorical_crossentropy',\n",
    "                                      metrics='accuracy')\n",
    "\n",
    "            filepath_val_loss = 'Run_' + str(i) + '_' + str(k) + '_weights_best_val_loss.h5'\n",
    "            # filepath_val_accuracy = 'Run_' + str(i) + '_' + str(k) + '_weights_best_val_accuracy.h5'\n",
    "\n",
    "            checkpoint_val_loss = \\\n",
    "                ModelCheckpoint(filepath_val_loss, monitor='val_loss', verbose=1, save_weights_only=True,\n",
    "                                save_best_only=True, mode='min')\n",
    "\n",
    "            model_history = classifier.fit(X_tr[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])],\n",
    "                                           y_tr,\n",
    "                                           sample_weight=X_tr[:, complete_feature_dictionary[\n",
    "                                                                 'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                                           epochs=2,\n",
    "                                           verbose=2,\n",
    "                                           validation_data=(x_val[:, np.array([complete_feature_dictionary[f] \n",
    "                                                                              for f in training_features])],\n",
    "                                                            y_val,\n",
    "                                                            x_val[:, complete_feature_dictionary[\n",
    "                                                                'combinedWeight_and_trigger_efficiency_correction']]),\n",
    "                                           callbacks=[checkpoint_val_loss],\n",
    "                                           shuffle=True)\n",
    "\n",
    "            # ROC Curves\n",
    "            classifier.load_weights('Run_' + str(i) + '_' + str(k) + '_weights_best_val_loss.h5')\n",
    "            predictions = classifier.predict(x_val[:, np.array([complete_feature_dictionary[f] \n",
    "                                                                       for f in training_features])])\n",
    "            fpr, tpr, thresholds = roc_curve(y_val,\n",
    "                                             predictions[:, 1],\n",
    "                                             sample_weight=x_val[:, complete_feature_dictionary[\n",
    "                                                                    'combinedWeight_and_trigger_efficiency_correction']])\n",
    "\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            if roc_auc > 0.5:\n",
    "                auc_array[i, k] = roc_auc\n",
    "                auc_array[k, i] = roc_auc\n",
    "            else:\n",
    "                auc_array[i, k] = 1 - roc_auc\n",
    "                auc_array[k, i] = 1 - roc_auc\n",
    "\n",
    "            backend.clear_session()\n",
    "            del classifier\n",
    "\n",
    "print(auc_array)\n",
    "np.save('auc_array_N_' + str(N), np.array(auc_array))\n",
    "\n",
    "x, y = f, f\n",
    "\n",
    "label_1vs2_btag = 'AUC value\\n1 vs. 2 b-tag'\n",
    "xx, yy = np.meshgrid(f, f)\n",
    "h = plt.pcolormesh(x, y, auc_array)\n",
    "plt.plot(0.103, 0.269, 'xr', markersize=12, mew=4)\n",
    "plt.xlabel('signal fraction $f_1$', fontsize=13)\n",
    "plt.ylabel('signal fraction $f_2$', fontsize=13)\n",
    "plt.legend({label_1vs2_btag: 'rx'}, fontsize=13)\n",
    "plt.colorbar()\n",
    "plt.savefig('AUC_2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CWoLa on data\n",
    "\n",
    "## 5.1 Training and evaluation\n",
    "\n",
    "Now the time has come to use this approach on data and evaluate on MC simualted events and data simultaneously. \n",
    "The modified procedure is shown in teh following figure.\n",
    "\n",
    "![title](img/CMS_data_CWoLa_separation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for training\n",
    "x_signal_normed_reweighted = \\\n",
    "    scaling_weights(CMS_2_btag,\n",
    "                    complete_feature_dictionary['weight.combinedWeight'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "x_background_normed_reweighted = \\\n",
    "    scaling_weights(CMS_1_btag,\n",
    "                    complete_feature_dictionary['weight.combinedWeight'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "\n",
    "# define model output location over date string\n",
    "try:\n",
    "    os.mkdir('CMS_Model_' + date_string)\n",
    "except:\n",
    "    pass\n",
    "os.chdir('CMS_Model_' + date_string)\n",
    "os.mkdir('Model_history')\n",
    "\n",
    "\n",
    "tenfold_split_data = k_fold_cross_validation(K_FOLDING,\n",
    "                                             x_signal_normed_reweighted,\n",
    "                                             x_background_normed_reweighted,\n",
    "                                             complete_feature_dictionary['weight.combinedWeight'])\n",
    "\n",
    "\n",
    "for i in range(K_FOLDING):\n",
    "    print('Run k = ', i, '\\n')\n",
    "    classifier = create_model(adam,\n",
    "                              nodes_list=(16, 32, 64, 32, 16, 8),\n",
    "                              batch_normalization=True,\n",
    "                              activation_function='selu',\n",
    "                              kernel_initializer='glorot_normal',\n",
    "                              bias_initializer='glorot_normal',\n",
    "                              batch_size=1024,\n",
    "                              loss='sparse_categorical_crossentropy',\n",
    "                              metrics='accuracy')\n",
    "    X_tr = []\n",
    "    y_tr = []\n",
    "    for k_fold in range(K_FOLDING):\n",
    "        if k_fold != i:\n",
    "            for event, label in zip(tenfold_split_data[k_fold][0], tenfold_split_data[k_fold][1]):\n",
    "                X_tr.append(event)\n",
    "                y_tr.append(label)\n",
    "\n",
    "    X_tr = np.array(X_tr)\n",
    "    y_tr = np.array(y_tr)\n",
    "\n",
    "    X_val = np.array(tenfold_split_data[i][0])\n",
    "    y_val = np.array(tenfold_split_data[i][1])\n",
    "\n",
    "    file_path_val_loss = 'Run_' + str(i) + '_weights_best_val_loss.h5'\n",
    "    file_path_val_accuracy = 'Run_' + str(i) + '_weights_best_val_accuracy.h5'\n",
    "\n",
    "    checkpoint_val_loss = \\\n",
    "        ModelCheckpoint(file_path_val_loss,\n",
    "                        monitor='val_loss',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        save_weights_only=True,\n",
    "                        mode='min')\n",
    "\n",
    "    checkpoint_val_accuracy = \\\n",
    "        ModelCheckpoint(file_path_val_accuracy,\n",
    "                        monitor='val_accuracy',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        save_weights_only=True,\n",
    "                        mode='max')\n",
    "\n",
    "    callbacks_list = [checkpoint_val_loss, checkpoint_val_accuracy]\n",
    "    model_history = classifier.fit(X_tr[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])],\n",
    "                                   y_tr,\n",
    "                                   sample_weight=X_tr[:, complete_feature_dictionary['weight.combinedWeight']],\n",
    "                                   epochs=EPOCHS,\n",
    "                                   verbose=2,\n",
    "                                   validation_data=(X_val[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])],\n",
    "                                                    y_val,\n",
    "                                                    X_val[:, complete_feature_dictionary['weight.combinedWeight']]),\n",
    "                                   callbacks=callbacks_list,\n",
    "                                   shuffle=True)\n",
    "\n",
    "    model_json = classifier.to_json()\n",
    "    with open('model.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    classifier.save_weights('after_last_epoch.h5')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(i)\n",
    "    plt.plot(model_history.history['accuracy'])\n",
    "    plt.plot(model_history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.savefig('Model_history/Model accuracy_' + str(i))\n",
    "    plt.close(i)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(i + K_FOLDING)\n",
    "    plt.plot(model_history.history['loss'])\n",
    "    plt.plot(model_history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.savefig('Model_history/Model loss_' + str(i))\n",
    "    plt.close(i + k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC value \n",
    "\n",
    "The AUC value and ROC curve can now be evaluated on 1/2 b-tag(s) CMS data and on $t\\bar{t}$ and QCD MC simulated events.\n",
    "If the AUC value on $t\\bar{t}$ and QCD MC simulated events is nearly as good as when trained only on them (see ch. 4.1) then a\n",
    "good classification is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_btag = np.concatenate([CMS_2_btag, CMS_1_btag])\n",
    "y_btag = np.concatenate([np.ones(len(CMS_2_btag)), np.zeros(len(CMS_1_btag))])\n",
    "\n",
    "X_true = np.concatenate([TTbar, QCD])\n",
    "y_true = np.concatenate([np.ones(len(TTbar)), np.zeros(len(QCD))])\n",
    "\n",
    "# define lists for tpr, fpr, auc\n",
    "tprs = []\n",
    "tprs_evaluate_on_MC = []\n",
    "aucs = []\n",
    "aucs_evaluate_on_MC = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_fpr_evaluate_on_MC = np.linspace(0, 1, 100)\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "for i in range(K_FOLDING):\n",
    "    loaded_model.load_weights('Run_' + str(i) + '_weights_best_val_loss.h5')\n",
    "    print('Loaded model ' + str(i) + ' from disk')\n",
    "    # loaded_model.load_weights('after_last_epoch.h5')\n",
    "\n",
    "    predictions = loaded_model.predict(X_btag[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])])\n",
    "    fpr, tpr, thresholds = roc_curve(y_btag,\n",
    "                                     predictions[:, 1],\n",
    "                                     sample_weight=X_btag[:, complete_feature_dictionary['weight.combinedWeight']])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "    predictions_evaluate_on_MC = loaded_model.predict(X_true[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])])\n",
    "    fpr_evaluate_on_MC, tpr_evaluate_on_MC, thresholds_evaluate_on_MC\\\n",
    "        = roc_curve(y_true,\n",
    "                    predictions_evaluate_on_MC[:, 1],\n",
    "                    sample_weight=MC_lum * X_true[:, complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']])\n",
    "    tprs_evaluate_on_MC.append(np.interp(mean_fpr_evaluate_on_MC, fpr_evaluate_on_MC, tpr_evaluate_on_MC))\n",
    "    tprs_evaluate_on_MC[-1][0] = 0.0\n",
    "    roc_auc_val = auc(fpr_evaluate_on_MC, tpr_evaluate_on_MC)\n",
    "    aucs_evaluate_on_MC.append(roc_auc_val)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "\n",
    "# Training data\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'CMS Val 1/2 b-tag (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.5,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "\n",
    "# Evaluation data\n",
    "mean_tpr_evaluate_on_MC = np.mean(tprs_evaluate_on_MC, axis=0)\n",
    "mean_tpr_evaluate_on_MC[-1] = 1.0\n",
    "mean_auc_val = auc(mean_fpr_evaluate_on_MC, mean_tpr_evaluate_on_MC)\n",
    "std_auc_val = np.std(aucs_evaluate_on_MC)\n",
    "plt.plot(mean_fpr_evaluate_on_MC, mean_tpr_evaluate_on_MC, color='g',\n",
    "         label=r'MC Test S/BG (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_val, std_auc_val),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "\n",
    "std_tpr_val = np.std(tprs_evaluate_on_MC, axis=0)\n",
    "tprs_upper_val = np.minimum(mean_tpr_evaluate_on_MC + std_tpr_val, 1)\n",
    "tprs_lower_val = np.maximum(mean_tpr_evaluate_on_MC - std_tpr_val, 0)\n",
    "plt.fill_between(mean_fpr_evaluate_on_MC, tprs_lower_val, tprs_upper_val, color='grey', alpha=.5,\n",
    "                 label='')\n",
    "\n",
    "\n",
    "plt.xlim([0, 1.0])\n",
    "plt.ylim([0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10-fold cross validation ROC-Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Jupyter Notebooks Workspace\\CMS_Model_2020_11_07_15_42_10\n",
      "0 of 461207 done of ttbar_172_5elapsed time: 0:00:00.35\n",
      "1000 of 461207 done of ttbar_172_5elapsed time: 0:00:38.41\n",
      "2000 of 461207 done of ttbar_172_5elapsed time: 0:01:10.69\n",
      "3000 of 461207 done of ttbar_172_5elapsed time: 0:01:47.59\n",
      "4000 of 461207 done of ttbar_172_5elapsed time: 0:02:21.94\n",
      "5000 of 461207 done of ttbar_172_5elapsed time: 0:02:51.66\n",
      "6000 of 461207 done of ttbar_172_5elapsed time: 0:03:21.06\n",
      "7000 of 461207 done of ttbar_172_5elapsed time: 0:03:53.45\n",
      "8000 of 461207 done of ttbar_172_5elapsed time: 0:04:24.03\n",
      "9000 of 461207 done of ttbar_172_5elapsed time: 0:04:55.13\n",
      "10000 of 461207 done of ttbar_172_5elapsed time: 0:05:28.54\n",
      "11000 of 461207 done of ttbar_172_5elapsed time: 0:05:58.34\n",
      "12000 of 461207 done of ttbar_172_5elapsed time: 0:06:30.57\n",
      "13000 of 461207 done of ttbar_172_5elapsed time: 0:07:05.90\n",
      "14000 of 461207 done of ttbar_172_5elapsed time: 0:07:37.28\n",
      "15000 of 461207 done of ttbar_172_5elapsed time: 0:08:08.42\n",
      "16000 of 461207 done of ttbar_172_5elapsed time: 0:08:45.76\n",
      "17000 of 461207 done of ttbar_172_5elapsed time: 0:09:20.90\n",
      "18000 of 461207 done of ttbar_172_5elapsed time: 0:09:56.79\n",
      "19000 of 461207 done of ttbar_172_5elapsed time: 0:10:28.06\n",
      "20000 of 461207 done of ttbar_172_5elapsed time: 0:11:01.17\n",
      "21000 of 461207 done of ttbar_172_5elapsed time: 0:11:31.64\n",
      "22000 of 461207 done of ttbar_172_5elapsed time: 0:12:02.61\n",
      "23000 of 461207 done of ttbar_172_5elapsed time: 0:12:36.22\n",
      "24000 of 461207 done of ttbar_172_5elapsed time: 0:13:07.68\n",
      "25000 of 461207 done of ttbar_172_5elapsed time: 0:13:40.51\n",
      "26000 of 461207 done of ttbar_172_5elapsed time: 0:14:15.40\n",
      "27000 of 461207 done of ttbar_172_5elapsed time: 0:14:46.66\n",
      "28000 of 461207 done of ttbar_172_5elapsed time: 0:15:17.68\n",
      "29000 of 461207 done of ttbar_172_5elapsed time: 0:15:51.69\n",
      "30000 of 461207 done of ttbar_172_5elapsed time: 0:16:23.04\n",
      "31000 of 461207 done of ttbar_172_5elapsed time: 0:16:54.17\n",
      "32000 of 461207 done of ttbar_172_5elapsed time: 0:17:28.49\n",
      "33000 of 461207 done of ttbar_172_5elapsed time: 0:18:01.14\n",
      "34000 of 461207 done of ttbar_172_5elapsed time: 0:18:32.85\n",
      "35000 of 461207 done of ttbar_172_5elapsed time: 0:19:07.68\n",
      "36000 of 461207 done of ttbar_172_5elapsed time: 0:19:38.08\n",
      "37000 of 461207 done of ttbar_172_5elapsed time: 0:20:09.08\n",
      "38000 of 461207 done of ttbar_172_5elapsed time: 0:20:43.84\n",
      "39000 of 461207 done of ttbar_172_5elapsed time: 0:21:16.04\n",
      "40000 of 461207 done of ttbar_172_5elapsed time: 0:21:47.97\n",
      "41000 of 461207 done of ttbar_172_5elapsed time: 0:22:23.24\n",
      "42000 of 461207 done of ttbar_172_5elapsed time: 0:22:54.84\n",
      "43000 of 461207 done of ttbar_172_5elapsed time: 0:23:26.02\n",
      "44000 of 461207 done of ttbar_172_5elapsed time: 0:24:00.80\n",
      "45000 of 461207 done of ttbar_172_5elapsed time: 0:24:32.11\n",
      "46000 of 461207 done of ttbar_172_5elapsed time: 0:25:02.67\n",
      "47000 of 461207 done of ttbar_172_5elapsed time: 0:25:37.49\n",
      "48000 of 461207 done of ttbar_172_5elapsed time: 0:26:08.91\n",
      "49000 of 461207 done of ttbar_172_5elapsed time: 0:26:40.77\n",
      "50000 of 461207 done of ttbar_172_5elapsed time: 0:27:15.17\n",
      "51000 of 461207 done of ttbar_172_5elapsed time: 0:27:46.69\n",
      "52000 of 461207 done of ttbar_172_5elapsed time: 0:28:17.13\n",
      "53000 of 461207 done of ttbar_172_5elapsed time: 0:28:50.59\n",
      "54000 of 461207 done of ttbar_172_5elapsed time: 0:29:22.16\n",
      "55000 of 461207 done of ttbar_172_5elapsed time: 0:29:52.93\n",
      "56000 of 461207 done of ttbar_172_5elapsed time: 0:30:24.75\n",
      "57000 of 461207 done of ttbar_172_5elapsed time: 0:30:59.71\n",
      "58000 of 461207 done of ttbar_172_5elapsed time: 0:31:31.00\n",
      "59000 of 461207 done of ttbar_172_5elapsed time: 0:32:03.31\n",
      "60000 of 461207 done of ttbar_172_5elapsed time: 0:32:37.17\n",
      "61000 of 461207 done of ttbar_172_5elapsed time: 0:33:08.21\n",
      "62000 of 461207 done of ttbar_172_5elapsed time: 0:33:40.42\n",
      "63000 of 461207 done of ttbar_172_5elapsed time: 0:34:15.81\n",
      "64000 of 461207 done of ttbar_172_5elapsed time: 0:34:47.91\n",
      "65000 of 461207 done of ttbar_172_5elapsed time: 0:35:19.71\n",
      "66000 of 461207 done of ttbar_172_5elapsed time: 0:35:53.83\n",
      "67000 of 461207 done of ttbar_172_5elapsed time: 0:36:25.20\n",
      "68000 of 461207 done of ttbar_172_5elapsed time: 0:36:57.48\n",
      "69000 of 461207 done of ttbar_172_5elapsed time: 0:37:32.24\n",
      "70000 of 461207 done of ttbar_172_5elapsed time: 0:38:04.25\n",
      "71000 of 461207 done of ttbar_172_5elapsed time: 0:38:36.53\n",
      "72000 of 461207 done of ttbar_172_5elapsed time: 0:39:10.50\n",
      "73000 of 461207 done of ttbar_172_5elapsed time: 0:39:44.13\n",
      "74000 of 461207 done of ttbar_172_5elapsed time: 0:40:15.93\n",
      "75000 of 461207 done of ttbar_172_5elapsed time: 0:40:50.40\n",
      "76000 of 461207 done of ttbar_172_5elapsed time: 0:41:21.06\n",
      "77000 of 461207 done of ttbar_172_5elapsed time: 0:41:52.11\n",
      "78000 of 461207 done of ttbar_172_5elapsed time: 0:42:27.40\n",
      "79000 of 461207 done of ttbar_172_5elapsed time: 0:43:00.67\n",
      "80000 of 461207 done of ttbar_172_5elapsed time: 0:43:37.96\n",
      "81000 of 461207 done of ttbar_172_5elapsed time: 0:44:18.98\n",
      "82000 of 461207 done of ttbar_172_5elapsed time: 0:44:54.43\n",
      "83000 of 461207 done of ttbar_172_5elapsed time: 0:45:29.96\n",
      "84000 of 461207 done of ttbar_172_5elapsed time: 0:46:07.36\n",
      "85000 of 461207 done of ttbar_172_5elapsed time: 0:46:44.18\n",
      "86000 of 461207 done of ttbar_172_5elapsed time: 0:47:18.16\n",
      "87000 of 461207 done of ttbar_172_5elapsed time: 0:47:54.89\n",
      "88000 of 461207 done of ttbar_172_5elapsed time: 0:48:28.59\n",
      "89000 of 461207 done of ttbar_172_5elapsed time: 0:49:02.82\n",
      "90000 of 461207 done of ttbar_172_5elapsed time: 0:49:40.52\n",
      "91000 of 461207 done of ttbar_172_5elapsed time: 0:50:14.71\n",
      "92000 of 461207 done of ttbar_172_5elapsed time: 0:50:49.51\n",
      "93000 of 461207 done of ttbar_172_5elapsed time: 0:51:28.16\n",
      "94000 of 461207 done of ttbar_172_5elapsed time: 0:52:05.08\n",
      "95000 of 461207 done of ttbar_172_5elapsed time: 0:52:40.08\n",
      "96000 of 461207 done of ttbar_172_5elapsed time: 0:53:14.54\n",
      "97000 of 461207 done of ttbar_172_5elapsed time: 0:53:47.92\n",
      "98000 of 461207 done of ttbar_172_5elapsed time: 0:54:19.54\n",
      "99000 of 461207 done of ttbar_172_5elapsed time: 0:54:48.78\n",
      "100000 of 461207 done of ttbar_172_5elapsed time: 0:55:20.61\n",
      "101000 of 461207 done of ttbar_172_5elapsed time: 0:55:49.86\n",
      "102000 of 461207 done of ttbar_172_5elapsed time: 0:56:18.97\n",
      "103000 of 461207 done of ttbar_172_5elapsed time: 0:56:50.97\n",
      "104000 of 461207 done of ttbar_172_5elapsed time: 0:57:20.20\n",
      "105000 of 461207 done of ttbar_172_5elapsed time: 0:57:49.44\n",
      "106000 of 461207 done of ttbar_172_5elapsed time: 0:58:22.60\n",
      "107000 of 461207 done of ttbar_172_5elapsed time: 0:58:56.74\n",
      "108000 of 461207 done of ttbar_172_5elapsed time: 0:59:30.13\n",
      "109000 of 461207 done of ttbar_172_5elapsed time: 1:00:03.46\n",
      "110000 of 461207 done of ttbar_172_5elapsed time: 1:00:32.52\n",
      "111000 of 461207 done of ttbar_172_5elapsed time: 1:01:01.66\n",
      "112000 of 461207 done of ttbar_172_5elapsed time: 1:01:33.63\n",
      "113000 of 461207 done of ttbar_172_5elapsed time: 1:02:02.73\n",
      "114000 of 461207 done of ttbar_172_5elapsed time: 1:02:31.88\n",
      "115000 of 461207 done of ttbar_172_5elapsed time: 1:03:03.67\n",
      "116000 of 461207 done of ttbar_172_5elapsed time: 1:03:32.68\n",
      "117000 of 461207 done of ttbar_172_5elapsed time: 1:04:01.85\n",
      "118000 of 461207 done of ttbar_172_5elapsed time: 1:04:33.74\n",
      "119000 of 461207 done of ttbar_172_5elapsed time: 1:05:02.69\n",
      "120000 of 461207 done of ttbar_172_5elapsed time: 1:05:31.79\n",
      "121000 of 461207 done of ttbar_172_5elapsed time: 1:06:03.69\n",
      "122000 of 461207 done of ttbar_172_5elapsed time: 1:06:32.65\n",
      "123000 of 461207 done of ttbar_172_5elapsed time: 1:07:01.56\n",
      "124000 of 461207 done of ttbar_172_5elapsed time: 1:07:33.35\n",
      "125000 of 461207 done of ttbar_172_5elapsed time: 1:08:02.38\n",
      "126000 of 461207 done of ttbar_172_5elapsed time: 1:08:31.54\n",
      "127000 of 461207 done of ttbar_172_5elapsed time: 1:09:03.36\n",
      "128000 of 461207 done of ttbar_172_5elapsed time: 1:09:32.42\n",
      "129000 of 461207 done of ttbar_172_5elapsed time: 1:10:01.69\n",
      "130000 of 461207 done of ttbar_172_5elapsed time: 1:10:33.53\n",
      "131000 of 461207 done of ttbar_172_5elapsed time: 1:11:02.73\n",
      "132000 of 461207 done of ttbar_172_5elapsed time: 1:11:32.06\n",
      "133000 of 461207 done of ttbar_172_5elapsed time: 1:12:01.16\n",
      "134000 of 461207 done of ttbar_172_5elapsed time: 1:12:33.28\n",
      "135000 of 461207 done of ttbar_172_5elapsed time: 1:13:02.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136000 of 461207 done of ttbar_172_5elapsed time: 1:13:31.61\n",
      "137000 of 461207 done of ttbar_172_5elapsed time: 1:14:04.06\n",
      "138000 of 461207 done of ttbar_172_5elapsed time: 1:14:34.13\n",
      "139000 of 461207 done of ttbar_172_5elapsed time: 1:15:03.42\n",
      "140000 of 461207 done of ttbar_172_5elapsed time: 1:15:35.26\n",
      "141000 of 461207 done of ttbar_172_5elapsed time: 1:16:04.40\n",
      "142000 of 461207 done of ttbar_172_5elapsed time: 1:16:33.49\n",
      "143000 of 461207 done of ttbar_172_5elapsed time: 1:17:05.11\n",
      "144000 of 461207 done of ttbar_172_5elapsed time: 1:17:33.93\n",
      "145000 of 461207 done of ttbar_172_5elapsed time: 1:18:03.88\n",
      "146000 of 461207 done of ttbar_172_5elapsed time: 1:18:35.64\n",
      "147000 of 461207 done of ttbar_172_5elapsed time: 1:19:04.79\n",
      "148000 of 461207 done of ttbar_172_5elapsed time: 1:19:33.84\n",
      "149000 of 461207 done of ttbar_172_5elapsed time: 1:20:05.42\n",
      "150000 of 461207 done of ttbar_172_5elapsed time: 1:20:34.29\n",
      "151000 of 461207 done of ttbar_172_5elapsed time: 1:21:03.28\n",
      "152000 of 461207 done of ttbar_172_5elapsed time: 1:21:35.01\n",
      "153000 of 461207 done of ttbar_172_5elapsed time: 1:22:04.14\n",
      "154000 of 461207 done of ttbar_172_5elapsed time: 1:22:33.23\n",
      "155000 of 461207 done of ttbar_172_5elapsed time: 1:23:05.05\n",
      "156000 of 461207 done of ttbar_172_5elapsed time: 1:23:34.32\n",
      "157000 of 461207 done of ttbar_172_5elapsed time: 1:24:03.34\n",
      "158000 of 461207 done of ttbar_172_5elapsed time: 1:24:35.28\n",
      "159000 of 461207 done of ttbar_172_5elapsed time: 1:25:04.06\n",
      "160000 of 461207 done of ttbar_172_5elapsed time: 1:25:32.97\n",
      "161000 of 461207 done of ttbar_172_5elapsed time: 1:26:04.57\n",
      "162000 of 461207 done of ttbar_172_5elapsed time: 1:26:33.43\n",
      "163000 of 461207 done of ttbar_172_5elapsed time: 1:27:02.33\n",
      "164000 of 461207 done of ttbar_172_5elapsed time: 1:27:33.89\n",
      "165000 of 461207 done of ttbar_172_5elapsed time: 1:28:02.96\n",
      "166000 of 461207 done of ttbar_172_5elapsed time: 1:28:31.62\n",
      "167000 of 461207 done of ttbar_172_5elapsed time: 1:29:04.01\n",
      "168000 of 461207 done of ttbar_172_5elapsed time: 1:29:32.78\n",
      "169000 of 461207 done of ttbar_172_5elapsed time: 1:30:01.56\n",
      "170000 of 461207 done of ttbar_172_5elapsed time: 1:30:33.34\n",
      "171000 of 461207 done of ttbar_172_5elapsed time: 1:31:02.14\n",
      "172000 of 461207 done of ttbar_172_5elapsed time: 1:31:32.58\n",
      "173000 of 461207 done of ttbar_172_5elapsed time: 1:32:05.81\n",
      "174000 of 461207 done of ttbar_172_5elapsed time: 1:32:41.11\n",
      "175000 of 461207 done of ttbar_172_5elapsed time: 1:33:10.88\n",
      "176000 of 461207 done of ttbar_172_5elapsed time: 1:33:39.72\n",
      "177000 of 461207 done of ttbar_172_5elapsed time: 1:34:11.22\n",
      "178000 of 461207 done of ttbar_172_5elapsed time: 1:34:39.82\n",
      "179000 of 461207 done of ttbar_172_5elapsed time: 1:35:08.55\n",
      "180000 of 461207 done of ttbar_172_5elapsed time: 1:35:40.20\n",
      "181000 of 461207 done of ttbar_172_5elapsed time: 1:36:09.27\n",
      "182000 of 461207 done of ttbar_172_5elapsed time: 1:36:41.04\n",
      "183000 of 461207 done of ttbar_172_5elapsed time: 1:37:16.32\n",
      "184000 of 461207 done of ttbar_172_5elapsed time: 1:37:49.67\n",
      "185000 of 461207 done of ttbar_172_5elapsed time: 1:38:18.72\n",
      "186000 of 461207 done of ttbar_172_5elapsed time: 1:38:50.39\n",
      "187000 of 461207 done of ttbar_172_5elapsed time: 1:39:19.99\n",
      "188000 of 461207 done of ttbar_172_5elapsed time: 1:39:49.21\n",
      "189000 of 461207 done of ttbar_172_5elapsed time: 1:40:21.03\n",
      "190000 of 461207 done of ttbar_172_5elapsed time: 1:40:50.59\n",
      "191000 of 461207 done of ttbar_172_5elapsed time: 1:41:19.79\n",
      "192000 of 461207 done of ttbar_172_5elapsed time: 1:41:56.39\n",
      "193000 of 461207 done of ttbar_172_5elapsed time: 1:42:29.76\n",
      "194000 of 461207 done of ttbar_172_5elapsed time: 1:43:01.40\n",
      "195000 of 461207 done of ttbar_172_5elapsed time: 1:43:33.67\n",
      "196000 of 461207 done of ttbar_172_5elapsed time: 1:44:03.66\n",
      "197000 of 461207 done of ttbar_172_5elapsed time: 1:44:33.21\n",
      "198000 of 461207 done of ttbar_172_5elapsed time: 1:45:05.98\n",
      "199000 of 461207 done of ttbar_172_5elapsed time: 1:45:35.29\n",
      "200000 of 461207 done of ttbar_172_5elapsed time: 1:46:04.78\n",
      "201000 of 461207 done of ttbar_172_5elapsed time: 1:46:39.92\n",
      "202000 of 461207 done of ttbar_172_5elapsed time: 1:47:13.47\n",
      "203000 of 461207 done of ttbar_172_5elapsed time: 1:47:46.78\n",
      "204000 of 461207 done of ttbar_172_5elapsed time: 1:48:19.12\n",
      "205000 of 461207 done of ttbar_172_5elapsed time: 1:48:48.54\n",
      "206000 of 461207 done of ttbar_172_5elapsed time: 1:49:17.99\n",
      "207000 of 461207 done of ttbar_172_5elapsed time: 1:49:50.38\n",
      "208000 of 461207 done of ttbar_172_5elapsed time: 1:50:19.85\n",
      "209000 of 461207 done of ttbar_172_5elapsed time: 1:50:49.40\n",
      "210000 of 461207 done of ttbar_172_5elapsed time: 1:51:21.78\n",
      "211000 of 461207 done of ttbar_172_5elapsed time: 1:51:55.22\n",
      "212000 of 461207 done of ttbar_172_5elapsed time: 1:52:28.66\n",
      "213000 of 461207 done of ttbar_172_5elapsed time: 1:53:00.48\n",
      "214000 of 461207 done of ttbar_172_5elapsed time: 1:53:32.58\n",
      "215000 of 461207 done of ttbar_172_5elapsed time: 1:54:02.22\n",
      "216000 of 461207 done of ttbar_172_5elapsed time: 1:54:31.79\n",
      "217000 of 461207 done of ttbar_172_5elapsed time: 1:55:04.09\n",
      "218000 of 461207 done of ttbar_172_5elapsed time: 1:55:33.69\n",
      "219000 of 461207 done of ttbar_172_5elapsed time: 1:56:03.38\n",
      "220000 of 461207 done of ttbar_172_5elapsed time: 1:56:35.44\n",
      "221000 of 461207 done of ttbar_172_5elapsed time: 1:57:05.06\n",
      "222000 of 461207 done of ttbar_172_5elapsed time: 1:57:34.48\n",
      "223000 of 461207 done of ttbar_172_5elapsed time: 1:58:06.70\n",
      "224000 of 461207 done of ttbar_172_5elapsed time: 1:58:39.41\n",
      "225000 of 461207 done of ttbar_172_5elapsed time: 1:59:12.75\n",
      "226000 of 461207 done of ttbar_172_5elapsed time: 1:59:48.81\n",
      "227000 of 461207 done of ttbar_172_5elapsed time: 2:00:18.18\n",
      "228000 of 461207 done of ttbar_172_5elapsed time: 2:00:47.50\n",
      "229000 of 461207 done of ttbar_172_5elapsed time: 2:01:19.69\n",
      "230000 of 461207 done of ttbar_172_5elapsed time: 2:01:53.47\n",
      "231000 of 461207 done of ttbar_172_5elapsed time: 2:02:26.84\n",
      "232000 of 461207 done of ttbar_172_5elapsed time: 2:03:01.93\n",
      "233000 of 461207 done of ttbar_172_5elapsed time: 2:03:31.50\n",
      "234000 of 461207 done of ttbar_172_5elapsed time: 2:04:01.24\n",
      "235000 of 461207 done of ttbar_172_5elapsed time: 2:04:33.62\n",
      "236000 of 461207 done of ttbar_172_5elapsed time: 2:05:02.86\n",
      "237000 of 461207 done of ttbar_172_5elapsed time: 2:05:32.76\n",
      "238000 of 461207 done of ttbar_172_5elapsed time: 2:06:05.26\n",
      "239000 of 461207 done of ttbar_172_5elapsed time: 2:06:36.65\n",
      "240000 of 461207 done of ttbar_172_5elapsed time: 2:07:10.58\n",
      "241000 of 461207 done of ttbar_172_5elapsed time: 2:07:46.85\n",
      "242000 of 461207 done of ttbar_172_5elapsed time: 2:08:16.64\n",
      "243000 of 461207 done of ttbar_172_5elapsed time: 2:08:46.30\n",
      "244000 of 461207 done of ttbar_172_5elapsed time: 2:09:18.74\n",
      "245000 of 461207 done of ttbar_172_5elapsed time: 2:09:48.50\n",
      "246000 of 461207 done of ttbar_172_5elapsed time: 2:10:18.30\n",
      "247000 of 461207 done of ttbar_172_5elapsed time: 2:10:50.84\n",
      "248000 of 461207 done of ttbar_172_5elapsed time: 2:11:20.48\n",
      "249000 of 461207 done of ttbar_172_5elapsed time: 2:11:54.14\n",
      "250000 of 461207 done of ttbar_172_5elapsed time: 2:12:27.83\n",
      "251000 of 461207 done of ttbar_172_5elapsed time: 2:13:02.41\n",
      "252000 of 461207 done of ttbar_172_5elapsed time: 2:13:32.33\n",
      "253000 of 461207 done of ttbar_172_5elapsed time: 2:14:02.73\n",
      "254000 of 461207 done of ttbar_172_5elapsed time: 2:14:34.79\n",
      "255000 of 461207 done of ttbar_172_5elapsed time: 2:15:04.59\n",
      "256000 of 461207 done of ttbar_172_5elapsed time: 2:15:34.20\n",
      "257000 of 461207 done of ttbar_172_5elapsed time: 2:16:06.49\n",
      "258000 of 461207 done of ttbar_172_5elapsed time: 2:16:38.23\n",
      "259000 of 461207 done of ttbar_172_5elapsed time: 2:17:12.06\n",
      "260000 of 461207 done of ttbar_172_5elapsed time: 2:17:48.86\n",
      "261000 of 461207 done of ttbar_172_5elapsed time: 2:18:18.82\n",
      "262000 of 461207 done of ttbar_172_5elapsed time: 2:18:48.39\n",
      "263000 of 461207 done of ttbar_172_5elapsed time: 2:19:20.81\n",
      "264000 of 461207 done of ttbar_172_5elapsed time: 2:19:50.52\n",
      "265000 of 461207 done of ttbar_172_5elapsed time: 2:20:20.20\n",
      "266000 of 461207 done of ttbar_172_5elapsed time: 2:20:52.80\n",
      "267000 of 461207 done of ttbar_172_5elapsed time: 2:21:22.40\n",
      "268000 of 461207 done of ttbar_172_5elapsed time: 2:21:56.58\n",
      "269000 of 461207 done of ttbar_172_5elapsed time: 2:22:33.20\n",
      "270000 of 461207 done of ttbar_172_5elapsed time: 2:23:04.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271000 of 461207 done of ttbar_172_5elapsed time: 2:23:34.55\n",
      "272000 of 461207 done of ttbar_172_5elapsed time: 2:24:06.98\n",
      "273000 of 461207 done of ttbar_172_5elapsed time: 2:24:36.44\n",
      "274000 of 461207 done of ttbar_172_5elapsed time: 2:25:06.07\n",
      "275000 of 461207 done of ttbar_172_5elapsed time: 2:25:38.60\n",
      "276000 of 461207 done of ttbar_172_5elapsed time: 2:26:07.98\n",
      "277000 of 461207 done of ttbar_172_5elapsed time: 2:26:40.01\n",
      "278000 of 461207 done of ttbar_172_5elapsed time: 2:27:16.23\n",
      "279000 of 461207 done of ttbar_172_5elapsed time: 2:27:49.82\n",
      "280000 of 461207 done of ttbar_172_5elapsed time: 2:28:19.61\n",
      "281000 of 461207 done of ttbar_172_5elapsed time: 2:28:52.68\n",
      "282000 of 461207 done of ttbar_172_5elapsed time: 2:29:22.22\n",
      "283000 of 461207 done of ttbar_172_5elapsed time: 2:29:51.69\n",
      "284000 of 461207 done of ttbar_172_5elapsed time: 2:30:24.14\n",
      "285000 of 461207 done of ttbar_172_5elapsed time: 2:30:53.77\n",
      "286000 of 461207 done of ttbar_172_5elapsed time: 2:31:23.52\n",
      "287000 of 461207 done of ttbar_172_5elapsed time: 2:32:00.58\n",
      "288000 of 461207 done of ttbar_172_5elapsed time: 2:32:34.14\n",
      "289000 of 461207 done of ttbar_172_5elapsed time: 2:33:05.22\n",
      "290000 of 461207 done of ttbar_172_5elapsed time: 2:33:34.71\n",
      "291000 of 461207 done of ttbar_172_5elapsed time: 2:34:07.12\n",
      "292000 of 461207 done of ttbar_172_5elapsed time: 2:34:36.85\n",
      "293000 of 461207 done of ttbar_172_5elapsed time: 2:35:06.43\n",
      "294000 of 461207 done of ttbar_172_5elapsed time: 2:35:38.67\n",
      "295000 of 461207 done of ttbar_172_5elapsed time: 2:36:08.47\n",
      "296000 of 461207 done of ttbar_172_5elapsed time: 2:36:40.66\n",
      "297000 of 461207 done of ttbar_172_5elapsed time: 2:37:16.63\n",
      "298000 of 461207 done of ttbar_172_5elapsed time: 2:37:50.52\n",
      "299000 of 461207 done of ttbar_172_5elapsed time: 2:38:20.50\n",
      "300000 of 461207 done of ttbar_172_5elapsed time: 2:38:52.96\n",
      "301000 of 461207 done of ttbar_172_5elapsed time: 2:39:23.40\n",
      "302000 of 461207 done of ttbar_172_5elapsed time: 2:39:53.03\n",
      "303000 of 461207 done of ttbar_172_5elapsed time: 2:40:25.20\n",
      "304000 of 461207 done of ttbar_172_5elapsed time: 2:40:54.85\n",
      "305000 of 461207 done of ttbar_172_5elapsed time: 2:41:24.32\n",
      "306000 of 461207 done of ttbar_172_5elapsed time: 2:42:01.07\n",
      "307000 of 461207 done of ttbar_172_5elapsed time: 2:42:34.79\n",
      "308000 of 461207 done of ttbar_172_5elapsed time: 2:43:05.87\n",
      "309000 of 461207 done of ttbar_172_5elapsed time: 2:43:38.22\n",
      "310000 of 461207 done of ttbar_172_5elapsed time: 2:44:08.32\n",
      "311000 of 461207 done of ttbar_172_5elapsed time: 2:44:38.31\n",
      "312000 of 461207 done of ttbar_172_5elapsed time: 2:45:11.03\n",
      "313000 of 461207 done of ttbar_172_5elapsed time: 2:45:40.47\n",
      "314000 of 461207 done of ttbar_172_5elapsed time: 2:46:10.60\n",
      "315000 of 461207 done of ttbar_172_5elapsed time: 2:46:46.09\n",
      "316000 of 461207 done of ttbar_172_5elapsed time: 2:47:19.08\n",
      "317000 of 461207 done of ttbar_172_5elapsed time: 2:47:52.45\n",
      "318000 of 461207 done of ttbar_172_5elapsed time: 2:48:25.03\n",
      "319000 of 461207 done of ttbar_172_5elapsed time: 2:48:54.48\n",
      "320000 of 461207 done of ttbar_172_5elapsed time: 2:49:24.01\n",
      "321000 of 461207 done of ttbar_172_5elapsed time: 2:49:56.40\n",
      "322000 of 461207 done of ttbar_172_5elapsed time: 2:50:26.17\n",
      "323000 of 461207 done of ttbar_172_5elapsed time: 2:50:55.77\n",
      "324000 of 461207 done of ttbar_172_5elapsed time: 2:51:28.68\n",
      "325000 of 461207 done of ttbar_172_5elapsed time: 2:52:02.07\n",
      "326000 of 461207 done of ttbar_172_5elapsed time: 2:52:35.03\n",
      "327000 of 461207 done of ttbar_172_5elapsed time: 2:53:06.55\n",
      "328000 of 461207 done of ttbar_172_5elapsed time: 2:53:39.41\n",
      "329000 of 461207 done of ttbar_172_5elapsed time: 2:54:09.28\n",
      "330000 of 461207 done of ttbar_172_5elapsed time: 2:54:38.94\n",
      "331000 of 461207 done of ttbar_172_5elapsed time: 2:55:11.39\n",
      "332000 of 461207 done of ttbar_172_5elapsed time: 2:55:41.06\n",
      "333000 of 461207 done of ttbar_172_5elapsed time: 2:56:10.96\n",
      "334000 of 461207 done of ttbar_172_5elapsed time: 2:56:46.58\n",
      "335000 of 461207 done of ttbar_172_5elapsed time: 2:57:19.28\n",
      "336000 of 461207 done of ttbar_172_5elapsed time: 2:57:52.71\n",
      "337000 of 461207 done of ttbar_172_5elapsed time: 2:58:26.72\n",
      "338000 of 461207 done of ttbar_172_5elapsed time: 2:59:01.06\n",
      "339000 of 461207 done of ttbar_172_5elapsed time: 2:59:34.56\n",
      "340000 of 461207 done of ttbar_172_5elapsed time: 3:00:07.61\n",
      "341000 of 461207 done of ttbar_172_5elapsed time: 3:00:36.89\n",
      "342000 of 461207 done of ttbar_172_5elapsed time: 3:01:06.29\n",
      "343000 of 461207 done of ttbar_172_5elapsed time: 3:01:41.17\n",
      "344000 of 461207 done of ttbar_172_5elapsed time: 3:02:14.73\n",
      "345000 of 461207 done of ttbar_172_5elapsed time: 3:02:48.43\n",
      "346000 of 461207 done of ttbar_172_5elapsed time: 3:03:21.24\n",
      "347000 of 461207 done of ttbar_172_5elapsed time: 3:03:51.02\n",
      "348000 of 461207 done of ttbar_172_5elapsed time: 3:04:20.88\n",
      "349000 of 461207 done of ttbar_172_5elapsed time: 3:04:53.41\n",
      "350000 of 461207 done of ttbar_172_5elapsed time: 3:05:22.88\n",
      "351000 of 461207 done of ttbar_172_5elapsed time: 3:05:52.38\n",
      "352000 of 461207 done of ttbar_172_5elapsed time: 3:06:25.01\n",
      "353000 of 461207 done of ttbar_172_5elapsed time: 3:06:59.37\n",
      "354000 of 461207 done of ttbar_172_5elapsed time: 3:07:33.06\n",
      "355000 of 461207 done of ttbar_172_5elapsed time: 3:08:07.21\n",
      "356000 of 461207 done of ttbar_172_5elapsed time: 3:08:37.03\n",
      "357000 of 461207 done of ttbar_172_5elapsed time: 3:09:06.66\n",
      "358000 of 461207 done of ttbar_172_5elapsed time: 3:09:38.95\n",
      "359000 of 461207 done of ttbar_172_5elapsed time: 3:10:08.58\n",
      "360000 of 461207 done of ttbar_172_5elapsed time: 3:10:38.26\n",
      "361000 of 461207 done of ttbar_172_5elapsed time: 3:11:10.63\n",
      "362000 of 461207 done of ttbar_172_5elapsed time: 3:11:43.08\n",
      "363000 of 461207 done of ttbar_172_5elapsed time: 3:12:16.25\n",
      "364000 of 461207 done of ttbar_172_5elapsed time: 3:12:53.09\n",
      "365000 of 461207 done of ttbar_172_5elapsed time: 3:13:22.52\n",
      "366000 of 461207 done of ttbar_172_5elapsed time: 3:13:52.26\n",
      "367000 of 461207 done of ttbar_172_5elapsed time: 3:14:23.19\n",
      "368000 of 461207 done of ttbar_172_5elapsed time: 3:14:55.58\n",
      "369000 of 461207 done of ttbar_172_5elapsed time: 3:15:25.23\n",
      "370000 of 461207 done of ttbar_172_5elapsed time: 3:15:55.02\n",
      "371000 of 461207 done of ttbar_172_5elapsed time: 3:16:28.07\n",
      "372000 of 461207 done of ttbar_172_5elapsed time: 3:17:02.41\n",
      "373000 of 461207 done of ttbar_172_5elapsed time: 3:17:36.15\n",
      "374000 of 461207 done of ttbar_172_5elapsed time: 3:18:10.85\n",
      "375000 of 461207 done of ttbar_172_5elapsed time: 3:18:40.58\n",
      "376000 of 461207 done of ttbar_172_5elapsed time: 3:19:10.19\n",
      "377000 of 461207 done of ttbar_172_5elapsed time: 3:19:42.57\n",
      "378000 of 461207 done of ttbar_172_5elapsed time: 3:20:12.13\n",
      "379000 of 461207 done of ttbar_172_5elapsed time: 3:20:42.24\n",
      "380000 of 461207 done of ttbar_172_5elapsed time: 3:21:14.66\n",
      "381000 of 461207 done of ttbar_172_5elapsed time: 3:21:47.91\n",
      "382000 of 461207 done of ttbar_172_5elapsed time: 3:22:20.51\n",
      "383000 of 461207 done of ttbar_172_5elapsed time: 3:22:56.44\n",
      "384000 of 461207 done of ttbar_172_5elapsed time: 3:23:26.09\n",
      "385000 of 461207 done of ttbar_172_5elapsed time: 3:23:55.62\n",
      "386000 of 461207 done of ttbar_172_5elapsed time: 3:24:27.99\n",
      "387000 of 461207 done of ttbar_172_5elapsed time: 3:24:58.02\n",
      "388000 of 461207 done of ttbar_172_5elapsed time: 3:25:27.68\n",
      "389000 of 461207 done of ttbar_172_5elapsed time: 3:26:00.27\n",
      "390000 of 461207 done of ttbar_172_5elapsed time: 3:26:31.09\n",
      "391000 of 461207 done of ttbar_172_5elapsed time: 3:27:05.25\n",
      "392000 of 461207 done of ttbar_172_5elapsed time: 3:27:41.16\n",
      "393000 of 461207 done of ttbar_172_5elapsed time: 3:28:11.49\n",
      "394000 of 461207 done of ttbar_172_5elapsed time: 3:28:40.77\n",
      "395000 of 461207 done of ttbar_172_5elapsed time: 3:29:13.55\n",
      "396000 of 461207 done of ttbar_172_5elapsed time: 3:29:43.04\n",
      "397000 of 461207 done of ttbar_172_5elapsed time: 3:30:12.73\n",
      "398000 of 461207 done of ttbar_172_5elapsed time: 3:30:45.19\n",
      "399000 of 461207 done of ttbar_172_5elapsed time: 3:31:14.81\n",
      "400000 of 461207 done of ttbar_172_5elapsed time: 3:31:48.01\n",
      "401000 of 461207 done of ttbar_172_5elapsed time: 3:32:23.54\n",
      "402000 of 461207 done of ttbar_172_5elapsed time: 3:32:56.32\n",
      "403000 of 461207 done of ttbar_172_5elapsed time: 3:33:25.74\n",
      "404000 of 461207 done of ttbar_172_5elapsed time: 3:33:55.41\n",
      "405000 of 461207 done of ttbar_172_5elapsed time: 3:34:27.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406000 of 461207 done of ttbar_172_5elapsed time: 3:34:56.89\n",
      "407000 of 461207 done of ttbar_172_5elapsed time: 3:35:26.00\n",
      "408000 of 461207 done of ttbar_172_5elapsed time: 3:35:58.37\n",
      "409000 of 461207 done of ttbar_172_5elapsed time: 3:36:28.70\n",
      "410000 of 461207 done of ttbar_172_5elapsed time: 3:37:02.24\n",
      "411000 of 461207 done of ttbar_172_5elapsed time: 3:37:38.62\n",
      "412000 of 461207 done of ttbar_172_5elapsed time: 3:38:09.55\n",
      "413000 of 461207 done of ttbar_172_5elapsed time: 3:38:39.13\n",
      "414000 of 461207 done of ttbar_172_5elapsed time: 3:39:11.47\n",
      "415000 of 461207 done of ttbar_172_5elapsed time: 3:39:41.59\n",
      "416000 of 461207 done of ttbar_172_5elapsed time: 3:40:11.15\n",
      "417000 of 461207 done of ttbar_172_5elapsed time: 3:40:43.57\n",
      "418000 of 461207 done of ttbar_172_5elapsed time: 3:41:13.09\n",
      "419000 of 461207 done of ttbar_172_5elapsed time: 3:41:45.83\n",
      "420000 of 461207 done of ttbar_172_5elapsed time: 3:42:21.78\n",
      "421000 of 461207 done of ttbar_172_5elapsed time: 3:42:55.13\n",
      "422000 of 461207 done of ttbar_172_5elapsed time: 3:43:24.72\n",
      "423000 of 461207 done of ttbar_172_5elapsed time: 3:43:56.91\n",
      "424000 of 461207 done of ttbar_172_5elapsed time: 3:44:26.99\n",
      "425000 of 461207 done of ttbar_172_5elapsed time: 3:44:56.87\n",
      "426000 of 461207 done of ttbar_172_5elapsed time: 3:45:29.67\n",
      "427000 of 461207 done of ttbar_172_5elapsed time: 3:45:59.31\n",
      "428000 of 461207 done of ttbar_172_5elapsed time: 3:46:30.79\n",
      "429000 of 461207 done of ttbar_172_5elapsed time: 3:47:07.48\n",
      "430000 of 461207 done of ttbar_172_5elapsed time: 3:47:40.69\n",
      "431000 of 461207 done of ttbar_172_5elapsed time: 3:48:11.04\n",
      "432000 of 461207 done of ttbar_172_5elapsed time: 3:48:43.55\n",
      "433000 of 461207 done of ttbar_172_5elapsed time: 3:49:13.36\n",
      "434000 of 461207 done of ttbar_172_5elapsed time: 3:49:43.25\n",
      "435000 of 461207 done of ttbar_172_5elapsed time: 3:50:15.73\n",
      "436000 of 461207 done of ttbar_172_5elapsed time: 3:50:45.42\n",
      "437000 of 461207 done of ttbar_172_5elapsed time: 3:51:15.21\n",
      "438000 of 461207 done of ttbar_172_5elapsed time: 3:51:47.64\n",
      "439000 of 461207 done of ttbar_172_5elapsed time: 3:52:17.43\n",
      "440000 of 461207 done of ttbar_172_5elapsed time: 3:52:47.53\n",
      "441000 of 461207 done of ttbar_172_5elapsed time: 3:53:20.68\n",
      "442000 of 461207 done of ttbar_172_5elapsed time: 3:53:50.46\n",
      "443000 of 461207 done of ttbar_172_5elapsed time: 3:54:20.29\n",
      "444000 of 461207 done of ttbar_172_5elapsed time: 3:54:50.09\n",
      "445000 of 461207 done of ttbar_172_5elapsed time: 3:55:22.76\n",
      "446000 of 461207 done of ttbar_172_5elapsed time: 3:55:52.78\n",
      "447000 of 461207 done of ttbar_172_5elapsed time: 3:56:22.66\n",
      "448000 of 461207 done of ttbar_172_5elapsed time: 3:56:55.42\n",
      "449000 of 461207 done of ttbar_172_5elapsed time: 3:57:25.41\n",
      "450000 of 461207 done of ttbar_172_5elapsed time: 3:57:55.06\n",
      "451000 of 461207 done of ttbar_172_5elapsed time: 3:58:29.44\n",
      "452000 of 461207 done of ttbar_172_5elapsed time: 3:59:03.26\n",
      "453000 of 461207 done of ttbar_172_5elapsed time: 3:59:36.84\n",
      "454000 of 461207 done of ttbar_172_5elapsed time: 4:00:10.31\n",
      "455000 of 461207 done of ttbar_172_5elapsed time: 4:00:40.24\n",
      "456000 of 461207 done of ttbar_172_5elapsed time: 4:01:10.09\n",
      "457000 of 461207 done of ttbar_172_5elapsed time: 4:01:42.77\n",
      "458000 of 461207 done of ttbar_172_5elapsed time: 4:02:12.76\n",
      "459000 of 461207 done of ttbar_172_5elapsed time: 4:02:42.57\n",
      "460000 of 461207 done of ttbar_172_5elapsed time: 4:03:15.28\n",
      "461000 of 461207 done of ttbar_172_5elapsed time: 4:03:45.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anacdonda\\envs\\tensorflow_keras\\lib\\site-packages\\ipykernel_launcher.py:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 144029 done of qcdelapsed time: 0:00:00.03\n",
      "1000 of 144029 done of qcdelapsed time: 0:00:29.99\n",
      "2000 of 144029 done of qcdelapsed time: 0:01:02.76\n",
      "3000 of 144029 done of qcdelapsed time: 0:01:32.88\n",
      "4000 of 144029 done of qcdelapsed time: 0:02:02.62\n",
      "5000 of 144029 done of qcdelapsed time: 0:02:35.23\n",
      "6000 of 144029 done of qcdelapsed time: 0:03:05.28\n",
      "7000 of 144029 done of qcdelapsed time: 0:03:35.10\n",
      "8000 of 144029 done of qcdelapsed time: 0:04:07.76\n",
      "9000 of 144029 done of qcdelapsed time: 0:04:37.58\n",
      "10000 of 144029 done of qcdelapsed time: 0:05:07.31\n",
      "11000 of 144029 done of qcdelapsed time: 0:05:39.78\n",
      "12000 of 144029 done of qcdelapsed time: 0:06:09.55\n",
      "13000 of 144029 done of qcdelapsed time: 0:06:39.14\n",
      "14000 of 144029 done of qcdelapsed time: 0:07:11.57\n",
      "15000 of 144029 done of qcdelapsed time: 0:07:41.06\n",
      "16000 of 144029 done of qcdelapsed time: 0:08:10.87\n",
      "17000 of 144029 done of qcdelapsed time: 0:08:43.77\n",
      "18000 of 144029 done of qcdelapsed time: 0:09:13.52\n",
      "19000 of 144029 done of qcdelapsed time: 0:09:43.53\n",
      "20000 of 144029 done of qcdelapsed time: 0:10:16.37\n",
      "21000 of 144029 done of qcdelapsed time: 0:10:46.66\n",
      "22000 of 144029 done of qcdelapsed time: 0:11:16.19\n",
      "23000 of 144029 done of qcdelapsed time: 0:11:48.94\n",
      "24000 of 144029 done of qcdelapsed time: 0:12:18.81\n",
      "25000 of 144029 done of qcdelapsed time: 0:12:48.72\n",
      "26000 of 144029 done of qcdelapsed time: 0:13:21.41\n",
      "27000 of 144029 done of qcdelapsed time: 0:13:51.86\n",
      "28000 of 144029 done of qcdelapsed time: 0:14:21.63\n",
      "29000 of 144029 done of qcdelapsed time: 0:14:52.39\n",
      "30000 of 144029 done of qcdelapsed time: 0:15:25.16\n",
      "31000 of 144029 done of qcdelapsed time: 0:15:55.08\n",
      "32000 of 144029 done of qcdelapsed time: 0:16:24.87\n",
      "33000 of 144029 done of qcdelapsed time: 0:16:57.42\n",
      "34000 of 144029 done of qcdelapsed time: 0:17:27.38\n",
      "35000 of 144029 done of qcdelapsed time: 0:17:57.35\n",
      "36000 of 144029 done of qcdelapsed time: 0:18:30.19\n",
      "37000 of 144029 done of qcdelapsed time: 0:18:59.90\n",
      "38000 of 144029 done of qcdelapsed time: 0:19:29.93\n",
      "39000 of 144029 done of qcdelapsed time: 0:20:02.78\n",
      "40000 of 144029 done of qcdelapsed time: 0:20:32.78\n",
      "41000 of 144029 done of qcdelapsed time: 0:21:02.75\n",
      "42000 of 144029 done of qcdelapsed time: 0:21:35.23\n",
      "43000 of 144029 done of qcdelapsed time: 0:22:05.03\n",
      "44000 of 144029 done of qcdelapsed time: 0:22:34.68\n",
      "45000 of 144029 done of qcdelapsed time: 0:23:07.34\n",
      "46000 of 144029 done of qcdelapsed time: 0:23:36.99\n",
      "47000 of 144029 done of qcdelapsed time: 0:24:06.81\n",
      "48000 of 144029 done of qcdelapsed time: 0:24:39.56\n",
      "49000 of 144029 done of qcdelapsed time: 0:25:09.30\n",
      "50000 of 144029 done of qcdelapsed time: 0:25:39.38\n",
      "51000 of 144029 done of qcdelapsed time: 0:26:11.95\n",
      "52000 of 144029 done of qcdelapsed time: 0:26:41.55\n",
      "53000 of 144029 done of qcdelapsed time: 0:27:11.35\n",
      "54000 of 144029 done of qcdelapsed time: 0:27:44.00\n",
      "55000 of 144029 done of qcdelapsed time: 0:28:13.70\n",
      "56000 of 144029 done of qcdelapsed time: 0:28:43.33\n",
      "57000 of 144029 done of qcdelapsed time: 0:29:15.95\n",
      "58000 of 144029 done of qcdelapsed time: 0:29:45.77\n",
      "59000 of 144029 done of qcdelapsed time: 0:30:15.84\n",
      "60000 of 144029 done of qcdelapsed time: 0:30:48.38\n",
      "61000 of 144029 done of qcdelapsed time: 0:31:18.10\n",
      "62000 of 144029 done of qcdelapsed time: 0:31:47.58\n",
      "63000 of 144029 done of qcdelapsed time: 0:32:19.90\n",
      "64000 of 144029 done of qcdelapsed time: 0:32:49.23\n",
      "65000 of 144029 done of qcdelapsed time: 0:33:18.91\n",
      "66000 of 144029 done of qcdelapsed time: 0:33:51.41\n",
      "67000 of 144029 done of qcdelapsed time: 0:34:21.32\n",
      "68000 of 144029 done of qcdelapsed time: 0:34:51.31\n",
      "69000 of 144029 done of qcdelapsed time: 0:35:21.79\n",
      "70000 of 144029 done of qcdelapsed time: 0:35:54.42\n",
      "71000 of 144029 done of qcdelapsed time: 0:36:24.17\n",
      "72000 of 144029 done of qcdelapsed time: 0:36:53.98\n",
      "73000 of 144029 done of qcdelapsed time: 0:37:26.55\n",
      "74000 of 144029 done of qcdelapsed time: 0:37:56.47\n",
      "75000 of 144029 done of qcdelapsed time: 0:38:26.37\n",
      "76000 of 144029 done of qcdelapsed time: 0:38:58.95\n",
      "77000 of 144029 done of qcdelapsed time: 0:39:28.62\n",
      "78000 of 144029 done of qcdelapsed time: 0:39:58.41\n",
      "79000 of 144029 done of qcdelapsed time: 0:40:31.51\n",
      "80000 of 144029 done of qcdelapsed time: 0:41:01.26\n",
      "81000 of 144029 done of qcdelapsed time: 0:41:31.20\n",
      "82000 of 144029 done of qcdelapsed time: 0:42:03.94\n",
      "83000 of 144029 done of qcdelapsed time: 0:42:34.07\n",
      "84000 of 144029 done of qcdelapsed time: 0:43:03.97\n",
      "85000 of 144029 done of qcdelapsed time: 0:43:36.74\n",
      "86000 of 144029 done of qcdelapsed time: 0:44:06.79\n",
      "87000 of 144029 done of qcdelapsed time: 0:44:36.60\n",
      "88000 of 144029 done of qcdelapsed time: 0:45:09.38\n",
      "89000 of 144029 done of qcdelapsed time: 0:45:39.17\n",
      "90000 of 144029 done of qcdelapsed time: 0:46:09.16\n",
      "91000 of 144029 done of qcdelapsed time: 0:46:41.86\n",
      "92000 of 144029 done of qcdelapsed time: 0:47:11.82\n",
      "93000 of 144029 done of qcdelapsed time: 0:47:41.92\n",
      "94000 of 144029 done of qcdelapsed time: 0:48:14.68\n",
      "95000 of 144029 done of qcdelapsed time: 0:48:44.63\n",
      "96000 of 144029 done of qcdelapsed time: 0:49:14.64\n",
      "97000 of 144029 done of qcdelapsed time: 0:49:47.34\n",
      "98000 of 144029 done of qcdelapsed time: 0:50:17.23\n",
      "99000 of 144029 done of qcdelapsed time: 0:50:47.09\n",
      "100000 of 144029 done of qcdelapsed time: 0:51:19.55\n",
      "101000 of 144029 done of qcdelapsed time: 0:51:48.95\n",
      "102000 of 144029 done of qcdelapsed time: 0:52:18.47\n",
      "103000 of 144029 done of qcdelapsed time: 0:52:50.77\n",
      "104000 of 144029 done of qcdelapsed time: 0:53:20.26\n",
      "105000 of 144029 done of qcdelapsed time: 0:53:49.75\n",
      "106000 of 144029 done of qcdelapsed time: 0:54:19.26\n",
      "107000 of 144029 done of qcdelapsed time: 0:54:56.11\n",
      "108000 of 144029 done of qcdelapsed time: 0:55:30.16\n",
      "109000 of 144029 done of qcdelapsed time: 0:56:02.18\n",
      "110000 of 144029 done of qcdelapsed time: 0:56:34.72\n",
      "111000 of 144029 done of qcdelapsed time: 0:57:04.41\n",
      "112000 of 144029 done of qcdelapsed time: 0:57:34.27\n",
      "113000 of 144029 done of qcdelapsed time: 0:58:06.62\n",
      "114000 of 144029 done of qcdelapsed time: 0:58:36.35\n",
      "115000 of 144029 done of qcdelapsed time: 0:59:06.27\n",
      "116000 of 144029 done of qcdelapsed time: 0:59:38.62\n",
      "117000 of 144029 done of qcdelapsed time: 1:00:08.53\n",
      "118000 of 144029 done of qcdelapsed time: 1:00:38.05\n",
      "119000 of 144029 done of qcdelapsed time: 1:01:10.27\n",
      "120000 of 144029 done of qcdelapsed time: 1:01:41.05\n",
      "121000 of 144029 done of qcdelapsed time: 1:02:15.16\n",
      "122000 of 144029 done of qcdelapsed time: 1:02:51.57\n",
      "123000 of 144029 done of qcdelapsed time: 1:03:28.18\n",
      "124000 of 144029 done of qcdelapsed time: 1:04:00.89\n",
      "125000 of 144029 done of qcdelapsed time: 1:04:34.31\n",
      "126000 of 144029 done of qcdelapsed time: 1:05:04.06\n",
      "127000 of 144029 done of qcdelapsed time: 1:05:33.68\n",
      "128000 of 144029 done of qcdelapsed time: 1:06:06.23\n",
      "129000 of 144029 done of qcdelapsed time: 1:06:35.99\n",
      "130000 of 144029 done of qcdelapsed time: 1:07:05.80\n",
      "131000 of 144029 done of qcdelapsed time: 1:07:38.72\n",
      "132000 of 144029 done of qcdelapsed time: 1:08:09.43\n",
      "133000 of 144029 done of qcdelapsed time: 1:08:39.47\n",
      "134000 of 144029 done of qcdelapsed time: 1:09:12.08\n",
      "135000 of 144029 done of qcdelapsed time: 1:09:42.14\n",
      "136000 of 144029 done of qcdelapsed time: 1:10:11.89\n",
      "137000 of 144029 done of qcdelapsed time: 1:10:45.34\n",
      "138000 of 144029 done of qcdelapsed time: 1:11:15.27\n",
      "139000 of 144029 done of qcdelapsed time: 1:11:45.31\n",
      "140000 of 144029 done of qcdelapsed time: 1:12:18.10\n",
      "141000 of 144029 done of qcdelapsed time: 1:12:47.94\n",
      "142000 of 144029 done of qcdelapsed time: 1:13:17.87\n",
      "143000 of 144029 done of qcdelapsed time: 1:13:50.55\n",
      "144000 of 144029 done of qcdelapsed time: 1:14:20.91\n",
      "0 of 1251449 done of cms_dataelapsed time: 0:00:00.03\n",
      "1000 of 1251449 done of cms_dataelapsed time: 0:00:30.12\n",
      "2000 of 1251449 done of cms_dataelapsed time: 0:00:59.69\n",
      "3000 of 1251449 done of cms_dataelapsed time: 0:01:32.43\n",
      "4000 of 1251449 done of cms_dataelapsed time: 0:02:02.50\n",
      "5000 of 1251449 done of cms_dataelapsed time: 0:02:32.60\n",
      "6000 of 1251449 done of cms_dataelapsed time: 0:03:05.36\n",
      "7000 of 1251449 done of cms_dataelapsed time: 0:03:35.47\n",
      "8000 of 1251449 done of cms_dataelapsed time: 0:04:05.38\n",
      "9000 of 1251449 done of cms_dataelapsed time: 0:04:38.24\n",
      "10000 of 1251449 done of cms_dataelapsed time: 0:05:08.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000 of 1251449 done of cms_dataelapsed time: 0:05:38.40\n",
      "12000 of 1251449 done of cms_dataelapsed time: 0:06:10.96\n",
      "13000 of 1251449 done of cms_dataelapsed time: 0:06:40.98\n",
      "14000 of 1251449 done of cms_dataelapsed time: 0:07:11.03\n",
      "15000 of 1251449 done of cms_dataelapsed time: 0:07:43.76\n",
      "16000 of 1251449 done of cms_dataelapsed time: 0:08:13.86\n",
      "17000 of 1251449 done of cms_dataelapsed time: 0:08:43.95\n",
      "18000 of 1251449 done of cms_dataelapsed time: 0:09:16.71\n",
      "19000 of 1251449 done of cms_dataelapsed time: 0:09:46.62\n",
      "20000 of 1251449 done of cms_dataelapsed time: 0:10:16.59\n",
      "21000 of 1251449 done of cms_dataelapsed time: 0:10:49.34\n",
      "22000 of 1251449 done of cms_dataelapsed time: 0:11:19.95\n",
      "23000 of 1251449 done of cms_dataelapsed time: 0:11:49.59\n",
      "24000 of 1251449 done of cms_dataelapsed time: 0:12:22.40\n",
      "25000 of 1251449 done of cms_dataelapsed time: 0:12:52.08\n",
      "26000 of 1251449 done of cms_dataelapsed time: 0:13:21.91\n",
      "27000 of 1251449 done of cms_dataelapsed time: 0:13:54.39\n",
      "28000 of 1251449 done of cms_dataelapsed time: 0:14:24.04\n",
      "29000 of 1251449 done of cms_dataelapsed time: 0:14:53.81\n",
      "30000 of 1251449 done of cms_dataelapsed time: 0:15:26.16\n",
      "31000 of 1251449 done of cms_dataelapsed time: 0:15:55.98\n",
      "32000 of 1251449 done of cms_dataelapsed time: 0:16:26.06\n",
      "33000 of 1251449 done of cms_dataelapsed time: 0:16:58.70\n",
      "34000 of 1251449 done of cms_dataelapsed time: 0:17:28.25\n",
      "35000 of 1251449 done of cms_dataelapsed time: 0:17:57.86\n",
      "36000 of 1251449 done of cms_dataelapsed time: 0:18:30.63\n",
      "37000 of 1251449 done of cms_dataelapsed time: 0:19:00.09\n",
      "38000 of 1251449 done of cms_dataelapsed time: 0:19:29.90\n",
      "39000 of 1251449 done of cms_dataelapsed time: 0:20:02.58\n",
      "40000 of 1251449 done of cms_dataelapsed time: 0:20:32.06\n",
      "41000 of 1251449 done of cms_dataelapsed time: 0:21:02.50\n",
      "42000 of 1251449 done of cms_dataelapsed time: 0:21:32.18\n",
      "43000 of 1251449 done of cms_dataelapsed time: 0:22:04.87\n",
      "44000 of 1251449 done of cms_dataelapsed time: 0:22:34.68\n",
      "45000 of 1251449 done of cms_dataelapsed time: 0:23:04.80\n",
      "46000 of 1251449 done of cms_dataelapsed time: 0:23:37.67\n",
      "47000 of 1251449 done of cms_dataelapsed time: 0:24:07.68\n",
      "48000 of 1251449 done of cms_dataelapsed time: 0:24:37.83\n",
      "49000 of 1251449 done of cms_dataelapsed time: 0:25:10.47\n",
      "50000 of 1251449 done of cms_dataelapsed time: 0:25:40.32\n",
      "51000 of 1251449 done of cms_dataelapsed time: 0:26:10.26\n",
      "52000 of 1251449 done of cms_dataelapsed time: 0:26:43.57\n",
      "53000 of 1251449 done of cms_dataelapsed time: 0:27:13.67\n",
      "54000 of 1251449 done of cms_dataelapsed time: 0:27:43.40\n",
      "55000 of 1251449 done of cms_dataelapsed time: 0:28:16.56\n",
      "56000 of 1251449 done of cms_dataelapsed time: 0:28:46.16\n",
      "57000 of 1251449 done of cms_dataelapsed time: 0:29:15.76\n",
      "58000 of 1251449 done of cms_dataelapsed time: 0:29:48.39\n",
      "59000 of 1251449 done of cms_dataelapsed time: 0:30:18.33\n",
      "60000 of 1251449 done of cms_dataelapsed time: 0:30:48.10\n",
      "61000 of 1251449 done of cms_dataelapsed time: 0:31:20.71\n",
      "62000 of 1251449 done of cms_dataelapsed time: 0:31:50.47\n",
      "63000 of 1251449 done of cms_dataelapsed time: 0:32:20.26\n",
      "64000 of 1251449 done of cms_dataelapsed time: 0:32:52.99\n",
      "65000 of 1251449 done of cms_dataelapsed time: 0:33:22.94\n",
      "66000 of 1251449 done of cms_dataelapsed time: 0:33:52.74\n",
      "67000 of 1251449 done of cms_dataelapsed time: 0:34:25.14\n",
      "68000 of 1251449 done of cms_dataelapsed time: 0:34:54.87\n",
      "69000 of 1251449 done of cms_dataelapsed time: 0:35:24.84\n",
      "70000 of 1251449 done of cms_dataelapsed time: 0:35:57.43\n",
      "71000 of 1251449 done of cms_dataelapsed time: 0:36:27.39\n",
      "72000 of 1251449 done of cms_dataelapsed time: 0:36:57.37\n",
      "73000 of 1251449 done of cms_dataelapsed time: 0:37:30.13\n",
      "74000 of 1251449 done of cms_dataelapsed time: 0:37:59.91\n",
      "75000 of 1251449 done of cms_dataelapsed time: 0:38:29.54\n",
      "76000 of 1251449 done of cms_dataelapsed time: 0:39:02.50\n",
      "77000 of 1251449 done of cms_dataelapsed time: 0:39:32.15\n",
      "78000 of 1251449 done of cms_dataelapsed time: 0:40:02.76\n",
      "79000 of 1251449 done of cms_dataelapsed time: 0:40:36.74\n",
      "80000 of 1251449 done of cms_dataelapsed time: 0:41:13.26\n",
      "81000 of 1251449 done of cms_dataelapsed time: 0:41:45.35\n",
      "82000 of 1251449 done of cms_dataelapsed time: 0:42:15.33\n",
      "83000 of 1251449 done of cms_dataelapsed time: 0:42:48.05\n",
      "84000 of 1251449 done of cms_dataelapsed time: 0:43:18.14\n",
      "85000 of 1251449 done of cms_dataelapsed time: 0:43:48.05\n",
      "86000 of 1251449 done of cms_dataelapsed time: 0:44:20.90\n",
      "87000 of 1251449 done of cms_dataelapsed time: 0:44:50.93\n",
      "88000 of 1251449 done of cms_dataelapsed time: 0:45:20.99\n",
      "89000 of 1251449 done of cms_dataelapsed time: 0:45:53.69\n",
      "90000 of 1251449 done of cms_dataelapsed time: 0:46:23.68\n",
      "91000 of 1251449 done of cms_dataelapsed time: 0:46:53.40\n",
      "92000 of 1251449 done of cms_dataelapsed time: 0:47:25.82\n",
      "93000 of 1251449 done of cms_dataelapsed time: 0:47:55.54\n",
      "94000 of 1251449 done of cms_dataelapsed time: 0:48:25.31\n",
      "95000 of 1251449 done of cms_dataelapsed time: 0:48:57.99\n",
      "96000 of 1251449 done of cms_dataelapsed time: 0:49:28.06\n",
      "97000 of 1251449 done of cms_dataelapsed time: 0:49:58.27\n",
      "98000 of 1251449 done of cms_dataelapsed time: 0:50:30.97\n",
      "99000 of 1251449 done of cms_dataelapsed time: 0:51:01.02\n",
      "100000 of 1251449 done of cms_dataelapsed time: 0:51:30.92\n",
      "101000 of 1251449 done of cms_dataelapsed time: 0:52:03.36\n",
      "102000 of 1251449 done of cms_dataelapsed time: 0:52:33.37\n",
      "103000 of 1251449 done of cms_dataelapsed time: 0:53:03.37\n",
      "104000 of 1251449 done of cms_dataelapsed time: 0:53:36.13\n",
      "105000 of 1251449 done of cms_dataelapsed time: 0:54:06.09\n",
      "106000 of 1251449 done of cms_dataelapsed time: 0:54:35.87\n",
      "107000 of 1251449 done of cms_dataelapsed time: 0:55:08.46\n",
      "108000 of 1251449 done of cms_dataelapsed time: 0:55:38.13\n",
      "109000 of 1251449 done of cms_dataelapsed time: 0:56:07.91\n",
      "110000 of 1251449 done of cms_dataelapsed time: 0:56:40.94\n",
      "111000 of 1251449 done of cms_dataelapsed time: 0:57:10.78\n",
      "112000 of 1251449 done of cms_dataelapsed time: 0:57:40.19\n",
      "113000 of 1251449 done of cms_dataelapsed time: 0:58:12.76\n",
      "114000 of 1251449 done of cms_dataelapsed time: 0:58:42.30\n",
      "115000 of 1251449 done of cms_dataelapsed time: 0:59:12.24\n",
      "116000 of 1251449 done of cms_dataelapsed time: 0:59:44.88\n",
      "117000 of 1251449 done of cms_dataelapsed time: 1:00:15.60\n",
      "118000 of 1251449 done of cms_dataelapsed time: 1:00:45.69\n",
      "119000 of 1251449 done of cms_dataelapsed time: 1:01:15.58\n",
      "120000 of 1251449 done of cms_dataelapsed time: 1:01:48.21\n",
      "121000 of 1251449 done of cms_dataelapsed time: 1:02:18.18\n",
      "122000 of 1251449 done of cms_dataelapsed time: 1:02:48.06\n",
      "123000 of 1251449 done of cms_dataelapsed time: 1:03:20.79\n",
      "124000 of 1251449 done of cms_dataelapsed time: 1:03:50.84\n",
      "125000 of 1251449 done of cms_dataelapsed time: 1:04:20.63\n",
      "126000 of 1251449 done of cms_dataelapsed time: 1:04:53.58\n",
      "127000 of 1251449 done of cms_dataelapsed time: 1:05:23.38\n",
      "128000 of 1251449 done of cms_dataelapsed time: 1:05:53.17\n",
      "129000 of 1251449 done of cms_dataelapsed time: 1:06:25.64\n",
      "130000 of 1251449 done of cms_dataelapsed time: 1:06:55.57\n",
      "131000 of 1251449 done of cms_dataelapsed time: 1:07:25.56\n",
      "132000 of 1251449 done of cms_dataelapsed time: 1:07:58.13\n",
      "133000 of 1251449 done of cms_dataelapsed time: 1:08:27.92\n",
      "134000 of 1251449 done of cms_dataelapsed time: 1:08:57.81\n",
      "135000 of 1251449 done of cms_dataelapsed time: 1:09:30.33\n",
      "136000 of 1251449 done of cms_dataelapsed time: 1:09:59.90\n",
      "137000 of 1251449 done of cms_dataelapsed time: 1:10:29.53\n",
      "138000 of 1251449 done of cms_dataelapsed time: 1:11:01.93\n",
      "139000 of 1251449 done of cms_dataelapsed time: 1:11:32.26\n",
      "140000 of 1251449 done of cms_dataelapsed time: 1:12:01.89\n",
      "141000 of 1251449 done of cms_dataelapsed time: 1:12:34.48\n",
      "142000 of 1251449 done of cms_dataelapsed time: 1:13:04.33\n",
      "143000 of 1251449 done of cms_dataelapsed time: 1:13:34.26\n",
      "144000 of 1251449 done of cms_dataelapsed time: 1:14:06.49\n",
      "145000 of 1251449 done of cms_dataelapsed time: 1:14:36.04\n",
      "146000 of 1251449 done of cms_dataelapsed time: 1:15:05.74\n",
      "147000 of 1251449 done of cms_dataelapsed time: 1:15:38.16\n",
      "148000 of 1251449 done of cms_dataelapsed time: 1:16:07.94\n",
      "149000 of 1251449 done of cms_dataelapsed time: 1:16:37.88\n",
      "150000 of 1251449 done of cms_dataelapsed time: 1:17:10.58\n",
      "151000 of 1251449 done of cms_dataelapsed time: 1:17:40.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152000 of 1251449 done of cms_dataelapsed time: 1:18:09.77\n",
      "153000 of 1251449 done of cms_dataelapsed time: 1:18:42.33\n",
      "154000 of 1251449 done of cms_dataelapsed time: 1:19:12.01\n",
      "155000 of 1251449 done of cms_dataelapsed time: 1:19:41.84\n",
      "156000 of 1251449 done of cms_dataelapsed time: 1:20:11.56\n",
      "157000 of 1251449 done of cms_dataelapsed time: 1:20:44.16\n",
      "158000 of 1251449 done of cms_dataelapsed time: 1:21:14.85\n",
      "159000 of 1251449 done of cms_dataelapsed time: 1:21:44.49\n",
      "160000 of 1251449 done of cms_dataelapsed time: 1:22:17.09\n",
      "161000 of 1251449 done of cms_dataelapsed time: 1:22:47.08\n",
      "162000 of 1251449 done of cms_dataelapsed time: 1:23:16.69\n",
      "163000 of 1251449 done of cms_dataelapsed time: 1:23:49.21\n",
      "164000 of 1251449 done of cms_dataelapsed time: 1:24:19.27\n",
      "165000 of 1251449 done of cms_dataelapsed time: 1:24:48.97\n",
      "166000 of 1251449 done of cms_dataelapsed time: 1:25:21.54\n",
      "167000 of 1251449 done of cms_dataelapsed time: 1:25:51.55\n",
      "168000 of 1251449 done of cms_dataelapsed time: 1:26:21.66\n",
      "169000 of 1251449 done of cms_dataelapsed time: 1:26:55.17\n",
      "170000 of 1251449 done of cms_dataelapsed time: 1:27:25.04\n",
      "171000 of 1251449 done of cms_dataelapsed time: 1:27:55.26\n",
      "172000 of 1251449 done of cms_dataelapsed time: 1:28:28.33\n",
      "173000 of 1251449 done of cms_dataelapsed time: 1:28:58.32\n",
      "174000 of 1251449 done of cms_dataelapsed time: 1:29:28.09\n",
      "175000 of 1251449 done of cms_dataelapsed time: 1:30:00.62\n",
      "176000 of 1251449 done of cms_dataelapsed time: 1:30:30.57\n",
      "177000 of 1251449 done of cms_dataelapsed time: 1:31:00.22\n",
      "178000 of 1251449 done of cms_dataelapsed time: 1:31:32.86\n",
      "179000 of 1251449 done of cms_dataelapsed time: 1:32:02.76\n",
      "180000 of 1251449 done of cms_dataelapsed time: 1:32:34.16\n",
      "181000 of 1251449 done of cms_dataelapsed time: 1:33:11.29\n",
      "182000 of 1251449 done of cms_dataelapsed time: 1:33:44.48\n",
      "183000 of 1251449 done of cms_dataelapsed time: 1:34:15.68\n",
      "184000 of 1251449 done of cms_dataelapsed time: 1:34:48.66\n",
      "185000 of 1251449 done of cms_dataelapsed time: 1:35:18.66\n",
      "186000 of 1251449 done of cms_dataelapsed time: 1:35:48.43\n",
      "187000 of 1251449 done of cms_dataelapsed time: 1:36:21.33\n",
      "188000 of 1251449 done of cms_dataelapsed time: 1:36:51.30\n",
      "189000 of 1251449 done of cms_dataelapsed time: 1:37:21.22\n",
      "190000 of 1251449 done of cms_dataelapsed time: 1:37:53.82\n",
      "191000 of 1251449 done of cms_dataelapsed time: 1:38:23.47\n",
      "192000 of 1251449 done of cms_dataelapsed time: 1:38:53.34\n",
      "193000 of 1251449 done of cms_dataelapsed time: 1:39:26.03\n",
      "194000 of 1251449 done of cms_dataelapsed time: 1:39:55.69\n",
      "195000 of 1251449 done of cms_dataelapsed time: 1:40:25.62\n",
      "196000 of 1251449 done of cms_dataelapsed time: 1:40:55.39\n",
      "197000 of 1251449 done of cms_dataelapsed time: 1:41:27.74\n",
      "198000 of 1251449 done of cms_dataelapsed time: 1:41:58.01\n",
      "199000 of 1251449 done of cms_dataelapsed time: 1:42:27.88\n",
      "200000 of 1251449 done of cms_dataelapsed time: 1:43:00.76\n",
      "201000 of 1251449 done of cms_dataelapsed time: 1:43:30.71\n",
      "202000 of 1251449 done of cms_dataelapsed time: 1:44:00.70\n",
      "203000 of 1251449 done of cms_dataelapsed time: 1:44:33.33\n",
      "204000 of 1251449 done of cms_dataelapsed time: 1:45:03.32\n",
      "205000 of 1251449 done of cms_dataelapsed time: 1:45:33.12\n",
      "206000 of 1251449 done of cms_dataelapsed time: 1:46:05.77\n",
      "207000 of 1251449 done of cms_dataelapsed time: 1:46:35.77\n",
      "208000 of 1251449 done of cms_dataelapsed time: 1:47:05.63\n",
      "209000 of 1251449 done of cms_dataelapsed time: 1:47:37.93\n",
      "210000 of 1251449 done of cms_dataelapsed time: 1:48:07.77\n",
      "211000 of 1251449 done of cms_dataelapsed time: 1:48:37.42\n",
      "212000 of 1251449 done of cms_dataelapsed time: 1:49:09.92\n",
      "213000 of 1251449 done of cms_dataelapsed time: 1:49:39.93\n",
      "214000 of 1251449 done of cms_dataelapsed time: 1:50:09.95\n",
      "215000 of 1251449 done of cms_dataelapsed time: 1:50:42.63\n",
      "216000 of 1251449 done of cms_dataelapsed time: 1:51:12.43\n",
      "217000 of 1251449 done of cms_dataelapsed time: 1:51:42.24\n",
      "218000 of 1251449 done of cms_dataelapsed time: 1:52:14.82\n",
      "219000 of 1251449 done of cms_dataelapsed time: 1:52:44.76\n",
      "220000 of 1251449 done of cms_dataelapsed time: 1:53:14.57\n",
      "221000 of 1251449 done of cms_dataelapsed time: 1:53:47.23\n",
      "222000 of 1251449 done of cms_dataelapsed time: 1:54:17.00\n",
      "223000 of 1251449 done of cms_dataelapsed time: 1:54:46.97\n",
      "224000 of 1251449 done of cms_dataelapsed time: 1:55:19.54\n",
      "225000 of 1251449 done of cms_dataelapsed time: 1:55:49.65\n",
      "226000 of 1251449 done of cms_dataelapsed time: 1:56:19.56\n",
      "227000 of 1251449 done of cms_dataelapsed time: 1:56:52.86\n",
      "228000 of 1251449 done of cms_dataelapsed time: 1:57:22.70\n",
      "229000 of 1251449 done of cms_dataelapsed time: 1:57:52.66\n",
      "230000 of 1251449 done of cms_dataelapsed time: 1:58:25.18\n",
      "231000 of 1251449 done of cms_dataelapsed time: 1:58:54.84\n",
      "232000 of 1251449 done of cms_dataelapsed time: 1:59:24.91\n",
      "233000 of 1251449 done of cms_dataelapsed time: 1:59:55.52\n",
      "234000 of 1251449 done of cms_dataelapsed time: 2:00:28.26\n",
      "235000 of 1251449 done of cms_dataelapsed time: 2:00:58.22\n",
      "236000 of 1251449 done of cms_dataelapsed time: 2:01:28.06\n",
      "237000 of 1251449 done of cms_dataelapsed time: 2:02:00.98\n",
      "238000 of 1251449 done of cms_dataelapsed time: 2:02:31.20\n",
      "239000 of 1251449 done of cms_dataelapsed time: 2:03:01.05\n",
      "240000 of 1251449 done of cms_dataelapsed time: 2:03:33.93\n",
      "241000 of 1251449 done of cms_dataelapsed time: 2:04:03.89\n",
      "242000 of 1251449 done of cms_dataelapsed time: 2:04:33.77\n",
      "243000 of 1251449 done of cms_dataelapsed time: 2:05:06.58\n",
      "244000 of 1251449 done of cms_dataelapsed time: 2:05:36.49\n",
      "245000 of 1251449 done of cms_dataelapsed time: 2:06:06.48\n",
      "246000 of 1251449 done of cms_dataelapsed time: 2:06:40.22\n",
      "247000 of 1251449 done of cms_dataelapsed time: 2:07:10.27\n",
      "248000 of 1251449 done of cms_dataelapsed time: 2:07:40.16\n",
      "249000 of 1251449 done of cms_dataelapsed time: 2:08:12.72\n",
      "250000 of 1251449 done of cms_dataelapsed time: 2:08:42.51\n",
      "251000 of 1251449 done of cms_dataelapsed time: 2:09:12.45\n",
      "252000 of 1251449 done of cms_dataelapsed time: 2:09:45.24\n",
      "253000 of 1251449 done of cms_dataelapsed time: 2:10:15.20\n",
      "254000 of 1251449 done of cms_dataelapsed time: 2:10:44.99\n",
      "255000 of 1251449 done of cms_dataelapsed time: 2:11:17.75\n",
      "256000 of 1251449 done of cms_dataelapsed time: 2:11:48.07\n",
      "257000 of 1251449 done of cms_dataelapsed time: 2:12:17.76\n",
      "258000 of 1251449 done of cms_dataelapsed time: 2:12:50.56\n",
      "259000 of 1251449 done of cms_dataelapsed time: 2:13:20.55\n",
      "260000 of 1251449 done of cms_dataelapsed time: 2:13:50.68\n",
      "261000 of 1251449 done of cms_dataelapsed time: 2:14:23.41\n",
      "262000 of 1251449 done of cms_dataelapsed time: 2:14:53.31\n",
      "263000 of 1251449 done of cms_dataelapsed time: 2:15:23.41\n",
      "264000 of 1251449 done of cms_dataelapsed time: 2:15:56.22\n",
      "265000 of 1251449 done of cms_dataelapsed time: 2:16:26.24\n",
      "266000 of 1251449 done of cms_dataelapsed time: 2:16:56.30\n",
      "267000 of 1251449 done of cms_dataelapsed time: 2:17:29.14\n",
      "268000 of 1251449 done of cms_dataelapsed time: 2:17:59.10\n",
      "269000 of 1251449 done of cms_dataelapsed time: 2:18:29.06\n",
      "270000 of 1251449 done of cms_dataelapsed time: 2:19:01.84\n",
      "271000 of 1251449 done of cms_dataelapsed time: 2:19:31.71\n",
      "272000 of 1251449 done of cms_dataelapsed time: 2:20:01.86\n",
      "273000 of 1251449 done of cms_dataelapsed time: 2:20:31.77\n",
      "274000 of 1251449 done of cms_dataelapsed time: 2:21:05.16\n",
      "275000 of 1251449 done of cms_dataelapsed time: 2:21:35.19\n",
      "276000 of 1251449 done of cms_dataelapsed time: 2:22:04.81\n",
      "277000 of 1251449 done of cms_dataelapsed time: 2:22:37.57\n",
      "278000 of 1251449 done of cms_dataelapsed time: 2:23:07.54\n",
      "279000 of 1251449 done of cms_dataelapsed time: 2:23:37.17\n",
      "280000 of 1251449 done of cms_dataelapsed time: 2:24:09.65\n",
      "281000 of 1251449 done of cms_dataelapsed time: 2:24:39.57\n",
      "282000 of 1251449 done of cms_dataelapsed time: 2:25:09.39\n",
      "283000 of 1251449 done of cms_dataelapsed time: 2:25:42.15\n",
      "284000 of 1251449 done of cms_dataelapsed time: 2:26:12.18\n",
      "285000 of 1251449 done of cms_dataelapsed time: 2:26:42.87\n",
      "286000 of 1251449 done of cms_dataelapsed time: 2:27:15.66\n",
      "287000 of 1251449 done of cms_dataelapsed time: 2:27:45.71\n",
      "288000 of 1251449 done of cms_dataelapsed time: 2:28:16.35\n",
      "289000 of 1251449 done of cms_dataelapsed time: 2:28:48.84\n",
      "290000 of 1251449 done of cms_dataelapsed time: 2:29:18.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291000 of 1251449 done of cms_dataelapsed time: 2:29:48.13\n",
      "292000 of 1251449 done of cms_dataelapsed time: 2:30:20.86\n",
      "293000 of 1251449 done of cms_dataelapsed time: 2:30:50.29\n",
      "294000 of 1251449 done of cms_dataelapsed time: 2:31:19.87\n",
      "295000 of 1251449 done of cms_dataelapsed time: 2:31:52.33\n",
      "296000 of 1251449 done of cms_dataelapsed time: 2:32:21.99\n",
      "297000 of 1251449 done of cms_dataelapsed time: 2:32:55.67\n",
      "298000 of 1251449 done of cms_dataelapsed time: 2:33:32.13\n",
      "299000 of 1251449 done of cms_dataelapsed time: 2:34:04.62\n",
      "300000 of 1251449 done of cms_dataelapsed time: 2:34:34.49\n",
      "301000 of 1251449 done of cms_dataelapsed time: 2:35:07.34\n",
      "302000 of 1251449 done of cms_dataelapsed time: 2:35:36.91\n",
      "303000 of 1251449 done of cms_dataelapsed time: 2:36:06.53\n",
      "304000 of 1251449 done of cms_dataelapsed time: 2:36:39.01\n",
      "305000 of 1251449 done of cms_dataelapsed time: 2:37:08.67\n",
      "306000 of 1251449 done of cms_dataelapsed time: 2:37:38.35\n",
      "307000 of 1251449 done of cms_dataelapsed time: 2:38:10.96\n",
      "308000 of 1251449 done of cms_dataelapsed time: 2:38:40.60\n",
      "309000 of 1251449 done of cms_dataelapsed time: 2:39:10.36\n",
      "310000 of 1251449 done of cms_dataelapsed time: 2:39:42.82\n",
      "311000 of 1251449 done of cms_dataelapsed time: 2:40:12.23\n",
      "312000 of 1251449 done of cms_dataelapsed time: 2:40:41.65\n",
      "313000 of 1251449 done of cms_dataelapsed time: 2:41:11.11\n",
      "314000 of 1251449 done of cms_dataelapsed time: 2:41:43.98\n",
      "315000 of 1251449 done of cms_dataelapsed time: 2:42:13.38\n",
      "316000 of 1251449 done of cms_dataelapsed time: 2:42:43.01\n",
      "317000 of 1251449 done of cms_dataelapsed time: 2:43:15.30\n",
      "318000 of 1251449 done of cms_dataelapsed time: 2:43:45.09\n",
      "319000 of 1251449 done of cms_dataelapsed time: 2:44:14.61\n",
      "320000 of 1251449 done of cms_dataelapsed time: 2:44:46.96\n",
      "321000 of 1251449 done of cms_dataelapsed time: 2:45:16.44\n",
      "322000 of 1251449 done of cms_dataelapsed time: 2:45:45.97\n",
      "323000 of 1251449 done of cms_dataelapsed time: 2:46:18.23\n",
      "324000 of 1251449 done of cms_dataelapsed time: 2:46:47.80\n",
      "325000 of 1251449 done of cms_dataelapsed time: 2:47:17.14\n",
      "326000 of 1251449 done of cms_dataelapsed time: 2:47:49.16\n",
      "327000 of 1251449 done of cms_dataelapsed time: 2:48:18.81\n",
      "328000 of 1251449 done of cms_dataelapsed time: 2:48:48.29\n",
      "329000 of 1251449 done of cms_dataelapsed time: 2:49:20.54\n",
      "330000 of 1251449 done of cms_dataelapsed time: 2:49:50.23\n",
      "331000 of 1251449 done of cms_dataelapsed time: 2:50:19.64\n",
      "332000 of 1251449 done of cms_dataelapsed time: 2:50:51.94\n",
      "333000 of 1251449 done of cms_dataelapsed time: 2:51:21.31\n",
      "334000 of 1251449 done of cms_dataelapsed time: 2:51:50.99\n",
      "335000 of 1251449 done of cms_dataelapsed time: 2:52:23.37\n",
      "336000 of 1251449 done of cms_dataelapsed time: 2:52:52.82\n",
      "337000 of 1251449 done of cms_dataelapsed time: 2:53:22.45\n",
      "338000 of 1251449 done of cms_dataelapsed time: 2:53:54.75\n",
      "339000 of 1251449 done of cms_dataelapsed time: 2:54:24.30\n",
      "340000 of 1251449 done of cms_dataelapsed time: 2:54:53.91\n",
      "341000 of 1251449 done of cms_dataelapsed time: 2:55:26.06\n",
      "342000 of 1251449 done of cms_dataelapsed time: 2:55:55.62\n",
      "343000 of 1251449 done of cms_dataelapsed time: 2:56:25.23\n",
      "344000 of 1251449 done of cms_dataelapsed time: 2:56:58.07\n",
      "345000 of 1251449 done of cms_dataelapsed time: 2:57:27.63\n",
      "346000 of 1251449 done of cms_dataelapsed time: 2:57:57.30\n",
      "347000 of 1251449 done of cms_dataelapsed time: 2:58:29.54\n",
      "348000 of 1251449 done of cms_dataelapsed time: 2:58:58.87\n",
      "349000 of 1251449 done of cms_dataelapsed time: 2:59:28.46\n",
      "350000 of 1251449 done of cms_dataelapsed time: 2:59:58.68\n",
      "351000 of 1251449 done of cms_dataelapsed time: 3:00:30.87\n",
      "352000 of 1251449 done of cms_dataelapsed time: 3:01:00.29\n",
      "353000 of 1251449 done of cms_dataelapsed time: 3:01:29.61\n",
      "354000 of 1251449 done of cms_dataelapsed time: 3:02:01.86\n",
      "355000 of 1251449 done of cms_dataelapsed time: 3:02:31.51\n",
      "356000 of 1251449 done of cms_dataelapsed time: 3:03:00.94\n",
      "357000 of 1251449 done of cms_dataelapsed time: 3:03:33.09\n",
      "358000 of 1251449 done of cms_dataelapsed time: 3:04:02.78\n",
      "359000 of 1251449 done of cms_dataelapsed time: 3:04:32.05\n",
      "360000 of 1251449 done of cms_dataelapsed time: 3:05:04.51\n",
      "361000 of 1251449 done of cms_dataelapsed time: 3:05:34.15\n",
      "362000 of 1251449 done of cms_dataelapsed time: 3:06:03.69\n",
      "363000 of 1251449 done of cms_dataelapsed time: 3:06:36.00\n",
      "364000 of 1251449 done of cms_dataelapsed time: 3:07:05.58\n",
      "365000 of 1251449 done of cms_dataelapsed time: 3:07:34.95\n",
      "366000 of 1251449 done of cms_dataelapsed time: 3:08:07.11\n",
      "367000 of 1251449 done of cms_dataelapsed time: 3:08:36.47\n",
      "368000 of 1251449 done of cms_dataelapsed time: 3:09:05.89\n",
      "369000 of 1251449 done of cms_dataelapsed time: 3:09:38.13\n",
      "370000 of 1251449 done of cms_dataelapsed time: 3:10:07.49\n",
      "371000 of 1251449 done of cms_dataelapsed time: 3:10:36.81\n",
      "372000 of 1251449 done of cms_dataelapsed time: 3:11:09.05\n",
      "373000 of 1251449 done of cms_dataelapsed time: 3:11:38.47\n",
      "374000 of 1251449 done of cms_dataelapsed time: 3:12:08.56\n",
      "375000 of 1251449 done of cms_dataelapsed time: 3:12:40.79\n",
      "376000 of 1251449 done of cms_dataelapsed time: 3:13:10.13\n",
      "377000 of 1251449 done of cms_dataelapsed time: 3:13:39.66\n",
      "378000 of 1251449 done of cms_dataelapsed time: 3:14:11.87\n",
      "379000 of 1251449 done of cms_dataelapsed time: 3:14:41.25\n",
      "380000 of 1251449 done of cms_dataelapsed time: 3:15:10.92\n",
      "381000 of 1251449 done of cms_dataelapsed time: 3:15:43.21\n",
      "382000 of 1251449 done of cms_dataelapsed time: 3:16:12.42\n",
      "383000 of 1251449 done of cms_dataelapsed time: 3:16:41.94\n",
      "384000 of 1251449 done of cms_dataelapsed time: 3:17:14.10\n",
      "385000 of 1251449 done of cms_dataelapsed time: 3:17:43.33\n",
      "386000 of 1251449 done of cms_dataelapsed time: 3:18:12.81\n",
      "387000 of 1251449 done of cms_dataelapsed time: 3:18:45.10\n",
      "388000 of 1251449 done of cms_dataelapsed time: 3:19:14.39\n",
      "389000 of 1251449 done of cms_dataelapsed time: 3:19:44.25\n",
      "390000 of 1251449 done of cms_dataelapsed time: 3:20:13.81\n",
      "391000 of 1251449 done of cms_dataelapsed time: 3:20:45.89\n",
      "392000 of 1251449 done of cms_dataelapsed time: 3:21:16.33\n",
      "393000 of 1251449 done of cms_dataelapsed time: 3:21:45.73\n",
      "394000 of 1251449 done of cms_dataelapsed time: 3:22:18.25\n",
      "395000 of 1251449 done of cms_dataelapsed time: 3:22:48.17\n",
      "396000 of 1251449 done of cms_dataelapsed time: 3:23:17.52\n",
      "397000 of 1251449 done of cms_dataelapsed time: 3:23:49.79\n",
      "398000 of 1251449 done of cms_dataelapsed time: 3:24:19.18\n",
      "399000 of 1251449 done of cms_dataelapsed time: 3:24:48.45\n",
      "400000 of 1251449 done of cms_dataelapsed time: 3:25:20.52\n",
      "401000 of 1251449 done of cms_dataelapsed time: 3:25:50.14\n",
      "402000 of 1251449 done of cms_dataelapsed time: 3:26:19.71\n",
      "403000 of 1251449 done of cms_dataelapsed time: 3:26:52.32\n",
      "404000 of 1251449 done of cms_dataelapsed time: 3:27:22.24\n",
      "405000 of 1251449 done of cms_dataelapsed time: 3:27:52.38\n",
      "406000 of 1251449 done of cms_dataelapsed time: 3:28:24.73\n",
      "407000 of 1251449 done of cms_dataelapsed time: 3:28:54.18\n",
      "408000 of 1251449 done of cms_dataelapsed time: 3:29:23.80\n",
      "409000 of 1251449 done of cms_dataelapsed time: 3:29:56.01\n",
      "410000 of 1251449 done of cms_dataelapsed time: 3:30:25.69\n",
      "411000 of 1251449 done of cms_dataelapsed time: 3:30:55.32\n",
      "412000 of 1251449 done of cms_dataelapsed time: 3:31:27.49\n",
      "413000 of 1251449 done of cms_dataelapsed time: 3:31:56.85\n",
      "414000 of 1251449 done of cms_dataelapsed time: 3:32:26.33\n",
      "415000 of 1251449 done of cms_dataelapsed time: 3:33:02.22\n",
      "416000 of 1251449 done of cms_dataelapsed time: 3:33:35.06\n",
      "417000 of 1251449 done of cms_dataelapsed time: 3:34:08.54\n",
      "418000 of 1251449 done of cms_dataelapsed time: 3:34:40.64\n",
      "419000 of 1251449 done of cms_dataelapsed time: 3:35:10.02\n",
      "420000 of 1251449 done of cms_dataelapsed time: 3:35:39.73\n",
      "421000 of 1251449 done of cms_dataelapsed time: 3:36:11.96\n",
      "422000 of 1251449 done of cms_dataelapsed time: 3:36:41.36\n",
      "423000 of 1251449 done of cms_dataelapsed time: 3:37:10.86\n",
      "424000 of 1251449 done of cms_dataelapsed time: 3:37:43.06\n",
      "425000 of 1251449 done of cms_dataelapsed time: 3:38:12.31\n",
      "426000 of 1251449 done of cms_dataelapsed time: 3:38:41.86\n",
      "427000 of 1251449 done of cms_dataelapsed time: 3:39:11.12\n",
      "428000 of 1251449 done of cms_dataelapsed time: 3:39:43.12\n",
      "429000 of 1251449 done of cms_dataelapsed time: 3:40:12.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430000 of 1251449 done of cms_dataelapsed time: 3:40:41.84\n",
      "431000 of 1251449 done of cms_dataelapsed time: 3:41:13.84\n",
      "432000 of 1251449 done of cms_dataelapsed time: 3:41:43.31\n",
      "433000 of 1251449 done of cms_dataelapsed time: 3:42:13.02\n",
      "434000 of 1251449 done of cms_dataelapsed time: 3:42:45.04\n",
      "435000 of 1251449 done of cms_dataelapsed time: 3:43:14.51\n",
      "436000 of 1251449 done of cms_dataelapsed time: 3:43:43.77\n",
      "437000 of 1251449 done of cms_dataelapsed time: 3:44:15.80\n",
      "438000 of 1251449 done of cms_dataelapsed time: 3:44:45.21\n",
      "439000 of 1251449 done of cms_dataelapsed time: 3:45:14.42\n",
      "440000 of 1251449 done of cms_dataelapsed time: 3:45:46.43\n",
      "441000 of 1251449 done of cms_dataelapsed time: 3:46:15.81\n",
      "442000 of 1251449 done of cms_dataelapsed time: 3:46:45.09\n",
      "443000 of 1251449 done of cms_dataelapsed time: 3:47:17.20\n",
      "444000 of 1251449 done of cms_dataelapsed time: 3:47:46.64\n",
      "445000 of 1251449 done of cms_dataelapsed time: 3:48:16.24\n",
      "446000 of 1251449 done of cms_dataelapsed time: 3:48:48.63\n",
      "447000 of 1251449 done of cms_dataelapsed time: 3:49:18.17\n",
      "448000 of 1251449 done of cms_dataelapsed time: 3:49:47.73\n",
      "449000 of 1251449 done of cms_dataelapsed time: 3:50:20.01\n",
      "450000 of 1251449 done of cms_dataelapsed time: 3:50:49.60\n",
      "451000 of 1251449 done of cms_dataelapsed time: 3:51:19.19\n",
      "452000 of 1251449 done of cms_dataelapsed time: 3:51:51.55\n",
      "453000 of 1251449 done of cms_dataelapsed time: 3:52:21.00\n",
      "454000 of 1251449 done of cms_dataelapsed time: 3:52:50.74\n",
      "455000 of 1251449 done of cms_dataelapsed time: 3:53:23.02\n",
      "456000 of 1251449 done of cms_dataelapsed time: 3:53:52.40\n",
      "457000 of 1251449 done of cms_dataelapsed time: 3:54:21.89\n",
      "458000 of 1251449 done of cms_dataelapsed time: 3:54:54.16\n",
      "459000 of 1251449 done of cms_dataelapsed time: 3:55:23.36\n",
      "460000 of 1251449 done of cms_dataelapsed time: 3:55:52.85\n",
      "461000 of 1251449 done of cms_dataelapsed time: 3:56:24.99\n",
      "462000 of 1251449 done of cms_dataelapsed time: 3:56:54.26\n",
      "463000 of 1251449 done of cms_dataelapsed time: 3:57:24.31\n",
      "464000 of 1251449 done of cms_dataelapsed time: 3:57:56.61\n",
      "465000 of 1251449 done of cms_dataelapsed time: 3:58:26.23\n",
      "466000 of 1251449 done of cms_dataelapsed time: 3:58:56.00\n",
      "467000 of 1251449 done of cms_dataelapsed time: 3:59:25.39\n",
      "468000 of 1251449 done of cms_dataelapsed time: 3:59:58.59\n",
      "469000 of 1251449 done of cms_dataelapsed time: 4:00:28.52\n",
      "470000 of 1251449 done of cms_dataelapsed time: 4:00:58.02\n",
      "471000 of 1251449 done of cms_dataelapsed time: 4:01:30.38\n",
      "472000 of 1251449 done of cms_dataelapsed time: 4:02:00.19\n",
      "473000 of 1251449 done of cms_dataelapsed time: 4:02:29.94\n",
      "474000 of 1251449 done of cms_dataelapsed time: 4:03:02.43\n",
      "475000 of 1251449 done of cms_dataelapsed time: 4:03:31.95\n",
      "476000 of 1251449 done of cms_dataelapsed time: 4:04:01.62\n",
      "477000 of 1251449 done of cms_dataelapsed time: 4:04:34.20\n",
      "478000 of 1251449 done of cms_dataelapsed time: 4:05:04.32\n",
      "479000 of 1251449 done of cms_dataelapsed time: 4:05:33.97\n",
      "480000 of 1251449 done of cms_dataelapsed time: 4:06:06.52\n",
      "481000 of 1251449 done of cms_dataelapsed time: 4:06:36.28\n",
      "482000 of 1251449 done of cms_dataelapsed time: 4:07:05.93\n",
      "483000 of 1251449 done of cms_dataelapsed time: 4:07:38.53\n",
      "484000 of 1251449 done of cms_dataelapsed time: 4:08:08.25\n",
      "485000 of 1251449 done of cms_dataelapsed time: 4:08:37.87\n",
      "486000 of 1251449 done of cms_dataelapsed time: 4:09:10.39\n",
      "487000 of 1251449 done of cms_dataelapsed time: 4:09:39.74\n",
      "488000 of 1251449 done of cms_dataelapsed time: 4:10:09.21\n",
      "489000 of 1251449 done of cms_dataelapsed time: 4:10:41.39\n",
      "490000 of 1251449 done of cms_dataelapsed time: 4:11:10.99\n",
      "491000 of 1251449 done of cms_dataelapsed time: 4:11:40.84\n",
      "492000 of 1251449 done of cms_dataelapsed time: 4:12:13.82\n",
      "493000 of 1251449 done of cms_dataelapsed time: 4:12:43.24\n",
      "494000 of 1251449 done of cms_dataelapsed time: 4:13:13.03\n",
      "495000 of 1251449 done of cms_dataelapsed time: 4:13:45.54\n",
      "496000 of 1251449 done of cms_dataelapsed time: 4:14:15.10\n",
      "497000 of 1251449 done of cms_dataelapsed time: 4:14:44.76\n",
      "498000 of 1251449 done of cms_dataelapsed time: 4:15:17.45\n",
      "499000 of 1251449 done of cms_dataelapsed time: 4:15:47.02\n",
      "500000 of 1251449 done of cms_dataelapsed time: 4:16:16.81\n",
      "501000 of 1251449 done of cms_dataelapsed time: 4:16:49.33\n",
      "502000 of 1251449 done of cms_dataelapsed time: 4:17:18.89\n",
      "503000 of 1251449 done of cms_dataelapsed time: 4:17:48.43\n",
      "504000 of 1251449 done of cms_dataelapsed time: 4:18:17.67\n",
      "505000 of 1251449 done of cms_dataelapsed time: 4:18:49.77\n",
      "506000 of 1251449 done of cms_dataelapsed time: 4:19:19.23\n",
      "507000 of 1251449 done of cms_dataelapsed time: 4:19:48.60\n",
      "508000 of 1251449 done of cms_dataelapsed time: 4:20:20.74\n",
      "509000 of 1251449 done of cms_dataelapsed time: 4:20:50.05\n",
      "510000 of 1251449 done of cms_dataelapsed time: 4:21:20.11\n",
      "511000 of 1251449 done of cms_dataelapsed time: 4:21:52.15\n",
      "512000 of 1251449 done of cms_dataelapsed time: 4:22:21.55\n",
      "513000 of 1251449 done of cms_dataelapsed time: 4:22:50.81\n",
      "514000 of 1251449 done of cms_dataelapsed time: 4:23:23.15\n",
      "515000 of 1251449 done of cms_dataelapsed time: 4:23:52.86\n",
      "516000 of 1251449 done of cms_dataelapsed time: 4:24:22.74\n",
      "517000 of 1251449 done of cms_dataelapsed time: 4:24:55.02\n",
      "518000 of 1251449 done of cms_dataelapsed time: 4:25:24.63\n",
      "519000 of 1251449 done of cms_dataelapsed time: 4:25:54.22\n",
      "520000 of 1251449 done of cms_dataelapsed time: 4:26:26.90\n",
      "521000 of 1251449 done of cms_dataelapsed time: 4:26:56.55\n",
      "522000 of 1251449 done of cms_dataelapsed time: 4:27:26.65\n",
      "523000 of 1251449 done of cms_dataelapsed time: 4:27:59.44\n",
      "524000 of 1251449 done of cms_dataelapsed time: 4:28:29.26\n",
      "525000 of 1251449 done of cms_dataelapsed time: 4:28:59.09\n",
      "526000 of 1251449 done of cms_dataelapsed time: 4:29:31.81\n",
      "527000 of 1251449 done of cms_dataelapsed time: 4:30:01.83\n",
      "528000 of 1251449 done of cms_dataelapsed time: 4:30:31.88\n",
      "529000 of 1251449 done of cms_dataelapsed time: 4:31:04.80\n",
      "530000 of 1251449 done of cms_dataelapsed time: 4:31:34.76\n",
      "531000 of 1251449 done of cms_dataelapsed time: 4:32:04.62\n",
      "532000 of 1251449 done of cms_dataelapsed time: 4:32:39.00\n",
      "533000 of 1251449 done of cms_dataelapsed time: 4:33:13.10\n",
      "534000 of 1251449 done of cms_dataelapsed time: 4:33:46.45\n",
      "535000 of 1251449 done of cms_dataelapsed time: 4:34:20.05\n",
      "536000 of 1251449 done of cms_dataelapsed time: 4:34:49.98\n",
      "537000 of 1251449 done of cms_dataelapsed time: 4:35:19.91\n",
      "538000 of 1251449 done of cms_dataelapsed time: 4:35:52.52\n",
      "539000 of 1251449 done of cms_dataelapsed time: 4:36:22.24\n",
      "540000 of 1251449 done of cms_dataelapsed time: 4:36:52.16\n",
      "541000 of 1251449 done of cms_dataelapsed time: 4:37:24.86\n",
      "542000 of 1251449 done of cms_dataelapsed time: 4:37:55.85\n",
      "543000 of 1251449 done of cms_dataelapsed time: 4:38:25.92\n",
      "544000 of 1251449 done of cms_dataelapsed time: 4:38:55.74\n",
      "545000 of 1251449 done of cms_dataelapsed time: 4:39:28.30\n",
      "546000 of 1251449 done of cms_dataelapsed time: 4:39:58.33\n",
      "547000 of 1251449 done of cms_dataelapsed time: 4:40:28.25\n",
      "548000 of 1251449 done of cms_dataelapsed time: 4:41:00.94\n",
      "549000 of 1251449 done of cms_dataelapsed time: 4:41:30.88\n",
      "550000 of 1251449 done of cms_dataelapsed time: 4:42:00.64\n",
      "551000 of 1251449 done of cms_dataelapsed time: 4:42:33.69\n",
      "552000 of 1251449 done of cms_dataelapsed time: 4:43:03.74\n",
      "553000 of 1251449 done of cms_dataelapsed time: 4:43:33.73\n",
      "554000 of 1251449 done of cms_dataelapsed time: 4:44:06.48\n",
      "555000 of 1251449 done of cms_dataelapsed time: 4:44:36.47\n",
      "556000 of 1251449 done of cms_dataelapsed time: 4:45:06.57\n",
      "557000 of 1251449 done of cms_dataelapsed time: 4:45:39.20\n",
      "558000 of 1251449 done of cms_dataelapsed time: 4:46:09.28\n",
      "559000 of 1251449 done of cms_dataelapsed time: 4:46:39.35\n",
      "560000 of 1251449 done of cms_dataelapsed time: 4:47:12.20\n",
      "561000 of 1251449 done of cms_dataelapsed time: 4:47:42.18\n",
      "562000 of 1251449 done of cms_dataelapsed time: 4:48:12.16\n",
      "563000 of 1251449 done of cms_dataelapsed time: 4:48:44.98\n",
      "564000 of 1251449 done of cms_dataelapsed time: 4:49:14.87\n",
      "565000 of 1251449 done of cms_dataelapsed time: 4:49:44.90\n",
      "566000 of 1251449 done of cms_dataelapsed time: 4:50:17.94\n",
      "567000 of 1251449 done of cms_dataelapsed time: 4:50:47.58\n",
      "568000 of 1251449 done of cms_dataelapsed time: 4:51:17.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569000 of 1251449 done of cms_dataelapsed time: 4:51:50.33\n",
      "570000 of 1251449 done of cms_dataelapsed time: 4:52:20.13\n",
      "571000 of 1251449 done of cms_dataelapsed time: 4:52:50.11\n",
      "572000 of 1251449 done of cms_dataelapsed time: 4:53:22.87\n",
      "573000 of 1251449 done of cms_dataelapsed time: 4:53:55.00\n",
      "574000 of 1251449 done of cms_dataelapsed time: 4:54:27.57\n",
      "575000 of 1251449 done of cms_dataelapsed time: 4:55:00.36\n",
      "576000 of 1251449 done of cms_dataelapsed time: 4:55:30.22\n",
      "577000 of 1251449 done of cms_dataelapsed time: 4:56:00.26\n",
      "578000 of 1251449 done of cms_dataelapsed time: 4:56:32.92\n",
      "579000 of 1251449 done of cms_dataelapsed time: 4:57:02.84\n",
      "580000 of 1251449 done of cms_dataelapsed time: 4:57:32.92\n",
      "581000 of 1251449 done of cms_dataelapsed time: 4:58:02.67\n",
      "582000 of 1251449 done of cms_dataelapsed time: 4:58:37.90\n",
      "583000 of 1251449 done of cms_dataelapsed time: 4:59:12.73\n",
      "584000 of 1251449 done of cms_dataelapsed time: 4:59:47.20\n",
      "585000 of 1251449 done of cms_dataelapsed time: 5:00:20.50\n",
      "586000 of 1251449 done of cms_dataelapsed time: 5:00:50.40\n",
      "587000 of 1251449 done of cms_dataelapsed time: 5:01:20.25\n",
      "588000 of 1251449 done of cms_dataelapsed time: 5:01:52.98\n",
      "589000 of 1251449 done of cms_dataelapsed time: 5:02:23.22\n",
      "590000 of 1251449 done of cms_dataelapsed time: 5:02:53.16\n",
      "591000 of 1251449 done of cms_dataelapsed time: 5:03:25.82\n",
      "592000 of 1251449 done of cms_dataelapsed time: 5:03:55.89\n",
      "593000 of 1251449 done of cms_dataelapsed time: 5:04:25.83\n",
      "594000 of 1251449 done of cms_dataelapsed time: 5:04:58.57\n",
      "595000 of 1251449 done of cms_dataelapsed time: 5:05:28.58\n",
      "596000 of 1251449 done of cms_dataelapsed time: 5:05:58.51\n",
      "597000 of 1251449 done of cms_dataelapsed time: 5:06:31.48\n",
      "598000 of 1251449 done of cms_dataelapsed time: 5:07:01.16\n",
      "599000 of 1251449 done of cms_dataelapsed time: 5:07:31.24\n",
      "600000 of 1251449 done of cms_dataelapsed time: 5:08:03.93\n",
      "601000 of 1251449 done of cms_dataelapsed time: 5:08:34.00\n",
      "602000 of 1251449 done of cms_dataelapsed time: 5:09:03.86\n",
      "603000 of 1251449 done of cms_dataelapsed time: 5:09:36.58\n",
      "604000 of 1251449 done of cms_dataelapsed time: 5:10:06.41\n",
      "605000 of 1251449 done of cms_dataelapsed time: 5:10:36.18\n",
      "606000 of 1251449 done of cms_dataelapsed time: 5:11:08.77\n",
      "607000 of 1251449 done of cms_dataelapsed time: 5:11:38.51\n",
      "608000 of 1251449 done of cms_dataelapsed time: 5:12:08.13\n",
      "609000 of 1251449 done of cms_dataelapsed time: 5:12:40.81\n",
      "610000 of 1251449 done of cms_dataelapsed time: 5:13:10.64\n",
      "611000 of 1251449 done of cms_dataelapsed time: 5:13:40.51\n",
      "612000 of 1251449 done of cms_dataelapsed time: 5:14:13.16\n",
      "613000 of 1251449 done of cms_dataelapsed time: 5:14:43.47\n",
      "614000 of 1251449 done of cms_dataelapsed time: 5:15:13.44\n",
      "615000 of 1251449 done of cms_dataelapsed time: 5:15:46.18\n",
      "616000 of 1251449 done of cms_dataelapsed time: 5:16:15.70\n",
      "617000 of 1251449 done of cms_dataelapsed time: 5:16:45.47\n",
      "618000 of 1251449 done of cms_dataelapsed time: 5:17:18.18\n",
      "619000 of 1251449 done of cms_dataelapsed time: 5:17:47.71\n",
      "620000 of 1251449 done of cms_dataelapsed time: 5:18:17.34\n",
      "621000 of 1251449 done of cms_dataelapsed time: 5:18:46.68\n",
      "622000 of 1251449 done of cms_dataelapsed time: 5:19:19.24\n",
      "623000 of 1251449 done of cms_dataelapsed time: 5:19:48.83\n",
      "624000 of 1251449 done of cms_dataelapsed time: 5:20:18.36\n",
      "625000 of 1251449 done of cms_dataelapsed time: 5:20:50.53\n",
      "626000 of 1251449 done of cms_dataelapsed time: 5:21:25.26\n",
      "627000 of 1251449 done of cms_dataelapsed time: 5:21:54.80\n",
      "628000 of 1251449 done of cms_dataelapsed time: 5:22:27.39\n",
      "629000 of 1251449 done of cms_dataelapsed time: 5:22:57.21\n",
      "630000 of 1251449 done of cms_dataelapsed time: 5:23:27.00\n",
      "631000 of 1251449 done of cms_dataelapsed time: 5:23:59.47\n",
      "632000 of 1251449 done of cms_dataelapsed time: 5:24:29.17\n",
      "633000 of 1251449 done of cms_dataelapsed time: 5:24:58.79\n",
      "634000 of 1251449 done of cms_dataelapsed time: 5:25:31.04\n",
      "635000 of 1251449 done of cms_dataelapsed time: 5:26:00.64\n",
      "636000 of 1251449 done of cms_dataelapsed time: 5:26:30.82\n",
      "637000 of 1251449 done of cms_dataelapsed time: 5:27:03.49\n",
      "638000 of 1251449 done of cms_dataelapsed time: 5:27:33.15\n",
      "639000 of 1251449 done of cms_dataelapsed time: 5:28:03.31\n",
      "640000 of 1251449 done of cms_dataelapsed time: 5:28:35.88\n",
      "641000 of 1251449 done of cms_dataelapsed time: 5:29:05.93\n",
      "642000 of 1251449 done of cms_dataelapsed time: 5:29:36.29\n",
      "643000 of 1251449 done of cms_dataelapsed time: 5:30:08.68\n",
      "644000 of 1251449 done of cms_dataelapsed time: 5:30:38.44\n",
      "645000 of 1251449 done of cms_dataelapsed time: 5:31:08.30\n",
      "646000 of 1251449 done of cms_dataelapsed time: 5:31:40.86\n",
      "647000 of 1251449 done of cms_dataelapsed time: 5:32:10.90\n",
      "648000 of 1251449 done of cms_dataelapsed time: 5:32:40.86\n",
      "649000 of 1251449 done of cms_dataelapsed time: 5:33:13.52\n",
      "650000 of 1251449 done of cms_dataelapsed time: 5:33:43.15\n",
      "651000 of 1251449 done of cms_dataelapsed time: 5:34:12.80\n",
      "652000 of 1251449 done of cms_dataelapsed time: 5:34:45.34\n",
      "653000 of 1251449 done of cms_dataelapsed time: 5:35:14.97\n",
      "654000 of 1251449 done of cms_dataelapsed time: 5:35:44.87\n",
      "655000 of 1251449 done of cms_dataelapsed time: 5:36:17.30\n",
      "656000 of 1251449 done of cms_dataelapsed time: 5:36:46.83\n",
      "657000 of 1251449 done of cms_dataelapsed time: 5:37:16.48\n",
      "658000 of 1251449 done of cms_dataelapsed time: 5:37:48.97\n",
      "659000 of 1251449 done of cms_dataelapsed time: 5:38:18.59\n",
      "660000 of 1251449 done of cms_dataelapsed time: 5:38:48.29\n",
      "661000 of 1251449 done of cms_dataelapsed time: 5:39:17.90\n",
      "662000 of 1251449 done of cms_dataelapsed time: 5:39:50.13\n",
      "663000 of 1251449 done of cms_dataelapsed time: 5:40:19.84\n",
      "664000 of 1251449 done of cms_dataelapsed time: 5:40:49.36\n",
      "665000 of 1251449 done of cms_dataelapsed time: 5:41:21.69\n",
      "666000 of 1251449 done of cms_dataelapsed time: 5:41:51.45\n",
      "667000 of 1251449 done of cms_dataelapsed time: 5:42:21.01\n",
      "668000 of 1251449 done of cms_dataelapsed time: 5:42:53.38\n",
      "669000 of 1251449 done of cms_dataelapsed time: 5:43:23.26\n",
      "670000 of 1251449 done of cms_dataelapsed time: 5:43:52.68\n",
      "671000 of 1251449 done of cms_dataelapsed time: 5:44:25.72\n",
      "672000 of 1251449 done of cms_dataelapsed time: 5:44:55.32\n",
      "673000 of 1251449 done of cms_dataelapsed time: 5:45:24.75\n",
      "674000 of 1251449 done of cms_dataelapsed time: 5:45:57.07\n",
      "675000 of 1251449 done of cms_dataelapsed time: 5:46:26.83\n",
      "676000 of 1251449 done of cms_dataelapsed time: 5:46:56.43\n",
      "677000 of 1251449 done of cms_dataelapsed time: 5:47:28.79\n",
      "678000 of 1251449 done of cms_dataelapsed time: 5:47:58.29\n",
      "679000 of 1251449 done of cms_dataelapsed time: 5:48:27.94\n",
      "680000 of 1251449 done of cms_dataelapsed time: 5:49:02.87\n",
      "681000 of 1251449 done of cms_dataelapsed time: 5:49:41.62\n",
      "682000 of 1251449 done of cms_dataelapsed time: 5:50:16.05\n",
      "683000 of 1251449 done of cms_dataelapsed time: 5:50:58.22\n",
      "684000 of 1251449 done of cms_dataelapsed time: 5:51:35.95\n",
      "685000 of 1251449 done of cms_dataelapsed time: 5:52:12.42\n",
      "686000 of 1251449 done of cms_dataelapsed time: 5:52:51.48\n",
      "687000 of 1251449 done of cms_dataelapsed time: 5:53:28.46\n",
      "688000 of 1251449 done of cms_dataelapsed time: 5:54:03.67\n",
      "689000 of 1251449 done of cms_dataelapsed time: 5:54:41.98\n",
      "690000 of 1251449 done of cms_dataelapsed time: 5:55:17.68\n",
      "691000 of 1251449 done of cms_dataelapsed time: 5:55:54.42\n",
      "692000 of 1251449 done of cms_dataelapsed time: 5:56:32.13\n",
      "693000 of 1251449 done of cms_dataelapsed time: 5:57:07.20\n",
      "694000 of 1251449 done of cms_dataelapsed time: 5:57:38.70\n",
      "695000 of 1251449 done of cms_dataelapsed time: 5:58:11.59\n",
      "696000 of 1251449 done of cms_dataelapsed time: 5:58:41.38\n",
      "697000 of 1251449 done of cms_dataelapsed time: 5:59:11.40\n",
      "698000 of 1251449 done of cms_dataelapsed time: 5:59:41.06\n",
      "699000 of 1251449 done of cms_dataelapsed time: 6:00:15.75\n",
      "700000 of 1251449 done of cms_dataelapsed time: 6:00:45.71\n",
      "701000 of 1251449 done of cms_dataelapsed time: 6:01:15.67\n",
      "702000 of 1251449 done of cms_dataelapsed time: 6:01:48.46\n",
      "703000 of 1251449 done of cms_dataelapsed time: 6:02:18.71\n",
      "704000 of 1251449 done of cms_dataelapsed time: 6:02:48.49\n",
      "705000 of 1251449 done of cms_dataelapsed time: 6:03:21.03\n",
      "706000 of 1251449 done of cms_dataelapsed time: 6:03:51.04\n",
      "707000 of 1251449 done of cms_dataelapsed time: 6:04:20.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708000 of 1251449 done of cms_dataelapsed time: 6:04:58.09\n",
      "709000 of 1251449 done of cms_dataelapsed time: 6:05:32.57\n",
      "710000 of 1251449 done of cms_dataelapsed time: 6:06:06.68\n",
      "711000 of 1251449 done of cms_dataelapsed time: 6:06:39.58\n",
      "712000 of 1251449 done of cms_dataelapsed time: 6:07:09.92\n",
      "713000 of 1251449 done of cms_dataelapsed time: 6:07:39.76\n",
      "714000 of 1251449 done of cms_dataelapsed time: 6:08:12.31\n",
      "715000 of 1251449 done of cms_dataelapsed time: 6:08:42.23\n",
      "716000 of 1251449 done of cms_dataelapsed time: 6:09:12.08\n",
      "717000 of 1251449 done of cms_dataelapsed time: 6:09:44.65\n",
      "718000 of 1251449 done of cms_dataelapsed time: 6:10:14.40\n",
      "719000 of 1251449 done of cms_dataelapsed time: 6:10:44.11\n",
      "720000 of 1251449 done of cms_dataelapsed time: 6:11:16.73\n",
      "721000 of 1251449 done of cms_dataelapsed time: 6:11:46.58\n",
      "722000 of 1251449 done of cms_dataelapsed time: 6:12:16.41\n",
      "723000 of 1251449 done of cms_dataelapsed time: 6:12:48.68\n",
      "724000 of 1251449 done of cms_dataelapsed time: 6:13:18.49\n",
      "725000 of 1251449 done of cms_dataelapsed time: 6:13:48.21\n",
      "726000 of 1251449 done of cms_dataelapsed time: 6:14:20.64\n",
      "727000 of 1251449 done of cms_dataelapsed time: 6:14:50.34\n",
      "728000 of 1251449 done of cms_dataelapsed time: 6:15:20.24\n",
      "729000 of 1251449 done of cms_dataelapsed time: 6:15:52.73\n",
      "730000 of 1251449 done of cms_dataelapsed time: 6:16:22.37\n",
      "731000 of 1251449 done of cms_dataelapsed time: 6:16:52.09\n",
      "732000 of 1251449 done of cms_dataelapsed time: 6:17:24.66\n",
      "733000 of 1251449 done of cms_dataelapsed time: 6:17:54.25\n",
      "734000 of 1251449 done of cms_dataelapsed time: 6:18:24.15\n",
      "735000 of 1251449 done of cms_dataelapsed time: 6:18:56.79\n",
      "736000 of 1251449 done of cms_dataelapsed time: 6:19:26.52\n",
      "737000 of 1251449 done of cms_dataelapsed time: 6:19:56.69\n",
      "738000 of 1251449 done of cms_dataelapsed time: 6:20:26.34\n",
      "739000 of 1251449 done of cms_dataelapsed time: 6:21:01.16\n",
      "740000 of 1251449 done of cms_dataelapsed time: 6:21:31.84\n",
      "741000 of 1251449 done of cms_dataelapsed time: 6:22:01.73\n",
      "742000 of 1251449 done of cms_dataelapsed time: 6:22:34.29\n",
      "743000 of 1251449 done of cms_dataelapsed time: 6:23:04.13\n",
      "744000 of 1251449 done of cms_dataelapsed time: 6:23:33.92\n",
      "745000 of 1251449 done of cms_dataelapsed time: 6:24:06.53\n",
      "746000 of 1251449 done of cms_dataelapsed time: 6:24:36.27\n",
      "747000 of 1251449 done of cms_dataelapsed time: 6:25:06.00\n",
      "748000 of 1251449 done of cms_dataelapsed time: 6:25:38.59\n",
      "749000 of 1251449 done of cms_dataelapsed time: 6:26:08.38\n",
      "750000 of 1251449 done of cms_dataelapsed time: 6:26:38.40\n",
      "751000 of 1251449 done of cms_dataelapsed time: 6:27:11.38\n",
      "752000 of 1251449 done of cms_dataelapsed time: 6:27:41.46\n",
      "753000 of 1251449 done of cms_dataelapsed time: 6:28:11.87\n",
      "754000 of 1251449 done of cms_dataelapsed time: 6:28:44.21\n",
      "755000 of 1251449 done of cms_dataelapsed time: 6:29:14.13\n",
      "756000 of 1251449 done of cms_dataelapsed time: 6:29:43.86\n",
      "757000 of 1251449 done of cms_dataelapsed time: 6:30:16.43\n",
      "758000 of 1251449 done of cms_dataelapsed time: 6:30:46.02\n",
      "759000 of 1251449 done of cms_dataelapsed time: 6:31:16.57\n",
      "760000 of 1251449 done of cms_dataelapsed time: 6:31:54.32\n",
      "761000 of 1251449 done of cms_dataelapsed time: 6:32:28.36\n",
      "762000 of 1251449 done of cms_dataelapsed time: 6:33:00.02\n",
      "763000 of 1251449 done of cms_dataelapsed time: 6:33:32.83\n",
      "764000 of 1251449 done of cms_dataelapsed time: 6:34:02.56\n",
      "765000 of 1251449 done of cms_dataelapsed time: 6:34:32.41\n",
      "766000 of 1251449 done of cms_dataelapsed time: 6:35:05.28\n",
      "767000 of 1251449 done of cms_dataelapsed time: 6:35:35.97\n",
      "768000 of 1251449 done of cms_dataelapsed time: 6:36:05.54\n",
      "769000 of 1251449 done of cms_dataelapsed time: 6:36:38.22\n",
      "770000 of 1251449 done of cms_dataelapsed time: 6:37:07.71\n",
      "771000 of 1251449 done of cms_dataelapsed time: 6:37:37.64\n",
      "772000 of 1251449 done of cms_dataelapsed time: 6:38:10.24\n",
      "773000 of 1251449 done of cms_dataelapsed time: 6:38:39.68\n",
      "774000 of 1251449 done of cms_dataelapsed time: 6:39:09.56\n",
      "775000 of 1251449 done of cms_dataelapsed time: 6:39:39.06\n",
      "776000 of 1251449 done of cms_dataelapsed time: 6:40:12.14\n",
      "777000 of 1251449 done of cms_dataelapsed time: 6:40:41.90\n",
      "778000 of 1251449 done of cms_dataelapsed time: 6:41:11.52\n",
      "779000 of 1251449 done of cms_dataelapsed time: 6:41:44.02\n",
      "780000 of 1251449 done of cms_dataelapsed time: 6:42:13.74\n",
      "781000 of 1251449 done of cms_dataelapsed time: 6:42:43.24\n",
      "782000 of 1251449 done of cms_dataelapsed time: 6:43:15.93\n",
      "783000 of 1251449 done of cms_dataelapsed time: 6:43:45.70\n",
      "784000 of 1251449 done of cms_dataelapsed time: 6:44:15.55\n",
      "785000 of 1251449 done of cms_dataelapsed time: 6:44:48.08\n",
      "786000 of 1251449 done of cms_dataelapsed time: 6:45:18.03\n",
      "787000 of 1251449 done of cms_dataelapsed time: 6:45:47.76\n",
      "788000 of 1251449 done of cms_dataelapsed time: 6:46:20.37\n",
      "789000 of 1251449 done of cms_dataelapsed time: 6:46:50.23\n",
      "790000 of 1251449 done of cms_dataelapsed time: 6:47:19.96\n",
      "791000 of 1251449 done of cms_dataelapsed time: 6:47:52.23\n",
      "792000 of 1251449 done of cms_dataelapsed time: 6:48:22.01\n",
      "793000 of 1251449 done of cms_dataelapsed time: 6:48:51.51\n",
      "794000 of 1251449 done of cms_dataelapsed time: 6:49:24.04\n",
      "795000 of 1251449 done of cms_dataelapsed time: 6:49:53.98\n",
      "796000 of 1251449 done of cms_dataelapsed time: 6:50:23.74\n",
      "797000 of 1251449 done of cms_dataelapsed time: 6:50:56.82\n",
      "798000 of 1251449 done of cms_dataelapsed time: 6:51:26.70\n",
      "799000 of 1251449 done of cms_dataelapsed time: 6:51:56.44\n",
      "800000 of 1251449 done of cms_dataelapsed time: 6:52:28.78\n",
      "801000 of 1251449 done of cms_dataelapsed time: 6:52:58.39\n",
      "802000 of 1251449 done of cms_dataelapsed time: 6:53:28.41\n",
      "803000 of 1251449 done of cms_dataelapsed time: 6:54:01.28\n",
      "804000 of 1251449 done of cms_dataelapsed time: 6:54:30.95\n",
      "805000 of 1251449 done of cms_dataelapsed time: 6:55:00.51\n",
      "806000 of 1251449 done of cms_dataelapsed time: 6:55:33.10\n",
      "807000 of 1251449 done of cms_dataelapsed time: 6:56:02.74\n",
      "808000 of 1251449 done of cms_dataelapsed time: 6:56:32.78\n",
      "809000 of 1251449 done of cms_dataelapsed time: 6:57:05.42\n",
      "810000 of 1251449 done of cms_dataelapsed time: 6:57:35.08\n",
      "811000 of 1251449 done of cms_dataelapsed time: 6:58:04.67\n",
      "812000 of 1251449 done of cms_dataelapsed time: 6:58:37.08\n",
      "813000 of 1251449 done of cms_dataelapsed time: 6:59:06.71\n",
      "814000 of 1251449 done of cms_dataelapsed time: 6:59:36.54\n",
      "815000 of 1251449 done of cms_dataelapsed time: 7:00:06.87\n",
      "816000 of 1251449 done of cms_dataelapsed time: 7:00:39.26\n",
      "817000 of 1251449 done of cms_dataelapsed time: 7:01:09.06\n",
      "818000 of 1251449 done of cms_dataelapsed time: 7:01:38.71\n",
      "819000 of 1251449 done of cms_dataelapsed time: 7:02:11.29\n",
      "820000 of 1251449 done of cms_dataelapsed time: 7:02:41.32\n",
      "821000 of 1251449 done of cms_dataelapsed time: 7:03:11.05\n",
      "822000 of 1251449 done of cms_dataelapsed time: 7:03:43.46\n",
      "823000 of 1251449 done of cms_dataelapsed time: 7:04:13.45\n",
      "824000 of 1251449 done of cms_dataelapsed time: 7:04:43.02\n",
      "825000 of 1251449 done of cms_dataelapsed time: 7:05:15.48\n",
      "826000 of 1251449 done of cms_dataelapsed time: 7:05:45.99\n",
      "827000 of 1251449 done of cms_dataelapsed time: 7:06:15.77\n",
      "828000 of 1251449 done of cms_dataelapsed time: 7:06:48.33\n",
      "829000 of 1251449 done of cms_dataelapsed time: 7:07:18.22\n",
      "830000 of 1251449 done of cms_dataelapsed time: 7:07:47.95\n",
      "831000 of 1251449 done of cms_dataelapsed time: 7:08:20.92\n",
      "832000 of 1251449 done of cms_dataelapsed time: 7:08:50.60\n",
      "833000 of 1251449 done of cms_dataelapsed time: 7:09:20.45\n",
      "834000 of 1251449 done of cms_dataelapsed time: 7:09:53.15\n",
      "835000 of 1251449 done of cms_dataelapsed time: 7:10:22.71\n",
      "836000 of 1251449 done of cms_dataelapsed time: 7:10:52.04\n",
      "837000 of 1251449 done of cms_dataelapsed time: 7:11:24.43\n",
      "838000 of 1251449 done of cms_dataelapsed time: 7:11:54.09\n",
      "839000 of 1251449 done of cms_dataelapsed time: 7:12:27.37\n",
      "840000 of 1251449 done of cms_dataelapsed time: 7:13:03.56\n",
      "841000 of 1251449 done of cms_dataelapsed time: 7:13:37.73\n",
      "842000 of 1251449 done of cms_dataelapsed time: 7:14:07.56\n",
      "843000 of 1251449 done of cms_dataelapsed time: 7:14:40.05\n",
      "844000 of 1251449 done of cms_dataelapsed time: 7:15:09.66\n",
      "845000 of 1251449 done of cms_dataelapsed time: 7:15:39.60\n",
      "846000 of 1251449 done of cms_dataelapsed time: 7:16:12.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "847000 of 1251449 done of cms_dataelapsed time: 7:16:41.52\n",
      "848000 of 1251449 done of cms_dataelapsed time: 7:17:11.36\n",
      "849000 of 1251449 done of cms_dataelapsed time: 7:17:43.96\n",
      "850000 of 1251449 done of cms_dataelapsed time: 7:18:13.50\n",
      "851000 of 1251449 done of cms_dataelapsed time: 7:18:43.87\n",
      "852000 of 1251449 done of cms_dataelapsed time: 7:19:13.45\n",
      "853000 of 1251449 done of cms_dataelapsed time: 7:19:46.27\n",
      "854000 of 1251449 done of cms_dataelapsed time: 7:20:15.99\n",
      "855000 of 1251449 done of cms_dataelapsed time: 7:20:46.55\n",
      "856000 of 1251449 done of cms_dataelapsed time: 7:21:19.99\n",
      "857000 of 1251449 done of cms_dataelapsed time: 7:21:49.86\n",
      "858000 of 1251449 done of cms_dataelapsed time: 7:22:19.53\n",
      "859000 of 1251449 done of cms_dataelapsed time: 7:22:52.21\n",
      "860000 of 1251449 done of cms_dataelapsed time: 7:23:22.22\n",
      "861000 of 1251449 done of cms_dataelapsed time: 7:23:51.71\n",
      "862000 of 1251449 done of cms_dataelapsed time: 7:24:24.18\n",
      "863000 of 1251449 done of cms_dataelapsed time: 7:24:54.03\n",
      "864000 of 1251449 done of cms_dataelapsed time: 7:25:23.78\n",
      "865000 of 1251449 done of cms_dataelapsed time: 7:25:56.35\n",
      "866000 of 1251449 done of cms_dataelapsed time: 7:26:26.17\n",
      "867000 of 1251449 done of cms_dataelapsed time: 7:26:55.85\n",
      "868000 of 1251449 done of cms_dataelapsed time: 7:27:28.49\n",
      "869000 of 1251449 done of cms_dataelapsed time: 7:27:59.51\n",
      "870000 of 1251449 done of cms_dataelapsed time: 7:28:29.43\n",
      "871000 of 1251449 done of cms_dataelapsed time: 7:29:02.21\n",
      "872000 of 1251449 done of cms_dataelapsed time: 7:29:32.24\n",
      "873000 of 1251449 done of cms_dataelapsed time: 7:30:02.09\n",
      "874000 of 1251449 done of cms_dataelapsed time: 7:30:34.74\n",
      "875000 of 1251449 done of cms_dataelapsed time: 7:31:04.48\n",
      "876000 of 1251449 done of cms_dataelapsed time: 7:31:34.35\n",
      "877000 of 1251449 done of cms_dataelapsed time: 7:32:07.04\n",
      "878000 of 1251449 done of cms_dataelapsed time: 7:32:36.79\n",
      "879000 of 1251449 done of cms_dataelapsed time: 7:33:06.58\n",
      "880000 of 1251449 done of cms_dataelapsed time: 7:33:39.02\n",
      "881000 of 1251449 done of cms_dataelapsed time: 7:34:08.56\n",
      "882000 of 1251449 done of cms_dataelapsed time: 7:34:38.65\n",
      "883000 of 1251449 done of cms_dataelapsed time: 7:35:11.33\n",
      "884000 of 1251449 done of cms_dataelapsed time: 7:35:41.22\n",
      "885000 of 1251449 done of cms_dataelapsed time: 7:36:11.71\n",
      "886000 of 1251449 done of cms_dataelapsed time: 7:36:44.27\n",
      "887000 of 1251449 done of cms_dataelapsed time: 7:37:14.73\n",
      "888000 of 1251449 done of cms_dataelapsed time: 7:37:44.82\n",
      "889000 of 1251449 done of cms_dataelapsed time: 7:38:17.44\n",
      "890000 of 1251449 done of cms_dataelapsed time: 7:38:47.18\n",
      "891000 of 1251449 done of cms_dataelapsed time: 7:39:17.06\n",
      "892000 of 1251449 done of cms_dataelapsed time: 7:39:46.89\n",
      "893000 of 1251449 done of cms_dataelapsed time: 7:40:19.47\n",
      "894000 of 1251449 done of cms_dataelapsed time: 7:40:49.42\n",
      "895000 of 1251449 done of cms_dataelapsed time: 7:41:19.25\n",
      "896000 of 1251449 done of cms_dataelapsed time: 7:41:51.80\n",
      "897000 of 1251449 done of cms_dataelapsed time: 7:42:21.67\n",
      "898000 of 1251449 done of cms_dataelapsed time: 7:42:51.55\n",
      "899000 of 1251449 done of cms_dataelapsed time: 7:43:24.46\n",
      "900000 of 1251449 done of cms_dataelapsed time: 7:43:54.54\n",
      "901000 of 1251449 done of cms_dataelapsed time: 7:44:24.36\n",
      "902000 of 1251449 done of cms_dataelapsed time: 7:44:58.41\n",
      "903000 of 1251449 done of cms_dataelapsed time: 7:45:28.62\n",
      "904000 of 1251449 done of cms_dataelapsed time: 7:45:58.64\n",
      "905000 of 1251449 done of cms_dataelapsed time: 7:46:31.49\n",
      "906000 of 1251449 done of cms_dataelapsed time: 7:47:01.53\n",
      "907000 of 1251449 done of cms_dataelapsed time: 7:47:31.57\n",
      "908000 of 1251449 done of cms_dataelapsed time: 7:48:04.46\n",
      "909000 of 1251449 done of cms_dataelapsed time: 7:48:34.10\n",
      "910000 of 1251449 done of cms_dataelapsed time: 7:49:06.40\n",
      "911000 of 1251449 done of cms_dataelapsed time: 7:49:43.53\n",
      "912000 of 1251449 done of cms_dataelapsed time: 7:50:17.64\n",
      "913000 of 1251449 done of cms_dataelapsed time: 7:50:48.54\n",
      "914000 of 1251449 done of cms_dataelapsed time: 7:51:21.62\n",
      "915000 of 1251449 done of cms_dataelapsed time: 7:51:51.39\n",
      "916000 of 1251449 done of cms_dataelapsed time: 7:52:21.12\n",
      "917000 of 1251449 done of cms_dataelapsed time: 7:52:53.86\n",
      "918000 of 1251449 done of cms_dataelapsed time: 7:53:23.68\n",
      "919000 of 1251449 done of cms_dataelapsed time: 7:53:56.81\n",
      "920000 of 1251449 done of cms_dataelapsed time: 7:54:32.80\n",
      "921000 of 1251449 done of cms_dataelapsed time: 7:55:05.37\n",
      "922000 of 1251449 done of cms_dataelapsed time: 7:55:38.80\n",
      "923000 of 1251449 done of cms_dataelapsed time: 7:56:15.28\n",
      "924000 of 1251449 done of cms_dataelapsed time: 7:56:48.22\n",
      "925000 of 1251449 done of cms_dataelapsed time: 7:57:21.02\n",
      "926000 of 1251449 done of cms_dataelapsed time: 7:57:56.46\n",
      "927000 of 1251449 done of cms_dataelapsed time: 7:58:28.99\n",
      "928000 of 1251449 done of cms_dataelapsed time: 7:59:01.85\n",
      "929000 of 1251449 done of cms_dataelapsed time: 7:59:36.02\n",
      "930000 of 1251449 done of cms_dataelapsed time: 8:00:12.37\n",
      "931000 of 1251449 done of cms_dataelapsed time: 8:00:45.25\n",
      "932000 of 1251449 done of cms_dataelapsed time: 8:01:17.74\n",
      "933000 of 1251449 done of cms_dataelapsed time: 8:01:55.68\n",
      "934000 of 1251449 done of cms_dataelapsed time: 8:02:30.89\n",
      "935000 of 1251449 done of cms_dataelapsed time: 8:03:05.44\n",
      "936000 of 1251449 done of cms_dataelapsed time: 8:03:45.18\n",
      "937000 of 1251449 done of cms_dataelapsed time: 8:04:19.07\n",
      "938000 of 1251449 done of cms_dataelapsed time: 8:04:49.87\n",
      "939000 of 1251449 done of cms_dataelapsed time: 8:05:23.05\n",
      "940000 of 1251449 done of cms_dataelapsed time: 8:05:53.84\n",
      "941000 of 1251449 done of cms_dataelapsed time: 8:06:29.18\n",
      "942000 of 1251449 done of cms_dataelapsed time: 8:07:04.29\n",
      "943000 of 1251449 done of cms_dataelapsed time: 8:07:42.25\n",
      "944000 of 1251449 done of cms_dataelapsed time: 8:08:19.92\n",
      "945000 of 1251449 done of cms_dataelapsed time: 8:08:59.70\n",
      "946000 of 1251449 done of cms_dataelapsed time: 8:09:36.69\n",
      "947000 of 1251449 done of cms_dataelapsed time: 8:10:16.44\n",
      "948000 of 1251449 done of cms_dataelapsed time: 8:10:56.34\n",
      "949000 of 1251449 done of cms_dataelapsed time: 8:11:32.85\n",
      "950000 of 1251449 done of cms_dataelapsed time: 8:12:09.66\n",
      "951000 of 1251449 done of cms_dataelapsed time: 8:12:48.24\n",
      "952000 of 1251449 done of cms_dataelapsed time: 8:13:22.76\n",
      "953000 of 1251449 done of cms_dataelapsed time: 8:13:57.42\n",
      "954000 of 1251449 done of cms_dataelapsed time: 8:14:34.69\n",
      "955000 of 1251449 done of cms_dataelapsed time: 8:15:10.60\n",
      "956000 of 1251449 done of cms_dataelapsed time: 8:15:46.16\n",
      "957000 of 1251449 done of cms_dataelapsed time: 8:16:27.36\n",
      "958000 of 1251449 done of cms_dataelapsed time: 8:17:02.23\n",
      "959000 of 1251449 done of cms_dataelapsed time: 8:17:38.41\n",
      "960000 of 1251449 done of cms_dataelapsed time: 8:18:17.20\n",
      "961000 of 1251449 done of cms_dataelapsed time: 8:18:52.95\n",
      "962000 of 1251449 done of cms_dataelapsed time: 8:19:29.04\n",
      "963000 of 1251449 done of cms_dataelapsed time: 8:20:08.14\n",
      "964000 of 1251449 done of cms_dataelapsed time: 8:20:45.10\n",
      "965000 of 1251449 done of cms_dataelapsed time: 8:21:16.35\n",
      "966000 of 1251449 done of cms_dataelapsed time: 8:21:51.12\n",
      "967000 of 1251449 done of cms_dataelapsed time: 8:22:21.61\n",
      "968000 of 1251449 done of cms_dataelapsed time: 8:22:52.31\n",
      "969000 of 1251449 done of cms_dataelapsed time: 8:23:22.92\n",
      "970000 of 1251449 done of cms_dataelapsed time: 8:23:56.23\n",
      "971000 of 1251449 done of cms_dataelapsed time: 8:24:26.87\n",
      "972000 of 1251449 done of cms_dataelapsed time: 8:24:57.10\n",
      "973000 of 1251449 done of cms_dataelapsed time: 8:25:30.64\n",
      "974000 of 1251449 done of cms_dataelapsed time: 8:26:01.53\n",
      "975000 of 1251449 done of cms_dataelapsed time: 8:26:34.59\n",
      "976000 of 1251449 done of cms_dataelapsed time: 8:27:13.52\n",
      "977000 of 1251449 done of cms_dataelapsed time: 8:27:48.22\n",
      "978000 of 1251449 done of cms_dataelapsed time: 8:28:20.10\n",
      "979000 of 1251449 done of cms_dataelapsed time: 8:28:53.76\n",
      "980000 of 1251449 done of cms_dataelapsed time: 8:29:24.63\n",
      "981000 of 1251449 done of cms_dataelapsed time: 8:29:55.24\n",
      "982000 of 1251449 done of cms_dataelapsed time: 8:30:28.64\n",
      "983000 of 1251449 done of cms_dataelapsed time: 8:30:59.40\n",
      "984000 of 1251449 done of cms_dataelapsed time: 8:31:30.72\n",
      "985000 of 1251449 done of cms_dataelapsed time: 8:32:04.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986000 of 1251449 done of cms_dataelapsed time: 8:32:34.94\n",
      "987000 of 1251449 done of cms_dataelapsed time: 8:33:05.59\n",
      "988000 of 1251449 done of cms_dataelapsed time: 8:33:39.01\n",
      "989000 of 1251449 done of cms_dataelapsed time: 8:34:09.67\n",
      "990000 of 1251449 done of cms_dataelapsed time: 8:34:40.47\n",
      "991000 of 1251449 done of cms_dataelapsed time: 8:35:14.07\n",
      "992000 of 1251449 done of cms_dataelapsed time: 8:35:44.92\n",
      "993000 of 1251449 done of cms_dataelapsed time: 8:36:15.87\n",
      "994000 of 1251449 done of cms_dataelapsed time: 8:36:49.46\n",
      "995000 of 1251449 done of cms_dataelapsed time: 8:37:20.03\n",
      "996000 of 1251449 done of cms_dataelapsed time: 8:37:50.70\n",
      "997000 of 1251449 done of cms_dataelapsed time: 8:38:24.05\n",
      "998000 of 1251449 done of cms_dataelapsed time: 8:38:54.34\n",
      "999000 of 1251449 done of cms_dataelapsed time: 8:39:24.96\n",
      "1000000 of 1251449 done of cms_dataelapsed time: 8:39:58.35\n",
      "1001000 of 1251449 done of cms_dataelapsed time: 8:40:28.80\n",
      "1002000 of 1251449 done of cms_dataelapsed time: 8:40:59.44\n",
      "1003000 of 1251449 done of cms_dataelapsed time: 8:41:33.01\n",
      "1004000 of 1251449 done of cms_dataelapsed time: 8:42:03.91\n",
      "1005000 of 1251449 done of cms_dataelapsed time: 8:42:34.81\n",
      "1006000 of 1251449 done of cms_dataelapsed time: 8:43:05.36\n",
      "1007000 of 1251449 done of cms_dataelapsed time: 8:43:38.97\n",
      "1008000 of 1251449 done of cms_dataelapsed time: 8:44:09.75\n",
      "1009000 of 1251449 done of cms_dataelapsed time: 8:44:40.19\n",
      "1010000 of 1251449 done of cms_dataelapsed time: 8:45:13.74\n",
      "1011000 of 1251449 done of cms_dataelapsed time: 8:45:44.71\n",
      "1012000 of 1251449 done of cms_dataelapsed time: 8:46:15.31\n",
      "1013000 of 1251449 done of cms_dataelapsed time: 8:46:49.08\n",
      "1014000 of 1251449 done of cms_dataelapsed time: 8:47:20.17\n",
      "1015000 of 1251449 done of cms_dataelapsed time: 8:47:50.87\n",
      "1016000 of 1251449 done of cms_dataelapsed time: 8:48:24.54\n",
      "1017000 of 1251449 done of cms_dataelapsed time: 8:48:55.45\n",
      "1018000 of 1251449 done of cms_dataelapsed time: 8:49:26.21\n",
      "1019000 of 1251449 done of cms_dataelapsed time: 8:49:59.88\n",
      "1020000 of 1251449 done of cms_dataelapsed time: 8:50:30.66\n",
      "1021000 of 1251449 done of cms_dataelapsed time: 8:51:01.12\n",
      "1022000 of 1251449 done of cms_dataelapsed time: 8:51:34.80\n",
      "1023000 of 1251449 done of cms_dataelapsed time: 8:52:05.43\n",
      "1024000 of 1251449 done of cms_dataelapsed time: 8:52:36.06\n",
      "1025000 of 1251449 done of cms_dataelapsed time: 8:53:09.57\n",
      "1026000 of 1251449 done of cms_dataelapsed time: 8:53:40.56\n",
      "1027000 of 1251449 done of cms_dataelapsed time: 8:54:11.17\n",
      "1028000 of 1251449 done of cms_dataelapsed time: 8:54:44.63\n",
      "1029000 of 1251449 done of cms_dataelapsed time: 8:55:15.29\n",
      "1030000 of 1251449 done of cms_dataelapsed time: 8:55:46.05\n",
      "1031000 of 1251449 done of cms_dataelapsed time: 8:56:19.68\n",
      "1032000 of 1251449 done of cms_dataelapsed time: 8:56:50.42\n",
      "1033000 of 1251449 done of cms_dataelapsed time: 8:57:22.01\n",
      "1034000 of 1251449 done of cms_dataelapsed time: 8:57:55.60\n",
      "1035000 of 1251449 done of cms_dataelapsed time: 8:58:26.38\n",
      "1036000 of 1251449 done of cms_dataelapsed time: 8:58:57.20\n",
      "1037000 of 1251449 done of cms_dataelapsed time: 8:59:32.07\n",
      "1038000 of 1251449 done of cms_dataelapsed time: 9:00:07.78\n",
      "1039000 of 1251449 done of cms_dataelapsed time: 9:00:42.15\n",
      "1040000 of 1251449 done of cms_dataelapsed time: 9:01:20.04\n",
      "1041000 of 1251449 done of cms_dataelapsed time: 9:01:50.60\n",
      "1042000 of 1251449 done of cms_dataelapsed time: 9:02:21.52\n",
      "1043000 of 1251449 done of cms_dataelapsed time: 9:02:55.09\n",
      "1044000 of 1251449 done of cms_dataelapsed time: 9:03:25.70\n",
      "1045000 of 1251449 done of cms_dataelapsed time: 9:03:56.59\n",
      "1046000 of 1251449 done of cms_dataelapsed time: 9:04:27.24\n",
      "1047000 of 1251449 done of cms_dataelapsed time: 9:05:00.98\n",
      "1048000 of 1251449 done of cms_dataelapsed time: 9:05:31.87\n",
      "1049000 of 1251449 done of cms_dataelapsed time: 9:06:02.36\n",
      "1050000 of 1251449 done of cms_dataelapsed time: 9:06:35.98\n",
      "1051000 of 1251449 done of cms_dataelapsed time: 9:07:06.81\n",
      "1052000 of 1251449 done of cms_dataelapsed time: 9:07:42.86\n",
      "1053000 of 1251449 done of cms_dataelapsed time: 9:08:22.43\n",
      "1054000 of 1251449 done of cms_dataelapsed time: 9:08:57.13\n",
      "1055000 of 1251449 done of cms_dataelapsed time: 9:09:32.05\n",
      "1056000 of 1251449 done of cms_dataelapsed time: 9:10:09.74\n",
      "1057000 of 1251449 done of cms_dataelapsed time: 9:10:44.70\n",
      "1058000 of 1251449 done of cms_dataelapsed time: 9:11:19.35\n",
      "1059000 of 1251449 done of cms_dataelapsed time: 9:11:56.63\n",
      "1060000 of 1251449 done of cms_dataelapsed time: 9:12:32.01\n",
      "1061000 of 1251449 done of cms_dataelapsed time: 9:13:06.74\n",
      "1062000 of 1251449 done of cms_dataelapsed time: 9:13:46.18\n",
      "1063000 of 1251449 done of cms_dataelapsed time: 9:14:22.36\n",
      "1064000 of 1251449 done of cms_dataelapsed time: 9:14:58.28\n",
      "1065000 of 1251449 done of cms_dataelapsed time: 9:15:36.23\n",
      "1066000 of 1251449 done of cms_dataelapsed time: 9:16:11.44\n",
      "1067000 of 1251449 done of cms_dataelapsed time: 9:16:47.46\n",
      "1068000 of 1251449 done of cms_dataelapsed time: 9:17:27.57\n",
      "1069000 of 1251449 done of cms_dataelapsed time: 9:18:04.45\n",
      "1070000 of 1251449 done of cms_dataelapsed time: 9:18:39.75\n",
      "1071000 of 1251449 done of cms_dataelapsed time: 9:19:18.57\n",
      "1072000 of 1251449 done of cms_dataelapsed time: 9:19:55.55\n",
      "1073000 of 1251449 done of cms_dataelapsed time: 9:20:31.88\n",
      "1074000 of 1251449 done of cms_dataelapsed time: 9:21:11.43\n",
      "1075000 of 1251449 done of cms_dataelapsed time: 9:21:46.33\n",
      "1076000 of 1251449 done of cms_dataelapsed time: 9:22:20.12\n",
      "1077000 of 1251449 done of cms_dataelapsed time: 9:22:57.26\n",
      "1078000 of 1251449 done of cms_dataelapsed time: 9:23:32.49\n",
      "1079000 of 1251449 done of cms_dataelapsed time: 9:24:08.15\n",
      "1080000 of 1251449 done of cms_dataelapsed time: 9:24:48.20\n",
      "1081000 of 1251449 done of cms_dataelapsed time: 9:25:22.83\n",
      "1082000 of 1251449 done of cms_dataelapsed time: 9:25:56.36\n",
      "1083000 of 1251449 done of cms_dataelapsed time: 9:26:32.87\n",
      "1084000 of 1251449 done of cms_dataelapsed time: 9:27:08.45\n",
      "1085000 of 1251449 done of cms_dataelapsed time: 9:27:46.75\n",
      "1086000 of 1251449 done of cms_dataelapsed time: 9:28:21.42\n",
      "1087000 of 1251449 done of cms_dataelapsed time: 9:28:59.92\n",
      "1088000 of 1251449 done of cms_dataelapsed time: 9:29:36.23\n",
      "1089000 of 1251449 done of cms_dataelapsed time: 9:30:12.28\n",
      "1090000 of 1251449 done of cms_dataelapsed time: 9:30:52.72\n",
      "1091000 of 1251449 done of cms_dataelapsed time: 9:31:29.86\n",
      "1092000 of 1251449 done of cms_dataelapsed time: 9:32:06.73\n",
      "1093000 of 1251449 done of cms_dataelapsed time: 9:32:45.79\n",
      "1094000 of 1251449 done of cms_dataelapsed time: 9:33:21.93\n",
      "1095000 of 1251449 done of cms_dataelapsed time: 9:33:57.56\n",
      "1096000 of 1251449 done of cms_dataelapsed time: 9:34:36.24\n",
      "1097000 of 1251449 done of cms_dataelapsed time: 9:35:11.61\n",
      "1098000 of 1251449 done of cms_dataelapsed time: 9:35:47.74\n",
      "1099000 of 1251449 done of cms_dataelapsed time: 9:36:28.06\n",
      "1100000 of 1251449 done of cms_dataelapsed time: 9:37:04.43\n",
      "1101000 of 1251449 done of cms_dataelapsed time: 9:37:39.27\n",
      "1102000 of 1251449 done of cms_dataelapsed time: 9:38:19.75\n",
      "1103000 of 1251449 done of cms_dataelapsed time: 9:38:55.40\n",
      "1104000 of 1251449 done of cms_dataelapsed time: 9:39:31.56\n",
      "1105000 of 1251449 done of cms_dataelapsed time: 9:40:10.71\n",
      "1106000 of 1251449 done of cms_dataelapsed time: 9:40:47.54\n",
      "1107000 of 1251449 done of cms_dataelapsed time: 9:41:27.28\n",
      "1108000 of 1251449 done of cms_dataelapsed time: 9:42:05.95\n",
      "1109000 of 1251449 done of cms_dataelapsed time: 9:42:36.21\n",
      "1110000 of 1251449 done of cms_dataelapsed time: 9:43:06.36\n",
      "1111000 of 1251449 done of cms_dataelapsed time: 9:43:39.46\n",
      "1112000 of 1251449 done of cms_dataelapsed time: 9:44:09.88\n",
      "1113000 of 1251449 done of cms_dataelapsed time: 9:44:40.37\n",
      "1114000 of 1251449 done of cms_dataelapsed time: 9:45:13.52\n",
      "1115000 of 1251449 done of cms_dataelapsed time: 9:45:43.62\n",
      "1116000 of 1251449 done of cms_dataelapsed time: 9:46:14.06\n",
      "1117000 of 1251449 done of cms_dataelapsed time: 9:46:47.31\n",
      "1118000 of 1251449 done of cms_dataelapsed time: 9:47:17.68\n",
      "1119000 of 1251449 done of cms_dataelapsed time: 9:47:47.82\n",
      "1120000 of 1251449 done of cms_dataelapsed time: 9:48:20.60\n",
      "1121000 of 1251449 done of cms_dataelapsed time: 9:48:50.40\n",
      "1122000 of 1251449 done of cms_dataelapsed time: 9:49:24.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1123000 of 1251449 done of cms_dataelapsed time: 9:49:58.33\n",
      "1124000 of 1251449 done of cms_dataelapsed time: 9:50:35.59\n",
      "1125000 of 1251449 done of cms_dataelapsed time: 9:51:05.73\n",
      "1126000 of 1251449 done of cms_dataelapsed time: 9:51:35.66\n",
      "1127000 of 1251449 done of cms_dataelapsed time: 9:52:08.54\n",
      "1128000 of 1251449 done of cms_dataelapsed time: 9:52:38.61\n",
      "1129000 of 1251449 done of cms_dataelapsed time: 9:53:08.26\n",
      "1130000 of 1251449 done of cms_dataelapsed time: 9:53:42.56\n",
      "1131000 of 1251449 done of cms_dataelapsed time: 9:54:20.45\n",
      "1132000 of 1251449 done of cms_dataelapsed time: 9:54:57.12\n",
      "1133000 of 1251449 done of cms_dataelapsed time: 9:55:36.30\n",
      "1134000 of 1251449 done of cms_dataelapsed time: 9:56:13.70\n",
      "1135000 of 1251449 done of cms_dataelapsed time: 9:56:49.29\n",
      "1136000 of 1251449 done of cms_dataelapsed time: 9:57:28.71\n",
      "1137000 of 1251449 done of cms_dataelapsed time: 9:58:05.14\n",
      "1138000 of 1251449 done of cms_dataelapsed time: 9:58:41.89\n",
      "1139000 of 1251449 done of cms_dataelapsed time: 9:59:22.47\n",
      "1140000 of 1251449 done of cms_dataelapsed time: 9:59:59.62\n",
      "1141000 of 1251449 done of cms_dataelapsed time: 10:00:34.90\n",
      "1142000 of 1251449 done of cms_dataelapsed time: 10:01:14.36\n",
      "1143000 of 1251449 done of cms_dataelapsed time: 10:01:51.40\n",
      "1144000 of 1251449 done of cms_dataelapsed time: 10:02:27.65\n",
      "1145000 of 1251449 done of cms_dataelapsed time: 10:03:05.88\n",
      "1146000 of 1251449 done of cms_dataelapsed time: 10:03:41.86\n",
      "1147000 of 1251449 done of cms_dataelapsed time: 10:04:18.20\n",
      "1148000 of 1251449 done of cms_dataelapsed time: 10:04:55.07\n",
      "1149000 of 1251449 done of cms_dataelapsed time: 10:05:31.72\n",
      "1150000 of 1251449 done of cms_dataelapsed time: 10:06:08.51\n",
      "1151000 of 1251449 done of cms_dataelapsed time: 10:06:48.55\n",
      "1152000 of 1251449 done of cms_dataelapsed time: 10:07:25.27\n",
      "1153000 of 1251449 done of cms_dataelapsed time: 10:08:00.94\n",
      "1154000 of 1251449 done of cms_dataelapsed time: 10:08:38.98\n",
      "1155000 of 1251449 done of cms_dataelapsed time: 10:09:13.26\n",
      "1156000 of 1251449 done of cms_dataelapsed time: 10:09:47.71\n",
      "1157000 of 1251449 done of cms_dataelapsed time: 10:10:28.31\n",
      "1158000 of 1251449 done of cms_dataelapsed time: 10:11:03.24\n",
      "1159000 of 1251449 done of cms_dataelapsed time: 10:11:34.79\n",
      "1160000 of 1251449 done of cms_dataelapsed time: 10:12:07.31\n",
      "1161000 of 1251449 done of cms_dataelapsed time: 10:12:36.59\n",
      "1162000 of 1251449 done of cms_dataelapsed time: 10:13:06.66\n",
      "1163000 of 1251449 done of cms_dataelapsed time: 10:13:36.27\n",
      "1164000 of 1251449 done of cms_dataelapsed time: 10:14:08.81\n",
      "1165000 of 1251449 done of cms_dataelapsed time: 10:14:38.53\n",
      "1166000 of 1251449 done of cms_dataelapsed time: 10:15:08.11\n",
      "1167000 of 1251449 done of cms_dataelapsed time: 10:15:40.58\n",
      "1168000 of 1251449 done of cms_dataelapsed time: 10:16:10.02\n",
      "1169000 of 1251449 done of cms_dataelapsed time: 10:16:39.39\n",
      "1170000 of 1251449 done of cms_dataelapsed time: 10:17:11.75\n",
      "1171000 of 1251449 done of cms_dataelapsed time: 10:17:41.53\n",
      "1172000 of 1251449 done of cms_dataelapsed time: 10:18:11.05\n",
      "1173000 of 1251449 done of cms_dataelapsed time: 10:18:43.52\n",
      "1174000 of 1251449 done of cms_dataelapsed time: 10:19:13.39\n",
      "1175000 of 1251449 done of cms_dataelapsed time: 10:19:43.15\n",
      "1176000 of 1251449 done of cms_dataelapsed time: 10:20:19.36\n",
      "1177000 of 1251449 done of cms_dataelapsed time: 10:20:53.26\n",
      "1178000 of 1251449 done of cms_dataelapsed time: 10:21:28.32\n",
      "1179000 of 1251449 done of cms_dataelapsed time: 10:22:00.75\n",
      "1180000 of 1251449 done of cms_dataelapsed time: 10:22:30.49\n",
      "1181000 of 1251449 done of cms_dataelapsed time: 10:22:59.97\n",
      "1182000 of 1251449 done of cms_dataelapsed time: 10:23:32.06\n",
      "1183000 of 1251449 done of cms_dataelapsed time: 10:24:01.81\n",
      "1184000 of 1251449 done of cms_dataelapsed time: 10:24:31.45\n",
      "1185000 of 1251449 done of cms_dataelapsed time: 10:25:04.00\n",
      "1186000 of 1251449 done of cms_dataelapsed time: 10:25:33.83\n",
      "1187000 of 1251449 done of cms_dataelapsed time: 10:26:03.65\n",
      "1188000 of 1251449 done of cms_dataelapsed time: 10:26:36.56\n",
      "1189000 of 1251449 done of cms_dataelapsed time: 10:27:06.28\n",
      "1190000 of 1251449 done of cms_dataelapsed time: 10:27:36.36\n",
      "1191000 of 1251449 done of cms_dataelapsed time: 10:28:09.66\n",
      "1192000 of 1251449 done of cms_dataelapsed time: 10:28:39.28\n",
      "1193000 of 1251449 done of cms_dataelapsed time: 10:29:09.10\n",
      "1194000 of 1251449 done of cms_dataelapsed time: 10:29:41.80\n",
      "1195000 of 1251449 done of cms_dataelapsed time: 10:30:11.46\n",
      "1196000 of 1251449 done of cms_dataelapsed time: 10:30:41.39\n",
      "1197000 of 1251449 done of cms_dataelapsed time: 10:31:14.03\n",
      "1198000 of 1251449 done of cms_dataelapsed time: 10:31:43.96\n",
      "1199000 of 1251449 done of cms_dataelapsed time: 10:32:14.05\n",
      "1200000 of 1251449 done of cms_dataelapsed time: 10:32:43.82\n",
      "1201000 of 1251449 done of cms_dataelapsed time: 10:33:16.31\n",
      "1202000 of 1251449 done of cms_dataelapsed time: 10:33:46.11\n",
      "1203000 of 1251449 done of cms_dataelapsed time: 10:34:15.84\n",
      "1204000 of 1251449 done of cms_dataelapsed time: 10:34:48.44\n",
      "1205000 of 1251449 done of cms_dataelapsed time: 10:35:18.60\n",
      "1206000 of 1251449 done of cms_dataelapsed time: 10:35:48.78\n",
      "1207000 of 1251449 done of cms_dataelapsed time: 10:36:21.29\n",
      "1208000 of 1251449 done of cms_dataelapsed time: 10:36:51.19\n",
      "1209000 of 1251449 done of cms_dataelapsed time: 10:37:20.87\n",
      "1210000 of 1251449 done of cms_dataelapsed time: 10:37:53.53\n",
      "1211000 of 1251449 done of cms_dataelapsed time: 10:38:23.45\n",
      "1212000 of 1251449 done of cms_dataelapsed time: 10:38:53.19\n",
      "1213000 of 1251449 done of cms_dataelapsed time: 10:39:25.70\n",
      "1214000 of 1251449 done of cms_dataelapsed time: 10:39:55.78\n",
      "1215000 of 1251449 done of cms_dataelapsed time: 10:40:25.77\n",
      "1216000 of 1251449 done of cms_dataelapsed time: 10:40:58.48\n",
      "1217000 of 1251449 done of cms_dataelapsed time: 10:41:28.37\n",
      "1218000 of 1251449 done of cms_dataelapsed time: 10:41:58.22\n",
      "1219000 of 1251449 done of cms_dataelapsed time: 10:42:30.87\n",
      "1220000 of 1251449 done of cms_dataelapsed time: 10:43:00.59\n",
      "1221000 of 1251449 done of cms_dataelapsed time: 10:43:30.55\n",
      "1222000 of 1251449 done of cms_dataelapsed time: 10:44:02.88\n",
      "1223000 of 1251449 done of cms_dataelapsed time: 10:44:32.70\n",
      "1224000 of 1251449 done of cms_dataelapsed time: 10:45:02.60\n",
      "1225000 of 1251449 done of cms_dataelapsed time: 10:45:35.28\n",
      "1226000 of 1251449 done of cms_dataelapsed time: 10:46:05.04\n",
      "1227000 of 1251449 done of cms_dataelapsed time: 10:46:34.98\n",
      "1228000 of 1251449 done of cms_dataelapsed time: 10:47:07.74\n",
      "1229000 of 1251449 done of cms_dataelapsed time: 10:47:37.52\n",
      "1230000 of 1251449 done of cms_dataelapsed time: 10:48:07.57\n",
      "1231000 of 1251449 done of cms_dataelapsed time: 10:48:40.18\n",
      "1232000 of 1251449 done of cms_dataelapsed time: 10:49:09.71\n",
      "1233000 of 1251449 done of cms_dataelapsed time: 10:49:39.49\n",
      "1234000 of 1251449 done of cms_dataelapsed time: 10:50:12.41\n",
      "1235000 of 1251449 done of cms_dataelapsed time: 10:50:42.86\n",
      "1236000 of 1251449 done of cms_dataelapsed time: 10:51:12.83\n",
      "1237000 of 1251449 done of cms_dataelapsed time: 10:51:45.50\n",
      "1238000 of 1251449 done of cms_dataelapsed time: 10:52:15.15\n",
      "1239000 of 1251449 done of cms_dataelapsed time: 10:52:45.24\n",
      "1240000 of 1251449 done of cms_dataelapsed time: 10:53:15.14\n",
      "1241000 of 1251449 done of cms_dataelapsed time: 10:53:47.80\n",
      "1242000 of 1251449 done of cms_dataelapsed time: 10:54:17.87\n",
      "1243000 of 1251449 done of cms_dataelapsed time: 10:54:47.59\n",
      "1244000 of 1251449 done of cms_dataelapsed time: 10:55:20.18\n",
      "1245000 of 1251449 done of cms_dataelapsed time: 10:55:50.03\n",
      "1246000 of 1251449 done of cms_dataelapsed time: 10:56:19.68\n",
      "1247000 of 1251449 done of cms_dataelapsed time: 10:56:52.24\n",
      "1248000 of 1251449 done of cms_dataelapsed time: 10:57:22.21\n",
      "1249000 of 1251449 done of cms_dataelapsed time: 10:57:51.94\n",
      "1250000 of 1251449 done of cms_dataelapsed time: 10:58:24.65\n",
      "1251000 of 1251449 done of cms_dataelapsed time: 10:58:54.69\n",
      "0 of 208255 done of cms_bkg_estimationelapsed time: 0:00:00.03\n",
      "1000 of 208255 done of cms_bkg_estimationelapsed time: 0:00:32.72\n",
      "2000 of 208255 done of cms_bkg_estimationelapsed time: 0:01:02.64\n",
      "3000 of 208255 done of cms_bkg_estimationelapsed time: 0:01:32.56\n",
      "4000 of 208255 done of cms_bkg_estimationelapsed time: 0:02:02.36\n",
      "5000 of 208255 done of cms_bkg_estimationelapsed time: 0:02:35.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 of 208255 done of cms_bkg_estimationelapsed time: 0:03:05.28\n",
      "7000 of 208255 done of cms_bkg_estimationelapsed time: 0:03:35.07\n",
      "8000 of 208255 done of cms_bkg_estimationelapsed time: 0:04:07.85\n",
      "9000 of 208255 done of cms_bkg_estimationelapsed time: 0:04:37.96\n",
      "10000 of 208255 done of cms_bkg_estimationelapsed time: 0:05:07.75\n",
      "11000 of 208255 done of cms_bkg_estimationelapsed time: 0:05:40.60\n",
      "12000 of 208255 done of cms_bkg_estimationelapsed time: 0:06:10.81\n",
      "13000 of 208255 done of cms_bkg_estimationelapsed time: 0:06:41.21\n",
      "14000 of 208255 done of cms_bkg_estimationelapsed time: 0:07:13.73\n",
      "15000 of 208255 done of cms_bkg_estimationelapsed time: 0:07:43.83\n",
      "16000 of 208255 done of cms_bkg_estimationelapsed time: 0:08:13.54\n",
      "17000 of 208255 done of cms_bkg_estimationelapsed time: 0:08:46.33\n",
      "18000 of 208255 done of cms_bkg_estimationelapsed time: 0:09:16.39\n",
      "19000 of 208255 done of cms_bkg_estimationelapsed time: 0:09:46.26\n",
      "20000 of 208255 done of cms_bkg_estimationelapsed time: 0:10:19.02\n",
      "21000 of 208255 done of cms_bkg_estimationelapsed time: 0:10:49.12\n",
      "22000 of 208255 done of cms_bkg_estimationelapsed time: 0:11:19.08\n",
      "23000 of 208255 done of cms_bkg_estimationelapsed time: 0:11:51.94\n",
      "24000 of 208255 done of cms_bkg_estimationelapsed time: 0:12:22.04\n",
      "25000 of 208255 done of cms_bkg_estimationelapsed time: 0:12:51.99\n",
      "26000 of 208255 done of cms_bkg_estimationelapsed time: 0:13:24.62\n",
      "27000 of 208255 done of cms_bkg_estimationelapsed time: 0:13:54.47\n",
      "28000 of 208255 done of cms_bkg_estimationelapsed time: 0:14:24.17\n",
      "29000 of 208255 done of cms_bkg_estimationelapsed time: 0:14:56.76\n",
      "30000 of 208255 done of cms_bkg_estimationelapsed time: 0:15:26.69\n",
      "31000 of 208255 done of cms_bkg_estimationelapsed time: 0:15:56.38\n",
      "32000 of 208255 done of cms_bkg_estimationelapsed time: 0:16:29.01\n",
      "33000 of 208255 done of cms_bkg_estimationelapsed time: 0:16:58.86\n",
      "34000 of 208255 done of cms_bkg_estimationelapsed time: 0:17:28.92\n",
      "35000 of 208255 done of cms_bkg_estimationelapsed time: 0:18:01.47\n",
      "36000 of 208255 done of cms_bkg_estimationelapsed time: 0:18:31.34\n",
      "37000 of 208255 done of cms_bkg_estimationelapsed time: 0:19:01.19\n",
      "38000 of 208255 done of cms_bkg_estimationelapsed time: 0:19:33.72\n",
      "39000 of 208255 done of cms_bkg_estimationelapsed time: 0:20:03.40\n",
      "40000 of 208255 done of cms_bkg_estimationelapsed time: 0:20:33.97\n",
      "41000 of 208255 done of cms_bkg_estimationelapsed time: 0:21:06.59\n",
      "42000 of 208255 done of cms_bkg_estimationelapsed time: 0:21:37.22\n",
      "43000 of 208255 done of cms_bkg_estimationelapsed time: 0:22:07.88\n",
      "44000 of 208255 done of cms_bkg_estimationelapsed time: 0:22:37.70\n",
      "45000 of 208255 done of cms_bkg_estimationelapsed time: 0:23:10.79\n",
      "46000 of 208255 done of cms_bkg_estimationelapsed time: 0:23:40.68\n",
      "47000 of 208255 done of cms_bkg_estimationelapsed time: 0:24:10.45\n",
      "48000 of 208255 done of cms_bkg_estimationelapsed time: 0:24:45.39\n",
      "49000 of 208255 done of cms_bkg_estimationelapsed time: 0:25:20.10\n",
      "50000 of 208255 done of cms_bkg_estimationelapsed time: 0:25:54.41\n",
      "51000 of 208255 done of cms_bkg_estimationelapsed time: 0:26:27.93\n",
      "52000 of 208255 done of cms_bkg_estimationelapsed time: 0:26:58.19\n",
      "53000 of 208255 done of cms_bkg_estimationelapsed time: 0:27:28.25\n",
      "54000 of 208255 done of cms_bkg_estimationelapsed time: 0:28:01.18\n",
      "55000 of 208255 done of cms_bkg_estimationelapsed time: 0:28:31.79\n",
      "56000 of 208255 done of cms_bkg_estimationelapsed time: 0:29:01.63\n",
      "57000 of 208255 done of cms_bkg_estimationelapsed time: 0:29:34.54\n",
      "58000 of 208255 done of cms_bkg_estimationelapsed time: 0:30:04.40\n",
      "59000 of 208255 done of cms_bkg_estimationelapsed time: 0:30:34.17\n",
      "60000 of 208255 done of cms_bkg_estimationelapsed time: 0:31:06.86\n",
      "61000 of 208255 done of cms_bkg_estimationelapsed time: 0:31:36.65\n",
      "62000 of 208255 done of cms_bkg_estimationelapsed time: 0:32:06.62\n",
      "63000 of 208255 done of cms_bkg_estimationelapsed time: 0:32:39.09\n",
      "64000 of 208255 done of cms_bkg_estimationelapsed time: 0:33:09.04\n",
      "65000 of 208255 done of cms_bkg_estimationelapsed time: 0:33:38.83\n",
      "66000 of 208255 done of cms_bkg_estimationelapsed time: 0:34:11.29\n",
      "67000 of 208255 done of cms_bkg_estimationelapsed time: 0:34:41.06\n",
      "68000 of 208255 done of cms_bkg_estimationelapsed time: 0:35:11.05\n",
      "69000 of 208255 done of cms_bkg_estimationelapsed time: 0:35:43.48\n",
      "70000 of 208255 done of cms_bkg_estimationelapsed time: 0:36:13.14\n",
      "71000 of 208255 done of cms_bkg_estimationelapsed time: 0:36:43.11\n",
      "72000 of 208255 done of cms_bkg_estimationelapsed time: 0:37:15.85\n",
      "73000 of 208255 done of cms_bkg_estimationelapsed time: 0:37:45.66\n",
      "74000 of 208255 done of cms_bkg_estimationelapsed time: 0:38:15.76\n",
      "75000 of 208255 done of cms_bkg_estimationelapsed time: 0:38:48.65\n",
      "76000 of 208255 done of cms_bkg_estimationelapsed time: 0:39:18.42\n",
      "77000 of 208255 done of cms_bkg_estimationelapsed time: 0:39:48.38\n",
      "78000 of 208255 done of cms_bkg_estimationelapsed time: 0:40:21.16\n",
      "79000 of 208255 done of cms_bkg_estimationelapsed time: 0:40:51.83\n",
      "80000 of 208255 done of cms_bkg_estimationelapsed time: 0:41:21.92\n",
      "81000 of 208255 done of cms_bkg_estimationelapsed time: 0:41:54.72\n",
      "82000 of 208255 done of cms_bkg_estimationelapsed time: 0:42:24.55\n",
      "83000 of 208255 done of cms_bkg_estimationelapsed time: 0:42:54.61\n",
      "84000 of 208255 done of cms_bkg_estimationelapsed time: 0:43:24.31\n",
      "85000 of 208255 done of cms_bkg_estimationelapsed time: 0:43:57.15\n",
      "86000 of 208255 done of cms_bkg_estimationelapsed time: 0:44:27.21\n",
      "87000 of 208255 done of cms_bkg_estimationelapsed time: 0:44:56.90\n",
      "88000 of 208255 done of cms_bkg_estimationelapsed time: 0:45:29.34\n",
      "89000 of 208255 done of cms_bkg_estimationelapsed time: 0:45:59.38\n",
      "90000 of 208255 done of cms_bkg_estimationelapsed time: 0:46:29.09\n",
      "91000 of 208255 done of cms_bkg_estimationelapsed time: 0:47:01.61\n",
      "92000 of 208255 done of cms_bkg_estimationelapsed time: 0:47:31.61\n",
      "93000 of 208255 done of cms_bkg_estimationelapsed time: 0:48:01.36\n",
      "94000 of 208255 done of cms_bkg_estimationelapsed time: 0:48:33.95\n",
      "95000 of 208255 done of cms_bkg_estimationelapsed time: 0:49:04.03\n",
      "96000 of 208255 done of cms_bkg_estimationelapsed time: 0:49:34.05\n",
      "97000 of 208255 done of cms_bkg_estimationelapsed time: 0:50:07.15\n",
      "98000 of 208255 done of cms_bkg_estimationelapsed time: 0:50:37.33\n",
      "99000 of 208255 done of cms_bkg_estimationelapsed time: 0:51:07.29\n",
      "100000 of 208255 done of cms_bkg_estimationelapsed time: 0:51:40.06\n",
      "101000 of 208255 done of cms_bkg_estimationelapsed time: 0:52:10.03\n",
      "102000 of 208255 done of cms_bkg_estimationelapsed time: 0:52:39.94\n",
      "103000 of 208255 done of cms_bkg_estimationelapsed time: 0:53:12.79\n",
      "104000 of 208255 done of cms_bkg_estimationelapsed time: 0:53:42.99\n",
      "105000 of 208255 done of cms_bkg_estimationelapsed time: 0:54:13.21\n",
      "106000 of 208255 done of cms_bkg_estimationelapsed time: 0:54:45.83\n",
      "107000 of 208255 done of cms_bkg_estimationelapsed time: 0:55:15.85\n",
      "108000 of 208255 done of cms_bkg_estimationelapsed time: 0:55:46.63\n",
      "109000 of 208255 done of cms_bkg_estimationelapsed time: 0:56:19.40\n",
      "110000 of 208255 done of cms_bkg_estimationelapsed time: 0:56:49.59\n",
      "111000 of 208255 done of cms_bkg_estimationelapsed time: 0:57:19.78\n",
      "112000 of 208255 done of cms_bkg_estimationelapsed time: 0:57:52.62\n",
      "113000 of 208255 done of cms_bkg_estimationelapsed time: 0:58:22.47\n",
      "114000 of 208255 done of cms_bkg_estimationelapsed time: 0:58:52.55\n",
      "115000 of 208255 done of cms_bkg_estimationelapsed time: 0:59:25.37\n",
      "116000 of 208255 done of cms_bkg_estimationelapsed time: 0:59:56.11\n",
      "117000 of 208255 done of cms_bkg_estimationelapsed time: 1:00:26.09\n",
      "118000 of 208255 done of cms_bkg_estimationelapsed time: 1:00:58.74\n",
      "119000 of 208255 done of cms_bkg_estimationelapsed time: 1:01:28.12\n",
      "120000 of 208255 done of cms_bkg_estimationelapsed time: 1:01:58.31\n",
      "121000 of 208255 done of cms_bkg_estimationelapsed time: 1:02:28.28\n",
      "122000 of 208255 done of cms_bkg_estimationelapsed time: 1:03:01.22\n",
      "123000 of 208255 done of cms_bkg_estimationelapsed time: 1:03:31.32\n",
      "124000 of 208255 done of cms_bkg_estimationelapsed time: 1:04:01.30\n",
      "125000 of 208255 done of cms_bkg_estimationelapsed time: 1:04:33.83\n",
      "126000 of 208255 done of cms_bkg_estimationelapsed time: 1:05:03.85\n",
      "127000 of 208255 done of cms_bkg_estimationelapsed time: 1:05:33.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 of 208255 done of cms_bkg_estimationelapsed time: 1:06:06.53\n",
      "129000 of 208255 done of cms_bkg_estimationelapsed time: 1:06:36.44\n",
      "130000 of 208255 done of cms_bkg_estimationelapsed time: 1:07:06.32\n",
      "131000 of 208255 done of cms_bkg_estimationelapsed time: 1:07:38.84\n",
      "132000 of 208255 done of cms_bkg_estimationelapsed time: 1:08:08.73\n",
      "133000 of 208255 done of cms_bkg_estimationelapsed time: 1:08:38.37\n",
      "134000 of 208255 done of cms_bkg_estimationelapsed time: 1:09:10.95\n",
      "135000 of 208255 done of cms_bkg_estimationelapsed time: 1:09:40.92\n",
      "136000 of 208255 done of cms_bkg_estimationelapsed time: 1:10:10.69\n",
      "137000 of 208255 done of cms_bkg_estimationelapsed time: 1:10:43.81\n",
      "138000 of 208255 done of cms_bkg_estimationelapsed time: 1:11:13.77\n",
      "139000 of 208255 done of cms_bkg_estimationelapsed time: 1:11:43.54\n",
      "140000 of 208255 done of cms_bkg_estimationelapsed time: 1:12:16.18\n",
      "141000 of 208255 done of cms_bkg_estimationelapsed time: 1:12:46.09\n",
      "142000 of 208255 done of cms_bkg_estimationelapsed time: 1:13:15.86\n",
      "143000 of 208255 done of cms_bkg_estimationelapsed time: 1:13:48.28\n",
      "144000 of 208255 done of cms_bkg_estimationelapsed time: 1:14:17.88\n",
      "145000 of 208255 done of cms_bkg_estimationelapsed time: 1:14:47.67\n",
      "146000 of 208255 done of cms_bkg_estimationelapsed time: 1:15:20.08\n",
      "147000 of 208255 done of cms_bkg_estimationelapsed time: 1:15:49.76\n",
      "148000 of 208255 done of cms_bkg_estimationelapsed time: 1:16:19.80\n",
      "149000 of 208255 done of cms_bkg_estimationelapsed time: 1:16:52.66\n",
      "150000 of 208255 done of cms_bkg_estimationelapsed time: 1:17:22.62\n",
      "151000 of 208255 done of cms_bkg_estimationelapsed time: 1:17:52.85\n",
      "152000 of 208255 done of cms_bkg_estimationelapsed time: 1:18:25.66\n",
      "153000 of 208255 done of cms_bkg_estimationelapsed time: 1:18:55.59\n",
      "154000 of 208255 done of cms_bkg_estimationelapsed time: 1:19:25.77\n",
      "155000 of 208255 done of cms_bkg_estimationelapsed time: 1:19:58.83\n",
      "156000 of 208255 done of cms_bkg_estimationelapsed time: 1:20:28.53\n",
      "157000 of 208255 done of cms_bkg_estimationelapsed time: 1:20:58.94\n",
      "158000 of 208255 done of cms_bkg_estimationelapsed time: 1:21:31.92\n",
      "159000 of 208255 done of cms_bkg_estimationelapsed time: 1:22:02.58\n",
      "160000 of 208255 done of cms_bkg_estimationelapsed time: 1:22:32.88\n",
      "161000 of 208255 done of cms_bkg_estimationelapsed time: 1:23:02.79\n",
      "162000 of 208255 done of cms_bkg_estimationelapsed time: 1:23:35.41\n",
      "163000 of 208255 done of cms_bkg_estimationelapsed time: 1:24:05.53\n",
      "164000 of 208255 done of cms_bkg_estimationelapsed time: 1:24:36.47\n",
      "165000 of 208255 done of cms_bkg_estimationelapsed time: 1:25:16.44\n",
      "166000 of 208255 done of cms_bkg_estimationelapsed time: 1:25:51.60\n",
      "167000 of 208255 done of cms_bkg_estimationelapsed time: 1:26:23.11\n",
      "168000 of 208255 done of cms_bkg_estimationelapsed time: 1:26:55.59\n",
      "169000 of 208255 done of cms_bkg_estimationelapsed time: 1:27:25.71\n",
      "170000 of 208255 done of cms_bkg_estimationelapsed time: 1:27:55.61\n",
      "171000 of 208255 done of cms_bkg_estimationelapsed time: 1:28:28.08\n",
      "172000 of 208255 done of cms_bkg_estimationelapsed time: 1:28:58.95\n",
      "173000 of 208255 done of cms_bkg_estimationelapsed time: 1:29:29.14\n",
      "174000 of 208255 done of cms_bkg_estimationelapsed time: 1:30:02.27\n",
      "175000 of 208255 done of cms_bkg_estimationelapsed time: 1:30:32.59\n",
      "176000 of 208255 done of cms_bkg_estimationelapsed time: 1:31:02.64\n",
      "177000 of 208255 done of cms_bkg_estimationelapsed time: 1:31:35.20\n",
      "178000 of 208255 done of cms_bkg_estimationelapsed time: 1:32:05.46\n",
      "179000 of 208255 done of cms_bkg_estimationelapsed time: 1:32:35.41\n",
      "180000 of 208255 done of cms_bkg_estimationelapsed time: 1:33:08.18\n",
      "181000 of 208255 done of cms_bkg_estimationelapsed time: 1:33:38.14\n",
      "182000 of 208255 done of cms_bkg_estimationelapsed time: 1:34:08.11\n",
      "183000 of 208255 done of cms_bkg_estimationelapsed time: 1:34:40.89\n",
      "184000 of 208255 done of cms_bkg_estimationelapsed time: 1:35:10.78\n",
      "185000 of 208255 done of cms_bkg_estimationelapsed time: 1:35:40.85\n",
      "186000 of 208255 done of cms_bkg_estimationelapsed time: 1:36:13.58\n",
      "187000 of 208255 done of cms_bkg_estimationelapsed time: 1:36:43.49\n",
      "188000 of 208255 done of cms_bkg_estimationelapsed time: 1:37:13.39\n",
      "189000 of 208255 done of cms_bkg_estimationelapsed time: 1:37:46.02\n",
      "190000 of 208255 done of cms_bkg_estimationelapsed time: 1:38:15.83\n",
      "191000 of 208255 done of cms_bkg_estimationelapsed time: 1:38:45.80\n",
      "192000 of 208255 done of cms_bkg_estimationelapsed time: 1:39:18.57\n",
      "193000 of 208255 done of cms_bkg_estimationelapsed time: 1:39:48.50\n",
      "194000 of 208255 done of cms_bkg_estimationelapsed time: 1:40:18.68\n",
      "195000 of 208255 done of cms_bkg_estimationelapsed time: 1:40:52.05\n",
      "196000 of 208255 done of cms_bkg_estimationelapsed time: 1:41:21.96\n",
      "197000 of 208255 done of cms_bkg_estimationelapsed time: 1:41:52.20\n",
      "198000 of 208255 done of cms_bkg_estimationelapsed time: 1:42:21.71\n",
      "199000 of 208255 done of cms_bkg_estimationelapsed time: 1:42:54.47\n",
      "200000 of 208255 done of cms_bkg_estimationelapsed time: 1:43:24.35\n",
      "201000 of 208255 done of cms_bkg_estimationelapsed time: 1:43:54.30\n",
      "202000 of 208255 done of cms_bkg_estimationelapsed time: 1:44:26.73\n",
      "203000 of 208255 done of cms_bkg_estimationelapsed time: 1:44:56.79\n",
      "204000 of 208255 done of cms_bkg_estimationelapsed time: 1:45:26.69\n",
      "205000 of 208255 done of cms_bkg_estimationelapsed time: 1:45:59.33\n",
      "206000 of 208255 done of cms_bkg_estimationelapsed time: 1:46:29.40\n",
      "207000 of 208255 done of cms_bkg_estimationelapsed time: 1:46:59.27\n",
      "208000 of 208255 done of cms_bkg_estimationelapsed time: 1:47:31.90\n",
      "0 of 68108 done of qcd_bkg_estimationelapsed time: 0:00:00.03\n",
      "1000 of 68108 done of qcd_bkg_estimationelapsed time: 0:00:30.02\n",
      "2000 of 68108 done of qcd_bkg_estimationelapsed time: 0:00:59.90\n",
      "3000 of 68108 done of qcd_bkg_estimationelapsed time: 0:01:32.50\n",
      "4000 of 68108 done of qcd_bkg_estimationelapsed time: 0:02:02.31\n",
      "5000 of 68108 done of qcd_bkg_estimationelapsed time: 0:02:31.96\n",
      "6000 of 68108 done of qcd_bkg_estimationelapsed time: 0:03:04.56\n",
      "7000 of 68108 done of qcd_bkg_estimationelapsed time: 0:03:34.53\n",
      "8000 of 68108 done of qcd_bkg_estimationelapsed time: 0:04:04.50\n",
      "9000 of 68108 done of qcd_bkg_estimationelapsed time: 0:04:36.84\n",
      "10000 of 68108 done of qcd_bkg_estimationelapsed time: 0:05:06.92\n",
      "11000 of 68108 done of qcd_bkg_estimationelapsed time: 0:05:36.48\n",
      "12000 of 68108 done of qcd_bkg_estimationelapsed time: 0:06:09.22\n",
      "13000 of 68108 done of qcd_bkg_estimationelapsed time: 0:06:39.02\n",
      "14000 of 68108 done of qcd_bkg_estimationelapsed time: 0:07:08.79\n",
      "15000 of 68108 done of qcd_bkg_estimationelapsed time: 0:07:41.33\n",
      "16000 of 68108 done of qcd_bkg_estimationelapsed time: 0:08:11.81\n",
      "17000 of 68108 done of qcd_bkg_estimationelapsed time: 0:08:41.74\n",
      "18000 of 68108 done of qcd_bkg_estimationelapsed time: 0:09:14.16\n",
      "19000 of 68108 done of qcd_bkg_estimationelapsed time: 0:09:43.95\n",
      "20000 of 68108 done of qcd_bkg_estimationelapsed time: 0:10:13.98\n",
      "21000 of 68108 done of qcd_bkg_estimationelapsed time: 0:10:46.64\n",
      "22000 of 68108 done of qcd_bkg_estimationelapsed time: 0:11:16.57\n",
      "23000 of 68108 done of qcd_bkg_estimationelapsed time: 0:11:46.54\n",
      "24000 of 68108 done of qcd_bkg_estimationelapsed time: 0:12:19.96\n",
      "25000 of 68108 done of qcd_bkg_estimationelapsed time: 0:12:49.98\n",
      "26000 of 68108 done of qcd_bkg_estimationelapsed time: 0:13:19.71\n",
      "27000 of 68108 done of qcd_bkg_estimationelapsed time: 0:13:52.09\n",
      "28000 of 68108 done of qcd_bkg_estimationelapsed time: 0:14:21.78\n",
      "29000 of 68108 done of qcd_bkg_estimationelapsed time: 0:14:51.71\n",
      "30000 of 68108 done of qcd_bkg_estimationelapsed time: 0:15:24.49\n",
      "31000 of 68108 done of qcd_bkg_estimationelapsed time: 0:15:54.25\n",
      "32000 of 68108 done of qcd_bkg_estimationelapsed time: 0:16:24.19\n",
      "33000 of 68108 done of qcd_bkg_estimationelapsed time: 0:16:57.04\n",
      "34000 of 68108 done of qcd_bkg_estimationelapsed time: 0:17:26.97\n",
      "35000 of 68108 done of qcd_bkg_estimationelapsed time: 0:17:57.14\n",
      "36000 of 68108 done of qcd_bkg_estimationelapsed time: 0:18:29.77\n",
      "37000 of 68108 done of qcd_bkg_estimationelapsed time: 0:18:59.58\n",
      "38000 of 68108 done of qcd_bkg_estimationelapsed time: 0:19:29.77\n",
      "39000 of 68108 done of qcd_bkg_estimationelapsed time: 0:19:59.58\n",
      "40000 of 68108 done of qcd_bkg_estimationelapsed time: 0:20:32.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41000 of 68108 done of qcd_bkg_estimationelapsed time: 0:21:02.39\n",
      "42000 of 68108 done of qcd_bkg_estimationelapsed time: 0:21:32.41\n",
      "43000 of 68108 done of qcd_bkg_estimationelapsed time: 0:22:05.16\n",
      "44000 of 68108 done of qcd_bkg_estimationelapsed time: 0:22:35.21\n",
      "45000 of 68108 done of qcd_bkg_estimationelapsed time: 0:23:04.98\n",
      "46000 of 68108 done of qcd_bkg_estimationelapsed time: 0:23:38.20\n",
      "47000 of 68108 done of qcd_bkg_estimationelapsed time: 0:24:08.28\n",
      "48000 of 68108 done of qcd_bkg_estimationelapsed time: 0:24:38.30\n",
      "49000 of 68108 done of qcd_bkg_estimationelapsed time: 0:25:11.14\n",
      "50000 of 68108 done of qcd_bkg_estimationelapsed time: 0:25:41.31\n",
      "51000 of 68108 done of qcd_bkg_estimationelapsed time: 0:26:11.11\n",
      "52000 of 68108 done of qcd_bkg_estimationelapsed time: 0:26:43.92\n",
      "53000 of 68108 done of qcd_bkg_estimationelapsed time: 0:27:14.11\n",
      "54000 of 68108 done of qcd_bkg_estimationelapsed time: 0:27:44.00\n",
      "55000 of 68108 done of qcd_bkg_estimationelapsed time: 0:28:17.07\n",
      "56000 of 68108 done of qcd_bkg_estimationelapsed time: 0:28:47.24\n",
      "57000 of 68108 done of qcd_bkg_estimationelapsed time: 0:29:17.39\n",
      "58000 of 68108 done of qcd_bkg_estimationelapsed time: 0:29:50.27\n",
      "59000 of 68108 done of qcd_bkg_estimationelapsed time: 0:30:20.46\n",
      "60000 of 68108 done of qcd_bkg_estimationelapsed time: 0:30:50.48\n",
      "61000 of 68108 done of qcd_bkg_estimationelapsed time: 0:31:23.39\n",
      "62000 of 68108 done of qcd_bkg_estimationelapsed time: 0:31:53.60\n",
      "63000 of 68108 done of qcd_bkg_estimationelapsed time: 0:32:23.66\n",
      "64000 of 68108 done of qcd_bkg_estimationelapsed time: 0:32:56.51\n",
      "65000 of 68108 done of qcd_bkg_estimationelapsed time: 0:33:26.50\n",
      "66000 of 68108 done of qcd_bkg_estimationelapsed time: 0:33:57.50\n",
      "67000 of 68108 done of qcd_bkg_estimationelapsed time: 0:34:30.52\n",
      "68000 of 68108 done of qcd_bkg_estimationelapsed time: 0:35:00.38\n"
     ]
    }
   ],
   "source": [
    "# Normalizing input data\n",
    "def normalize_on_ht(data_set, dictionary, feature_list):\n",
    "    \"\"\"This function takes a data set, a dictionary describing the properties of the data and a feature list\n",
    "    and returns a normalization of values given in GeV in parts of HT.\n",
    "    :param data_set: flattened numpy array\n",
    "    :param dictionary: dictionary describing the properties of the values inside the data set\n",
    "    :param feature_list: list of features which are normalized with respect to ht value\n",
    "    :return returns the on ht normalized data set\"\"\"\n",
    "    data_set_return = np.copy(data_set)\n",
    "    for feature in feature_list:\n",
    "        if feature != dictionary['top.fitProb'] and feature != dictionary['jet.HT']:\n",
    "            data_set_return[:, dictionary[feature]] = data_set_return[:, dictionary[feature]] / data_set_return[:, dictionary['jet.HT']]\n",
    "    data_set_return[:, dictionary['jet.HT']] = data_set_return[:, dictionary['jet.HT']] / 5000\n",
    "    return np.array(data_set_return)\n",
    "\n",
    "\n",
    "def apply_model(input_data,\n",
    "                features,\n",
    "                model,\n",
    "                save_name):\n",
    "    \"\"\"This function takes  non-flattened input data, a list of features, a trained model and a save name and\n",
    "    returns a tuple: a scoring list with the same length as the input data and the data set itself with then only one\n",
    "    permutation per event remaining.\n",
    "    example: input data = = [event, event, event,... ] and  event = [permutation, permutation, ...]\n",
    "    output data = ([score_event_1, score_event_2, ...], [best_permutation_event_1, best_permutation_event_2, ...]\n",
    "    :param input_data: non-flattened numpy_array\n",
    "    :param features: list of features for the model\n",
    "    :param model: trained model,\n",
    "    :param save_name: name of file which the tuple will be saved\n",
    "    :return (list of scores, list of events)\n",
    "    \"\"\"\n",
    "    output_event_list = []\n",
    "    output_score_list = []\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    for num, event in enumerate(input_data):\n",
    "        normed_event = normalize_on_ht(event,\n",
    "                                       complete_feature_dictionary,\n",
    "                                       training_features)\n",
    "        permutation_number = -1\n",
    "        max_prediction = -1\n",
    "        predictions = model.predict(normed_event[:, features])\n",
    "        for k, perm in enumerate(normed_event):\n",
    "            # prediction = model.predict(np.expand_dims(perm[features], axis=0))[0][1]\n",
    "            # prediction = model.predict(perm[features])[1]\n",
    "            if predictions[k][1] > max_prediction:\n",
    "                max_prediction = predictions[k][1]\n",
    "                permutation_number = k\n",
    "        output_event_list.append(event[permutation_number])\n",
    "        output_score_list.append(max_prediction)\n",
    "        \n",
    "        if num % 100000 == 0:\n",
    "            one_thousand_elapsed = time.time() - start\n",
    "            print(str(num) + ' of ' + str(len(input_data)) + \n",
    "                  ' done of ' + \n",
    "                  save_name + \n",
    "                  ', elapsed time: ' \n",
    "                  + hms_string(one_thousand_elapsed))\n",
    "    np.save(save_name + '_predictions', np.array([output_score_list, output_event_list]))\n",
    "\n",
    "\n",
    "    \n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights('Run_' + str(randrange(K_FOLDING)) + '_weights_best_val_loss.h5')\n",
    "\n",
    "try: \n",
    "    os.mkdir('Applied Model outputs')\n",
    "except:\n",
    "    pass\n",
    "os.chdir('Applied Model outputs')\n",
    "\n",
    "# applying on data sets\n",
    "\n",
    "apply_model(TTbar_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'ttbar_172_5')\n",
    "apply_model(QCD_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'qcd')\n",
    "apply_model(CMS_data_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'cms_data')\n",
    "apply_model(CMS_background_estimation_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'cms_bkg_estimation')\n",
    "apply_model(QCD_background_estimation_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'qcd_bkg_estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Output distributions\n",
    "\n",
    "Now the output distributions and the results in numbers will be displayed. \n",
    "In this part the output distributions of MC ttbar and qcd samples are shown and the selection efficiency \n",
    "in contrast to cms data is calculated <br>\n",
    "1) output distributions for ttbar correct + $t\\bar{t}$ wrong + qcd <br>\n",
    "2) output distributions for ttbar correct + $t\\bar{t}$ wrong <br>\n",
    "3) comparing 1 and 2 btag output distributions in various plots <br>\n",
    "4) selection efficiency numbers <br>\n",
    "5) W mass distribution and $m_t^{fit}$ distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold value:  0.6 remaining ttbar:  56450.5 remaining cms:  227625.0 : selection efficiency:  24.8 %\n",
      "Threshold value:  0.61 remaining ttbar:  48666.1 remaining cms:  195029.0 : selection efficiency:  25.0 %\n",
      "Threshold value:  0.62 remaining ttbar:  41205.6 remaining cms:  164639.0 : selection efficiency:  25.0 %\n",
      "Threshold value:  0.63 remaining ttbar:  34273.0 remaining cms:  137255.0 : selection efficiency:  25.0 %\n",
      "Threshold value:  0.64 remaining ttbar:  28068.3 remaining cms:  112573.0 : selection efficiency:  24.9 %\n",
      "Threshold value:  0.65 remaining ttbar:  22675.6 remaining cms:  91196.0 : selection efficiency:  24.9 %\n",
      "Threshold value:  0.66 remaining ttbar:  18090.2 remaining cms:  72877.0 : selection efficiency:  24.8 %\n",
      "Threshold value:  0.67 remaining ttbar:  14203.3 remaining cms:  57455.0 : selection efficiency:  24.7 %\n",
      "Threshold value:  0.68 remaining ttbar:  10952.2 remaining cms:  44530.0 : selection efficiency:  24.6 %\n",
      "Threshold value:  0.69 remaining ttbar:  8242.7 remaining cms:  33680.0 : selection efficiency:  24.5 %\n",
      "Threshold value:  0.7 remaining ttbar:  6003.2 remaining cms:  24735.0 : selection efficiency:  24.3 %\n",
      "Threshold value:  0.71 remaining ttbar:  4168.4 remaining cms:  17329.0 : selection efficiency:  24.1 %\n",
      "Threshold value:  0.72 remaining ttbar:  2723.2 remaining cms:  11454.0 : selection efficiency:  23.8 %\n",
      "Threshold value:  0.73 remaining ttbar:  1646.9 remaining cms:  7037.0 : selection efficiency:  23.4 %\n",
      "Threshold value:  0.74 remaining ttbar:  934.4 remaining cms:  4017.0 : selection efficiency:  23.3 %\n",
      "Threshold value:  0.75 remaining ttbar:  478.2 remaining cms:  2202.0 : selection efficiency:  21.7 %\n",
      "Threshold value:  0.76 remaining ttbar:  227.3 remaining cms:  1056.0 : selection efficiency:  21.5 %\n",
      "Threshold value:  0.77 remaining ttbar:  99.0 remaining cms:  485.0 : selection efficiency:  20.4 %\n",
      "Threshold value:  0.78 remaining ttbar:  42.4 remaining cms:  196.0 : selection efficiency:  21.6 %\n",
      "Threshold value:  0.79 remaining ttbar:  23.4 remaining cms:  96.0 : selection efficiency:  24.4 %\n"
     ]
    }
   ],
   "source": [
    "os.chdir(workspace_folder)\n",
    "# os.chdir('CMS_Model_' + date_string)\n",
    "os.chdir('E:\\Jupyter Notebooks Workspace\\CMS_Model_2020_11_07_15_42_10')\n",
    "\n",
    "# set paths for output distributions\n",
    "data_1725_output_distribution = np.load('Applied Model outputs/ttbar_172_5_predictions.npy', allow_pickle=True)\n",
    "qcd_output_distribution = np.load('Applied Model outputs/qcd_predictions.npy', allow_pickle=True)\n",
    "cms_output_distribution = np.load('Applied Model outputs/cms_data_predictions.npy', allow_pickle=True)\n",
    "\n",
    "os.mkdir('output_distributions')\n",
    "os.chdir('output_distributions')\n",
    "\n",
    "current_folder = os.getcwd()                                            # needed for later directory changing\n",
    "\n",
    "correct_permutations_ttbar_scores = []\n",
    "wrong_permutations_ttbar_scores = []\n",
    "correct_permutations_ttbar_events = []\n",
    "wrong_permutations_ttbar_events = []\n",
    "\n",
    "two_btag_ttbar_scores = []\n",
    "one_btag_ttbar_scores = []\n",
    "two_btag_ttbar_events = []\n",
    "one_btag_ttbar_events = []\n",
    "\n",
    "two_btag_qcd_scores = []\n",
    "one_btag_qcd_scores = []\n",
    "two_btag_qcd_events = []\n",
    "one_btag_qcd_events = []\n",
    "\n",
    "# dividing by number of btags\n",
    "for qcd_score, qcd_event in zip(qcd_output_distribution[0], qcd_output_distribution[1]):\n",
    "    if qcd_event[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "        two_btag_qcd_scores.append(qcd_score)\n",
    "        two_btag_qcd_events.append(qcd_event)\n",
    "    else:\n",
    "        one_btag_qcd_scores.append(qcd_score)\n",
    "        one_btag_qcd_events.append(qcd_event)\n",
    "\n",
    "for ttbar_score, ttbar_event in zip(data_1725_output_distribution[0], data_1725_output_distribution[1]):\n",
    "    if ttbar_event[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "        two_btag_ttbar_scores.append(ttbar_score)\n",
    "        two_btag_ttbar_events.append(ttbar_event)\n",
    "    else:\n",
    "        one_btag_ttbar_scores.append(ttbar_score)\n",
    "        one_btag_ttbar_events.append(ttbar_event)\n",
    "\n",
    "# dividing by combination type\n",
    "for ttbar_score, ttbar_event in zip(data_1725_output_distribution[0], data_1725_output_distribution[1]):\n",
    "    if ttbar_event[complete_feature_dictionary['top.combinationType']] == 1:\n",
    "        correct_permutations_ttbar_scores.append(ttbar_score)\n",
    "        correct_permutations_ttbar_events.append(ttbar_event)\n",
    "    else:\n",
    "        wrong_permutations_ttbar_scores.append(ttbar_score)\n",
    "        wrong_permutations_ttbar_events.append(ttbar_event)\n",
    "\n",
    "# defining plot range\n",
    "plot_range = (0.2, 0.9)\n",
    "num_bins = 100\n",
    "lum = MC_lum\n",
    "\n",
    "# defining labels\n",
    "label_ttbar_correct = r'$t\\bar{t}$ correct'\n",
    "label_ttbar_wrong = r'$t\\bar{t}$ wrong'\n",
    "label_ttbar_one_btag = r'1 b-tag $t\\bar{t}$'\n",
    "label_ttbar_two_btag = r'2 b-tag $t\\bar{t}$'\n",
    "label_qcd = r'Multijet'\n",
    "\n",
    "# ttbar correct + wrong\n",
    "plt.figure()\n",
    "plt.hist([qcd_output_distribution[0],\n",
    "          correct_permutations_ttbar_scores,\n",
    "          wrong_permutations_ttbar_scores],\n",
    "         weights=[[qcd[complete_feature_dictionary[\n",
    "             'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for qcd in qcd_output_distribution[1]],\n",
    "                  [correct[complete_feature_dictionary[\n",
    "                      'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for correct in correct_permutations_ttbar_events],\n",
    "                  [wrong[complete_feature_dictionary[\n",
    "                      'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for wrong in wrong_permutations_ttbar_events]],\n",
    "         bins=num_bins,\n",
    "         stacked=True,\n",
    "         alpha=1,\n",
    "         range=plot_range,\n",
    "         edgecolor='k',\n",
    "         color=['grey', '#CC0000', '#FF6666'])\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'Events \\ 0.01', fontsize=13)\n",
    "plt.legend({label_qcd: 'grey', label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ttbar correct + wrong\n",
    "plt.figure()\n",
    "plt.hist([correct_permutations_ttbar_scores, wrong_permutations_ttbar_scores],\n",
    "         weights=[[correct[complete_feature_dictionary[\n",
    "             'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for correct in correct_permutations_ttbar_events],\n",
    "                  [wrong[complete_feature_dictionary[\n",
    "                      'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for wrong in wrong_permutations_ttbar_events]],\n",
    "         bins=num_bins,\n",
    "         stacked=True,\n",
    "         alpha=1,\n",
    "         range=plot_range,\n",
    "         edgecolor='k',\n",
    "         color=['#CC0000', '#FF6666'])\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'Events \\ 0.01', fontsize=13)\n",
    "plt.legend({label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ttbar 1 + 2 btag\n",
    "plt.figure()\n",
    "plt.hist(np.array(one_btag_ttbar_scores),\n",
    "         weights=lum * np.array(one_btag_ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                              'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         density=True,\n",
    "         histtype='step',\n",
    "         range=plot_range,\n",
    "         linewidth=2,\n",
    "         edgecolor='seagreen',\n",
    "         color='seagreen')\n",
    "\n",
    "plt.hist(np.array(two_btag_ttbar_scores),\n",
    "         weights=lum * np.array(two_btag_ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                              'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         density=True,\n",
    "         histtype='step',\n",
    "         range=plot_range,\n",
    "         linewidth=2,\n",
    "         edgecolor='lightsteelblue',\n",
    "         color='lightsteelblue')\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'normalized', fontsize=13)\n",
    "plt.legend({r'1 b-tag $t\\bar{t}$': 'seagreen',\n",
    "            r'2 b-tag $t\\bar{t}$': 'lightsteelblue'}, fontsize=13)\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations_btag_ttbar')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# qcd 1 + 2 btag\n",
    "plt.figure()\n",
    "plt.hist(np.array(one_btag_qcd_scores),\n",
    "         weights=lum * np.array(one_btag_qcd_events)[:, complete_feature_dictionary[\n",
    "                                                            'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         histtype='step',\n",
    "         density=True,\n",
    "         range=plot_range,\n",
    "         edgecolor='darkgreen',\n",
    "         linewidth=2,\n",
    "         color='darkgreen')\n",
    "\n",
    "plt.hist(np.array(two_btag_qcd_scores),\n",
    "         weights=lum * np.array(two_btag_qcd_events)[:, complete_feature_dictionary[\n",
    "                                                            'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         histtype='step',\n",
    "         density=True,\n",
    "         range=plot_range,\n",
    "         edgecolor='cornflowerblue',\n",
    "         linewidth=2,\n",
    "         color='cornflowerblue')\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'normalized', fontsize=13)\n",
    "plt.legend({r'1 b-tag multijet': 'darkgreen',\n",
    "            r'2 b-tag multijet': 'cornflowerblue'}, fontsize=13, loc='best')\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations_btag_qcd')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ttbar + qcd 2 btag\n",
    "plt.figure()\n",
    "plt.hist(np.array(two_btag_qcd_scores),\n",
    "         weights=lum * np.array(two_btag_qcd_events)[:, complete_feature_dictionary[\n",
    "                                                            'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         histtype='step',\n",
    "         density=True,\n",
    "         range=plot_range,\n",
    "         edgecolor='cornflowerblue',\n",
    "         linewidth=2,\n",
    "         color='cornflowerblue')\n",
    "\n",
    "plt.hist(np.array(two_btag_ttbar_scores),\n",
    "         weights=lum * np.array(two_btag_ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                              'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         density=True,\n",
    "         histtype='step',\n",
    "         range=plot_range,\n",
    "         linewidth=2,\n",
    "         edgecolor='lightsteelblue',\n",
    "         color='lightsteelblue')\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'normalized', fontsize=13)\n",
    "plt.legend({r'2 b-tag multijet': 'cornflowerblue',\n",
    "            r'2 b-tag $t\\bar{t}$': 'lightsteelblue'}, fontsize=13)\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations_2_btag_qcd_vs_ttbar')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ttbar + qcd 1 btag\n",
    "plt.figure()\n",
    "plt.hist(np.array(one_btag_qcd_scores),\n",
    "         weights=lum * np.array(one_btag_qcd_events)[:, complete_feature_dictionary[\n",
    "                                                            'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         histtype='step',\n",
    "         density=True,\n",
    "         range=plot_range,\n",
    "         edgecolor='darkgreen',\n",
    "         linewidth=2,\n",
    "         color='darkgreen')\n",
    "\n",
    "plt.hist(np.array(one_btag_ttbar_scores),\n",
    "         weights=lum * np.array(one_btag_ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                              'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         density=True,\n",
    "         histtype='step',\n",
    "         range=plot_range,\n",
    "         linewidth=2,\n",
    "         edgecolor='seagreen',\n",
    "         color='seagreen')\n",
    "\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'normalized', fontsize=13)\n",
    "plt.legend({r'1 b-tag multijet': 'darkgreen',\n",
    "            r'1 b-tag $t\\bar{t}$': 'seagreen'}, fontsize=13, loc='best')\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations_1_btag_qcd_vs_ttbar')\n",
    "plt.close()\n",
    "\n",
    "# calculating selection efficiencies\n",
    "suggested_threshold_values = output_threshold_values = [round(num, 3) for num in [0.6 + 0.01 * i for i in range(15)]]\n",
    "remaining_cms_data = np.zeros(len(suggested_threshold_values))\n",
    "remaining_ttbar_data = np.zeros(len(suggested_threshold_values))\n",
    "\n",
    "for position, threshold in enumerate(suggested_threshold_values):\n",
    "    m_t_fit_list_correct = []\n",
    "    m_W_reco_list_correct = []\n",
    "    event_weights_correct = []\n",
    "\n",
    "    m_t_fit_list_wrong = []\n",
    "    m_W_reco_list_wrong = []\n",
    "    event_weights_wrong = []\n",
    "\n",
    "    for ttbar_score, ttbar_event in zip(data_1725_output_distribution[0], data_1725_output_distribution[1]):\n",
    "        if ttbar_score >= threshold and ttbar_event[complete_feature_dictionary['n_bjets']] >= 1:\n",
    "            remaining_ttbar_data[position] += lum * ttbar_event[complete_feature_dictionary[\n",
    "                'combinedWeight_and_trigger_efficiency_correction']]\n",
    "            if ttbar_event[complete_feature_dictionary['top.combinationType']] == 1:\n",
    "                m_t_fit_list_correct.append(ttbar_event[complete_feature_dictionary['top.fitTop1.M']])\n",
    "                event_weights_correct.append(lum * ttbar_event[complete_feature_dictionary[\n",
    "                    'combinedWeight_and_trigger_efficiency_correction']])\n",
    "                m_W_reco_list_correct.append(\n",
    "                    0.5 * ttbar_event[complete_feature_dictionary['top.recoW1.M']] +\n",
    "                    0.5 * ttbar_event[complete_feature_dictionary['top.recoW1.M']])\n",
    "            else:\n",
    "                m_t_fit_list_wrong.append(ttbar_event[complete_feature_dictionary['top.fitTop1.M']])\n",
    "                event_weights_wrong.append(lum * ttbar_event[complete_feature_dictionary[\n",
    "                    'combinedWeight_and_trigger_efficiency_correction']])\n",
    "                m_W_reco_list_wrong.append(0.5 * ttbar_event[\n",
    "                    complete_feature_dictionary['top.recoW1.M']] +\n",
    "                                            0.5 * ttbar_event[\n",
    "                    complete_feature_dictionary['top.recoW2.M']])\n",
    "\n",
    "    for cms_score, cms_event in zip(cms_output_distribution[0], cms_output_distribution[1]):\n",
    "        if cms_score >= threshold and cms_event[complete_feature_dictionary['n_bjets']] >= 1:\n",
    "            remaining_cms_data[position] += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist([m_t_fit_list_correct, m_t_fit_list_wrong],\n",
    "             weights=[event_weights_correct, event_weights_wrong],\n",
    "             bins=75,\n",
    "             stacked=True,\n",
    "             alpha=1,\n",
    "             range=(100, 250),\n",
    "             edgecolor='k',\n",
    "             color=['#CC0000', '#FF6666'])\n",
    "\n",
    "    plt.xlabel('$m_t^{fit}$ [GeV]', fontsize=13)\n",
    "    plt.ylabel(r'Events \\ 2 GeV', fontsize=13)\n",
    "    plt.legend({label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "    plt.title(r'simulation $\\cdot$ private work'\n",
    "              '                  '\n",
    "              r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "    plt.savefig('m_t_fit_' + str(threshold).replace('.', '_'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist([m_W_reco_list_correct, m_W_reco_list_wrong],\n",
    "             weights=[event_weights_correct, event_weights_wrong],\n",
    "             bins=60,\n",
    "             stacked=True,\n",
    "             alpha=1,\n",
    "             range=(40, 160),\n",
    "             edgecolor='k',\n",
    "             color=['#CC0000', '#FF6666'])\n",
    "\n",
    "    plt.xlabel('$m_W^{reco}$ [GeV]', fontsize=13)\n",
    "    plt.ylabel(r'Events \\ 2 GeV', fontsize=13)\n",
    "    plt.legend({label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "    plt.title(r'simulation $\\cdot$ private work'\n",
    "              '                  '\n",
    "              r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "    plt.savefig('m_W_reco' + str(threshold).replace('.', '_'))\n",
    "    plt.close()\n",
    "\n",
    "for threshold, r_ttbar, r_cms in zip(suggested_threshold_values, remaining_ttbar_data, remaining_cms_data):\n",
    "    print('Threshold value: ',\n",
    "          threshold,\n",
    "          'remaining ttbar: ', np.around(r_ttbar, decimals=1),\n",
    "          'remaining cms: ', np.around(r_cms, decimals=1),\n",
    "          ': selection efficiency: ', np.around(100 * r_ttbar/r_cms, decimals=1), r'%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Background estimation and feature distributions correct/wrong and 1/2 btag(s)\n",
    "\n",
    "1) A background closure test between direct simulation and prediction from MC simulated samples and compares this\n",
    "   to prediction from cms data <br>\n",
    "2) Compare cms data and ttbar MC imulated samples. The difference between these data is \n",
    "   calculated and the background estimation from CMS data is scaled with a constant factor. <br>\n",
    "   --> CMS  = ttbar + scale_factor * background_estimation_CMS<br>\n",
    "3) The output distributions of ttbar data, divided into<br>\n",
    "   a) correct/wrong permutations <br>\n",
    "   b) 1/2 btag permutations <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anacdonda\\envs\\tensorflow_keras\\lib\\site-packages\\ipykernel_launcher.py:529: RuntimeWarning: divide by zero encountered in true_divide\n",
      "E:\\Anacdonda\\envs\\tensorflow_keras\\lib\\site-packages\\ipykernel_launcher.py:549: RuntimeWarning: divide by zero encountered in true_divide\n",
      "E:\\Anacdonda\\envs\\tensorflow_keras\\lib\\site-packages\\ipykernel_launcher.py:529: RuntimeWarning: invalid value encountered in true_divide\n",
      "E:\\Anacdonda\\envs\\tensorflow_keras\\lib\\site-packages\\ipykernel_launcher.py:549: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "os.chdir(workspace_folder)\n",
    "os.chdir('E:\\Jupyter Notebooks Workspace\\CMS_Model_2020_11_07_15_42_10')\n",
    "suggested_threshold_values = output_threshold_values = [round(num, 3) for num in [0.6 + 0.01 * i for i in range(15)]]\n",
    "\n",
    "# load data & predictions\n",
    "qcd_zero = np.load('Applied Model outputs/qcd_bkg_estimation_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "qcd = np.load('Applied Model outputs/qcd_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "cms_zero = np.load('Applied Model outputs/cms_bkg_estimation_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "cms = np.load('Applied Model outputs/cms_data_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "ttbar = np.load('Applied Model outputs/ttbar_172_5_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "\n",
    "# getting dictionries from cwola files\n",
    "range_bins_list = range_dict_input_features\n",
    "variable_list = features_of_interest\n",
    "x_label_list = x_label_dictionary\n",
    "\n",
    "# creating parent folder for file\n",
    "try:\n",
    "    os.mkdir('background_prediction_and_output_distributions')\n",
    "except:\n",
    "    pass\n",
    "os.chdir('background_prediction_and_output_distributions')\n",
    "background_prediction_and_output_distributions_folder = os.getcwd()\n",
    "\n",
    "# creating closure folder\n",
    "try:\n",
    "    os.mkdir('closure_tests')\n",
    "except:\n",
    "    pass\n",
    "os.chdir('closure_tests')\n",
    "closure_tests_folder = os.getcwd()\n",
    "\n",
    "# looping over threshold values and fill lists with remaining events\n",
    "for threshold_value in output_threshold_values:\n",
    "    qcd_zero_events = []\n",
    "    qcd_events = []\n",
    "    cms_zero_events = []\n",
    "    cms_events = []\n",
    "    ttbar_events = []\n",
    "\n",
    "    # creating folder with threshold value in name\n",
    "    try:\n",
    "        os.mkdir(str(threshold_value).replace('.', '_'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(str(threshold_value).replace('.', '_'))\n",
    "\n",
    "    for qcd_zero_score, qcd_zero_e in zip(qcd_zero[0], qcd_zero[1]):\n",
    "        if qcd_zero_score >= threshold_value:\n",
    "            qcd_zero_events.append(qcd_zero_e)\n",
    "\n",
    "    for qcd_score, qcd_e in zip(qcd[0], qcd[1]):\n",
    "        if qcd_score >= threshold_value:\n",
    "            qcd_events.append(qcd_e)\n",
    "\n",
    "    for cms_zero_score, cms_zero_e in zip(cms_zero[0], cms_zero[1]):\n",
    "        if cms_zero_score >= threshold_value:\n",
    "            cms_zero_events.append(cms_zero_e)\n",
    "\n",
    "    for cms_score, cms_e in zip(cms[0], cms[1]):\n",
    "        if cms_score >= threshold_value:\n",
    "            cms_events.append(cms_e)\n",
    "\n",
    "    for ttbar_score, ttbar_e in zip(ttbar[0], ttbar[1]):\n",
    "        if ttbar_score >= threshold_value:\n",
    "            ttbar_events.append(ttbar_e)\n",
    "\n",
    "    # looping over each variable of interest\n",
    "    for variable, x_label in zip(variable_list, x_label_list):\n",
    "        bins = 40\n",
    "        range_bins = range_bins_list[variable]\n",
    "        sum_events_zero_btags = 0\n",
    "        sum_events_1_btags = 0\n",
    "        sum_events_cms_qcd_estimation = 0\n",
    "\n",
    "        for event in qcd_events:\n",
    "            if event[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                pass\n",
    "            if event[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                pass\n",
    "            else:\n",
    "                sum_events_1_btags += event[complete_feature_dictionary[\n",
    "                    'combinedWeight_and_trigger_efficiency_correction']]\n",
    "\n",
    "        for event in qcd_zero_events:\n",
    "            if event[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                pass\n",
    "            if event[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                pass\n",
    "            else:\n",
    "                sum_events_zero_btags += event[complete_feature_dictionary[\n",
    "                    'combinedWeight_and_trigger_efficiency_correction']]\n",
    "\n",
    "        for event in cms_zero_events:\n",
    "            if event[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                pass\n",
    "            if event[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                pass\n",
    "            else:\n",
    "                sum_events_cms_qcd_estimation += event[complete_feature_dictionary[\n",
    "                    'weight.combinedWeight']]\n",
    "\n",
    "        if x_label == 'top.recoW1.M':                                 # calculating mean value of both W bosons\n",
    "            y_label = (range_bins[1] - range_bins[0]) / bins\n",
    "            bin_shift_for_plotting = y_label / 2\n",
    "\n",
    "            # getting weights for error bars qcd 2 btags\n",
    "            error_bars_marker = np.zeros(bins)\n",
    "            for entry in qcd_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW1.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_1_btags) ** 2\n",
    "\n",
    "            for entry in qcd_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW2.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_1_btags) ** 2\n",
    "\n",
    "            # getting weights for error bars qcd zero btags\n",
    "            error_bars_marker2 = np.zeros(bins)\n",
    "            for entry in qcd_zero_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW1.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker2[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_zero_btags) ** 2\n",
    "\n",
    "            for entry in qcd_zero_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW2.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker2[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_zero_btags) ** 2\n",
    "\n",
    "            # error bars marker cms_background estimation\n",
    "            error_bars_marker3 = np.zeros(bins)\n",
    "            for entry in cms_zero_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW1.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker3[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'weight.combinedWeight']] / sum_events_cms_qcd_estimation) ** 2\n",
    "\n",
    "            for entry in cms_zero_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW2.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker3[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'weight.combinedWeight']] / sum_events_cms_qcd_estimation) ** 2\n",
    "\n",
    "            # norming histograms to let them sum up to 1\n",
    "            qcd_allcut_1_btags_copy = np.copy(qcd_events)\n",
    "            qcd_allcut_zero_btags_copy = np.copy(qcd_zero_events)\n",
    "            cms_qcd_est_copy = np.copy(cms_zero_events)\n",
    "\n",
    "            qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] = \\\n",
    "                qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']]))\n",
    "\n",
    "            qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] = \\\n",
    "                qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']]))\n",
    "\n",
    "            cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'weight.combinedWeight']] = \\\n",
    "                cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'weight.combinedWeight']]))\n",
    "\n",
    "            n_qcd_1_btags, bins_qcd_1_btags, patches_qcd_1_btags = \\\n",
    "                plt.hist([0.5 * qcd_allcut_1_btags_copy[:, complete_feature_dictionary['top.recoW1.M']] +\n",
    "                          0.5 * qcd_allcut_1_btags_copy[:, complete_feature_dictionary['top.recoW2.M']]],\n",
    "                         weights=qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                                                'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            n_cms_qcd_est, bins_cms_qcd_est, patches_cms_qcd_est = \\\n",
    "                plt.hist([0.5 * cms_qcd_est_copy[:, complete_feature_dictionary['top.recoW1.M']] +\n",
    "                          0.5 * cms_qcd_est_copy[:, complete_feature_dictionary['top.recoW2.M']]],\n",
    "                         weights=cms_qcd_est_copy[:, complete_feature_dictionary['weight.combinedWeight']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            n_qcd_zero_btags, bins_qcd_zero, patches_qcd_zero_btags = \\\n",
    "                plt.hist(np.array(qcd_zero_events)[:, complete_feature_dictionary[variable]],\n",
    "                         weights=np.array(qcd_zero_events)[:, complete_feature_dictionary[\n",
    "                                                                  'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         density=True,\n",
    "                         color='grey')\n",
    "\n",
    "            plt.errorbar((bins_cms_qcd_est + bin_shift_for_plotting)[0:-1],\n",
    "                         n_cms_qcd_est,\n",
    "                         yerr=np.sqrt(error_bars_marker3) / sum_events_cms_qcd_estimation,\n",
    "                         color='b',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_qcd_1_btags + bin_shift_for_plotting)[0:-1],\n",
    "                         n_qcd_1_btags,\n",
    "                         yerr=np.sqrt(error_bars_marker),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_qcd_1_btags + bin_shift_for_plotting)[0:-1],\n",
    "                         n_qcd_zero_btags,\n",
    "                         yerr=np.sqrt(error_bars_marker2),\n",
    "                         color='grey',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.plot([], [], 'grey', label='prediction')\n",
    "            plt.plot([], [], 'k', label='direct simulation')\n",
    "            plt.plot([], [], 'blue', label='prediction (data)')\n",
    "            plt.ylim(bottom=0)\n",
    "\n",
    "            plt.legend(loc='best', fontsize=13)\n",
    "            plt.title(r'private work'\n",
    "                      '                                        '\n",
    "                      r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "\n",
    "            plt.ylabel(r'normalized', fontsize=13)\n",
    "            plt.xlabel(x_label_list[x_label], fontsize=13)\n",
    "\n",
    "            plt.savefig(\n",
    "                'QCD_estimation_closure_' + str(variable).replace('.', '_') + '_' +\n",
    "                str(threshold_value).replace('.', '_'))\n",
    "            plt.close()\n",
    "        elif x_label == 'top.recoW2.M':\n",
    "            continue\n",
    "        else:\n",
    "            y_label = (range_bins[1] - range_bins[0]) / bins\n",
    "            bin_shift_for_plotting = y_label / 2\n",
    "\n",
    "            # getting weights for error bars qcd 2 btags\n",
    "            error_bars_marker = np.zeros(bins)\n",
    "            for entry in qcd_events:\n",
    "                if entry[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary[variable]] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker[bin_number] += (entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_1_btags) ** 2\n",
    "\n",
    "            # getting weights for error bars qcd zero btags\n",
    "            error_bars_marker2 = np.zeros(bins)\n",
    "            for entry in qcd_zero_events:\n",
    "                if entry[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary[variable]] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker2[bin_number] += (entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_zero_btags) ** 2\n",
    "\n",
    "            error_bars_marker3 = np.zeros(bins)\n",
    "            for entry in cms_zero_events:\n",
    "                if entry[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary[variable]] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker3[bin_number] += (entry[complete_feature_dictionary[\n",
    "                        'weight.combinedWeight']] / sum_events_cms_qcd_estimation) ** 2\n",
    "\n",
    "            # norming histograms to let them sum up to 1\n",
    "            qcd_allcut_1_btags_copy = np.copy(qcd_events)\n",
    "            qcd_allcut_zero_btags_copy = np.copy(qcd_zero_events)\n",
    "            cms_qcd_est_copy = np.copy(cms_zero_events)\n",
    "\n",
    "            qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] = \\\n",
    "                qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']]))\n",
    "\n",
    "            qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] = \\\n",
    "                qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']]))\n",
    "\n",
    "            cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'weight.combinedWeight']] = \\\n",
    "                cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'weight.combinedWeight']]))\n",
    "\n",
    "            n_qcd_1_btags, bins_qcd_1_btags, patches_qcd_1_btags = \\\n",
    "                plt.hist(qcd_allcut_1_btags_copy[:, complete_feature_dictionary[variable]],\n",
    "                         weights=qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                                                'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            n_cms_qcd_est, bins_cms_qcd_est, patches_cms_qcd_est = \\\n",
    "                plt.hist(cms_qcd_est_copy[:, complete_feature_dictionary[variable]],\n",
    "                         weights=cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                                         'weight.combinedWeight']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            n_qcd_zero_btags, bins_qcd_zero_btags, patches_qcd_zero_btags = \\\n",
    "                plt.hist(qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[variable]],\n",
    "                         weights=qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                                                   'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            plt.errorbar((bins_cms_qcd_est + bin_shift_for_plotting)[0:-1],\n",
    "                         n_cms_qcd_est,\n",
    "                         yerr=np.sqrt(error_bars_marker3) / sum_events_cms_qcd_estimation,\n",
    "                         color='b',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_qcd_1_btags + bin_shift_for_plotting)[0:-1],\n",
    "                         n_qcd_zero_btags,\n",
    "                         yerr=np.sqrt(error_bars_marker2),\n",
    "                         color='grey',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_qcd_1_btags + bin_shift_for_plotting)[0:-1],\n",
    "                         n_qcd_1_btags,\n",
    "                         yerr=np.sqrt(error_bars_marker),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.plot([], [], 'grey', label='prediction')\n",
    "            plt.plot([], [], 'k', label='direct simulation')\n",
    "            plt.plot([], [], 'blue', label='prediction (data)')\n",
    "            plt.ylim(bottom=0)\n",
    "\n",
    "            plt.legend(loc='best', fontsize=13)\n",
    "            plt.title(r'private work'\n",
    "                      '                                        '\n",
    "                      r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "\n",
    "            plt.ylabel(r'normalized', fontsize=13)\n",
    "            plt.xlabel(x_label_list[x_label], fontsize=13)\n",
    "\n",
    "            plt.savefig(\n",
    "                'QCD_estimation_closure_' + str(variable).replace('.', '_') + '_' +\n",
    "                str(threshold_value).replace('.', '_'))\n",
    "            plt.close()\n",
    "\n",
    "    # create folder for background estiamtion\n",
    "    os.chdir(background_prediction_and_output_distributions_folder)\n",
    "    try:\n",
    "        os.mkdir('background_estimation')\n",
    "    except: pass\n",
    "    os.chdir('background_estimation')\n",
    "    try: \n",
    "        os.mkdir(str(threshold_value).replace('.', '_'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(str(threshold_value).replace('.', '_'))\n",
    "\n",
    "    for variable, x_label in zip(variable_list, x_label_list):\n",
    "        bins = 80\n",
    "        range_bins = range_bins_list[variable]\n",
    "        y_label = np.around(((range_bins[1] - range_bins[0]) / bins), decimals=2)\n",
    "        bin_shift_for_plotting = y_label / 2\n",
    "\n",
    "        if '[GeV]' in x_label:\n",
    "            y_label_string = r'events / ' + str(y_label) + ' GeV'\n",
    "        else:\n",
    "            y_label_string = r'events / ' + str(y_label)\n",
    "\n",
    "        n_cms, bins_cms, patches_cms = plt.hist(np.array(cms_events)[\n",
    "                                                :, complete_feature_dictionary[variable]],\n",
    "                                                weights=np.array(cms_events)[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                                   'weight.combinedWeight']],\n",
    "                                                range=range_bins,\n",
    "                                                bins=bins,\n",
    "                                                color='k')\n",
    "        n_ttbar, bins_ttbar, patches_ttbar = plt.hist(np.array(ttbar_events)[\n",
    "                                                      :, complete_feature_dictionary[variable]],\n",
    "                                                      weights=np.array(ttbar_events)[\n",
    "                                                              :, complete_feature_dictionary[\n",
    "                                                                     'combinedWeight_and_trigger_efficiency_correction']\n",
    "                                                              ] * MC_lum,\n",
    "                                                      range=range_bins,\n",
    "                                                      bins=bins,\n",
    "                                                      color='r')\n",
    "\n",
    "        n_cms_qcd_est, bins_cms_qcd_est, patches_cms_qcd_est = \\\n",
    "            plt.hist(np.array(cms_zero_events)[:, complete_feature_dictionary[variable]],\n",
    "                     weights=np.array(cms_zero_events)[:, complete_feature_dictionary[\n",
    "                                                              'weight.combinedWeight']],\n",
    "                     range=range_bins,\n",
    "                     bins=bins,\n",
    "                     color='grey')\n",
    "\n",
    "        n_res = n_cms - n_ttbar\n",
    "        scaling_factor = (sum(np.array(cms_events)[:, complete_feature_dictionary[\n",
    "                                                          'weight.combinedWeight']])\n",
    "                          - (sum(np.array(ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                             'combinedWeight_and_trigger_efficiency_correction']]) *\n",
    "                             MC_lum)) / sum(np.array(cms_zero_events)[:,\n",
    "                                                          complete_feature_dictionary[\n",
    "                                                              'weight.combinedWeight']])\n",
    "\n",
    "        plt.close()\n",
    "        fig1 = plt.figure(1)\n",
    "\n",
    "        frame1 = fig1.add_axes((.15, .3, .8, .6))\n",
    "\n",
    "        frame1.set_title('Remaining events after subtracting' + r'$t\\bar{t}$' + ' from CMS data')\n",
    "        if variable == 'top.fitTop1.M + ANSTATT AUSKOMMENTIERT':\n",
    "            # blind 140 - 210 GeV for mass\n",
    "            plt.errorbar((bins_cms + bin_shift_for_plotting)[0:8],\n",
    "                         n_cms[0:8],\n",
    "                         yerr=np.sqrt(n_cms[0:8]),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=1.75,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_cms + bin_shift_for_plotting)[22:-1],\n",
    "                         n_cms[22:],\n",
    "                         yerr=np.sqrt(n_cms[22:]),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=1.75,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "        else:\n",
    "            plt.errorbar((bins_cms + bin_shift_for_plotting)[0:-1],\n",
    "                         n_cms,\n",
    "                         yerr=np.sqrt(n_cms),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=1.75,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "        n_combined, bins_combined, patches_combined = \\\n",
    "            plt.hist([np.array(cms_zero_events)[:, complete_feature_dictionary[variable]],\n",
    "                      np.array(ttbar_events)[:, complete_feature_dictionary[variable]]],\n",
    "                     weights=[np.array(cms_zero_events)[:, complete_feature_dictionary[\n",
    "                                                               'weight.combinedWeight']] * scaling_factor,\n",
    "                              np.array(ttbar_events)[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] * MC_lum],\n",
    "                     stacked=True,\n",
    "                     range=range_bins,\n",
    "                     bins=bins,\n",
    "                     color=['grey', 'r'],\n",
    "                     edgecolor='k')\n",
    "\n",
    "        n_combined_total = n_combined[1]\n",
    "        factor_between_cms_background_prediction_plus_ttbar_mc_and_cms_data = n_cms / n_combined_total\n",
    "\n",
    "        frame1.plot([], [], 'k.', label='CMS data:' + '\\nN = ' + str(int(sum(n_cms))) + ' events')\n",
    "        frame1.plot([], [], 'r', label=r'$t\\bar{t}$ ' + 'MC:\\n' + 'N = ' + str(int(np.around(sum(n_ttbar), decimals=1)))\n",
    "                                       + ' events')\n",
    "        frame1.plot([], [], 'grey', label='scaled QCD Estimation \\nfrom 0 btag CMS data')\n",
    "\n",
    "        frame1.legend(loc='best')\n",
    "\n",
    "        plt.title(r'CMS data and MC $\\cdot$ private work'\n",
    "                  '                                                 '\n",
    "                  r'35.9 $fb^{-1}$ (13 TeV)', fontsize=9)\n",
    "\n",
    "        frame1.set_ylabel(y_label_string)\n",
    "        frame2 = fig1.add_axes((.15, .075, .8, .125))\n",
    "\n",
    "        # blind 140-210 GeV remove COMMENTED for blinding\n",
    "\n",
    "        frame2.errorbar(bins_cms[0:-1] + bin_shift_for_plotting,\n",
    "                        factor_between_cms_background_prediction_plus_ttbar_mc_and_cms_data,\n",
    "                        yerr=np.sqrt(n_cms) / n_combined_total,\n",
    "                        color='k',\n",
    "                        marker='o',\n",
    "                        markersize=1.75,\n",
    "                        markeredgewidth=1.75,\n",
    "                        linestyle='')\n",
    "\n",
    "        frame2.set_ylim([0, 2])\n",
    "        frame2.set_xlim([range_bins[0], range_bins[1]])\n",
    "        frame2.set_xticklabels([])  # Remove x-tic labels from the first frame\n",
    "        frame2.set_yticks([0, 1, 2])\n",
    "        frame2.set_ylabel(r'data / MC')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.savefig('QCD_background_prediction_' +\n",
    "                    str(variable).replace('.', '_') + '_' + str(threshold_value).replace('.', '_'))\n",
    "        plt.close()\n",
    "\n",
    "    # create folder for output distributions\n",
    "    os.chdir(background_prediction_and_output_distributions_folder)\n",
    "    try:\n",
    "        os.mkdir('output_distributions')\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir('output_distributions')\n",
    "    try:\n",
    "        os.mkdir(str(threshold_value).replace('.', '_'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(str(threshold_value).replace('.', '_'))\n",
    "\n",
    "    correct_permutations_ttbar = []\n",
    "    wrong_permutations_ttbar = []\n",
    "\n",
    "    one_btag_ttbar = []\n",
    "    two_btag_ttbar = []\n",
    "\n",
    "    one_btag_correct_permutation_ttbar = []\n",
    "    two_btag_correct_permutation_ttbar = []\n",
    "\n",
    "    one_btag_wrong_permutation_ttbar = []\n",
    "    two_btag_wrong_permutation_ttbar = []\n",
    "\n",
    "    for ttbar_event in ttbar_events:\n",
    "        if ttbar_event[complete_feature_dictionary['top.combinationType']] == 1:\n",
    "            correct_permutations_ttbar.append(ttbar_event)\n",
    "            if ttbar_event[complete_feature_dictionary['n_bjets']] == 2:\n",
    "                two_btag_correct_permutation_ttbar.append(ttbar_event)\n",
    "            else:\n",
    "                one_btag_correct_permutation_ttbar.append(ttbar_event)\n",
    "        else:\n",
    "            wrong_permutations_ttbar.append(ttbar_event)\n",
    "            if ttbar_event[complete_feature_dictionary['n_bjets']] == 2:\n",
    "                two_btag_wrong_permutation_ttbar.append(ttbar_event)\n",
    "            else:\n",
    "                one_btag_wrong_permutation_ttbar.append(ttbar_event)\n",
    "\n",
    "    correct_permutations_ttbar = np.array(correct_permutations_ttbar)\n",
    "    wrong_permutations_ttbar = np.array(wrong_permutations_ttbar)\n",
    "\n",
    "    one_btag_ttbar = np.array(one_btag_ttbar)\n",
    "    two_btag_ttbar = np.array(two_btag_ttbar)\n",
    "\n",
    "    one_btag_correct_permutation_ttbar = np.array(one_btag_correct_permutation_ttbar)\n",
    "    two_btag_correct_permutation_ttbar = np.array(two_btag_correct_permutation_ttbar)\n",
    "\n",
    "    one_btag_wrong_permutation_ttbar = np.array(one_btag_wrong_permutation_ttbar)\n",
    "    two_btag_wrong_permutation_ttbar = np.array(two_btag_wrong_permutation_ttbar)\n",
    "\n",
    "    label_ttbar_correct = r'$t\\bar{t}$ correct'\n",
    "    label_ttbar_wrong = r'$t\\bar{t}$ wrong'\n",
    "\n",
    "    for variable, x_label in zip(variable_list, x_label_list):\n",
    "        range_bins = range_bins_list[variable]\n",
    "\n",
    "        # correct / wrong permutations\n",
    "        n_D_perm, n_D_perm_bins, n_D_perm_patches = plt.hist([correct_permutations_ttbar[\n",
    "                                                              :, complete_feature_dictionary[variable]],\n",
    "                                                              wrong_permutations_ttbar[\n",
    "                                                              :, complete_feature_dictionary[variable]]],\n",
    "                                                             weights=[correct_permutations_ttbar[:, -1] *\n",
    "                                                                      MC_lum,\n",
    "                                                                      wrong_permutations_ttbar[:, complete_feature_dictionary[\n",
    "                                                                          'combinedWeight_and_trigger_efficiency_correction']] \n",
    "                                                                      * MC_lum],\n",
    "                                                             bins=BIN_NUMBER,\n",
    "                                                             stacked=True,\n",
    "                                                             alpha=1,\n",
    "                                                             edgecolor='k',\n",
    "                                                             range=range_bins,\n",
    "                                                             color=['#CC0000', '#FF6666'])\n",
    "\n",
    "        x_label_string = x_label\n",
    "        y_label = np.around(((range_bins[1] - range_bins[0]) / BIN_NUMBER), decimals=2)\n",
    "        if '[GeV]' in x_label:\n",
    "            y_label_string = r'Events / ' + str(y_label) + ' GeV'\n",
    "        else:\n",
    "            y_label_string = r'Events / ' + str(y_label)\n",
    "\n",
    "        plt.title(r'simulation $\\cdot$ private work'\n",
    "                  '                  '\n",
    "                  r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "        plt.xlabel(x_label_string, fontsize=13)\n",
    "        plt.ylabel(y_label_string, fontsize=13)\n",
    "        plt.legend({label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "        plt.savefig('Permutation_my_cut_' + str(variable).replace('.', '_') + str(threshold_value).replace('.', '_'))\n",
    "        plt.close()\n",
    "\n",
    "        # 1/2 btag + correct/wrong\n",
    "        n_D_, n_D_bins, n_D_patches = plt.hist([two_btag_correct_permutation_ttbar[\n",
    "                                                :, complete_feature_dictionary[variable]],\n",
    "                                                two_btag_wrong_permutation_ttbar[\n",
    "                                                :, complete_feature_dictionary[variable]],\n",
    "                                                one_btag_correct_permutation_ttbar[\n",
    "                                                :, complete_feature_dictionary[variable]],\n",
    "                                                one_btag_wrong_permutation_ttbar[\n",
    "                                                :, complete_feature_dictionary[variable]]],\n",
    "                                               weights=[two_btag_correct_permutation_ttbar[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                               'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                                                        * MC_lum,\n",
    "                                                        two_btag_wrong_permutation_ttbar[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                               'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                                                        * MC_lum,\n",
    "                                                        one_btag_correct_permutation_ttbar[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                               'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                                                        * MC_lum,\n",
    "                                                        one_btag_wrong_permutation_ttbar[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                               'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                                                        * MC_lum,],\n",
    "                                               bins=BIN_NUMBER,\n",
    "                                               stacked=True,\n",
    "                                               alpha=1,\n",
    "                                               edgecolor='k',\n",
    "                                               range=range_bins,\n",
    "                                               color=['green', 'lime', 'blue', 'royalblue'])\n",
    "\n",
    "        x_label_string = x_label\n",
    "        y_label = np.around(((range_bins[1] - range_bins[0]) / BIN_NUMBER), decimals=2)\n",
    "        if '[GeV]' in x_label:\n",
    "            y_label_string = r'Events / ' + str(y_label) + ' GeV'\n",
    "        else:\n",
    "            y_label_string = r'Events / ' + str(y_label)\n",
    "\n",
    "        plt.title(r'simulation $\\cdot$ private work'\n",
    "                  '                  '\n",
    "                  r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "        plt.xlabel(x_label_string, fontsize=13)\n",
    "        plt.ylabel(y_label_string, fontsize=13)\n",
    "        if variable == 'top.fitProb':\n",
    "            plt.yscale('log')\n",
    "        plt.legend({'2 b-tag ' + label_ttbar_correct: 'green',\n",
    "                    '2 b-tag ' + label_ttbar_wrong: 'lime',\n",
    "                    '1 b-tag ' + label_ttbar_correct: 'blue',\n",
    "                    '1 b-tag ' + label_ttbar_wrong: 'royalblue'\n",
    "                    }, fontsize=13)\n",
    "        plt.savefig('1vs2btag_my_cut_' + str(variable).replace('.', '_') + str(threshold_value).replace('.', '_'))\n",
    "        plt.close()\n",
    "\n",
    "    os.chdir(closure_tests_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow_keras)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
