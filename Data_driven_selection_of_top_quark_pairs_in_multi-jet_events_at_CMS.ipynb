{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data driven selection of top quark pairs in multi-jet events at CMS\n",
    "\n",
    "## This is meant to be a shortened description of my Master Thesis work and is BY NO MEANS optimized in performance or beautiful coding, but should be at least not hard to follow...\n",
    "\n",
    "1. Preselection\n",
    "2. Input Features and normalization <br>\n",
    "    1.1 Input feature distribution <br>\n",
    "    1.2 Normalization of data <br>\n",
    "3. Building the Neural Network\n",
    "4. Tests on MC simulated events <br>\n",
    "    4.1 MC Simulation $t\\bar{t}$ vs. QCD  <br>\n",
    "    4.2 MC Simulation $t\\bar{t}$ vs. QCD CWoLa <br>\n",
    "5. CWoLa on data\n",
    "6. Background estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preselection\n",
    "\n",
    "This work aims to select $t\\bar{t}$ events in a large background of QCD multi-jets. I would not like to go into detail here, but just show the $t\\bar{t}$ decay hypothesis and QCD multi-jet events faking this topology.\n",
    "![alt](img/ttbar_decay.png) | ![alt](img/QCD_multijet.png)\n",
    "\n",
    "The preselection is done to increase and imbalance signal fractions in mixed samples $M_1$ and $M_2$, which will be introduced in the CWoLa section. The preselection cutflow is displayed below. One event may consist of up to 90 different assignments of the 6 leading jets. Here, only the first 6 jets are used, ordered via $P_{GoF}$, a p-value, derived from the kinematic fit. The $\\chi^2$ value, calculated with the kinematic fit is displayed in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/cut_flow.png)\n",
    "![title](img/kinematic_fit_chi_squared.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Input Features and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One assignment of jets is described through 27 parameters in this thesis. The parameters are listed in the following. For better readability the description along with their corresponding place in the parameter list is stored in a dictionary:\n",
    "\n",
    "0: $P_T$ of b-jet 1 <br>\n",
    "1: $P_T$ of b-jet 2 <br>\n",
    "2: $P_T$ of product 1 of W boson 1 <br>\n",
    "3: $P_T$ of product 2 of W boson 1 <br>\n",
    "4: $P_T$ of product 1 of W boson 2 <br>\n",
    "5: $P_T$ of product 2 of W boson 2 <br>\n",
    "6: $\\Delta R_{b\\bar{b}}$ <br>\n",
    "7: b-tag value of b-jet 1 <br>\n",
    "8: b-tag value of b-jet 2 <br>\n",
    "9: b-tag value of product 1 of W boson 1 <br>\n",
    "10: b-tag value of product 2 of W boson 1 <br>\n",
    "11: b-tag value of product 1 of W boson 2 <br>\n",
    "12: b-tag value of product 2 of W boson 2 <br>\n",
    "13: reconstructed mass of W boson 1: $m_{W1}^{reco}$ <br>\n",
    "14: reconstructed mass of W boson 2: $m_{W2}^{reco}$ <br>\n",
    "15: fitted top mass: $m_t^{fit}$ <br>\n",
    "16: p-value obtained through $\\chi^2$ from the kinematik fit $P_{GoF}$ <br>\n",
    "17: combination type <br>\n",
    "18: decay channel <br>\n",
    "19: Hadronic Activity $H_T$ <br>\n",
    "20: $p_T$ of the 6th jet <br>\n",
    "21: # of b-assigned jets <br>\n",
    "22: lumiblock <br>\n",
    "23: run number <br>\n",
    "24: event number <br>\n",
    "25: combined weight <br>\n",
    "26: combined weight * trigger efficiency correction @dissertation Johannes Lange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_feature_dictionary = {'top.recoB1.Pt': 0,\n",
    "                               'top.recoB2.Pt': 1,\n",
    "                               'top.recoW1Prod1.Pt': 2,\n",
    "                               'top.recoW1Prod2.Pt': 3,\n",
    "                               'top.recoW2Prod1.Pt': 4,\n",
    "                               'top.recoW2Prod2.Pt': 5,\n",
    "                               'Delta_Rbb': 6,\n",
    "                               'jet.bTag_B1': 7,\n",
    "                               'jet.bTag_B2': 8,\n",
    "                               'jet.bTag_W1P1': 9,\n",
    "                               'jet.bTag_W1P2': 10,\n",
    "                               'jet.bTag_W2P1': 11,\n",
    "                               'jet.bTag_W2P2': 12,\n",
    "                               'top.recoW1.M': 13,\n",
    "                               'top.recoW2.M': 14,\n",
    "                               'top.fitTop1.M': 15,\n",
    "                               'top.fitProb': 16,\n",
    "                               'top.combinationType': 17,\n",
    "                               'top.decayChannel': 18,\n",
    "                               'jet.HT': 19,\n",
    "                               'jet.jet[5].Pt': 20,\n",
    "                               'n_bjets': 21,\n",
    "                               'top.lumiblock': 22,\n",
    "                               'top.run': 23,\n",
    "                               'top.event': 24,\n",
    "                               'weight.combinedWeight': 25,\n",
    "                               'combinedWeight_and_trigger_efficiency_correction': 26}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Input features\n",
    "\n",
    "Only few of this features are used for this project. Some of them are just used to verify the correctness of this analysis, like the unique combination of run number, event number and lumiblock, or for separating data like the assigned b-tag values and of course the weights of the events itself. \n",
    "\n",
    "The input featers which are used for the analysis are:\n",
    "\n",
    "0: $P_T$ of b-jet 1 <br>\n",
    "1: $P_T$ of b-jet 2 <br>\n",
    "2: $P_T$ of product 1 of W boson 1 <br>\n",
    "3: $P_T$ of product 2 of W boson 1 <br>\n",
    "4: $P_T$ of product 1 of W boson 2 <br>\n",
    "5: $P_T$ of product 2 of W boson 2 <br>\n",
    "13: reconstructed mass of W boson 1: $m_{W1}^{reco}$ <br>\n",
    "14: reconstructed mass of W boson 2: $m_{W2}^{reco}$ <br>\n",
    "15: fitted top mass: $m_t^{fit}$ <br>\n",
    "16: p-value obtained through $\\chi^2$ from the kinematik fit $P_{GoF}$ <br>\n",
    "19: Hadronic Activity $H_T$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = ['top.recoB1.Pt',\n",
    "                     'top.recoB2.Pt',\n",
    "                     'top.recoW1Prod1.Pt',\n",
    "                     'top.recoW1Prod2.Pt',\n",
    "                     'top.recoW2Prod1.Pt',\n",
    "                     'top.recoW2Prod2.Pt',\n",
    "                     'top.recoW1.M',\n",
    "                     'top.recoW2.M',\n",
    "                     'top.fitTop1.M',\n",
    "                     'top.fitProb',\n",
    "                     'jet.HT']\n",
    "\n",
    "features_of_interest =['top.recoB1.Pt',\n",
    "                       'top.recoB2.Pt',\n",
    "                       'top.recoW1Prod1.Pt',\n",
    "                       'top.recoW1Prod2.Pt',\n",
    "                       'top.recoW2Prod1.Pt',\n",
    "                       'top.recoW2Prod2.Pt',\n",
    "                       'Delta_Rbb',\n",
    "                       'top.recoW1.M',\n",
    "                       'top.recoW2.M',\n",
    "                       'top.fitTop1.M',\n",
    "                       'top.fitProb',\n",
    "                       'jet.HT',\n",
    "                       'jet.jet[5].Pt']\n",
    "\n",
    "x_label_dictionary = {'top.recoB1.Pt': '$p_T^{recoB1}$ [GeV]',\n",
    "                      'top.recoB2.Pt': '$p_T^{recoB2}$ [GeV]',\n",
    "                      'top.recoW1Prod1.Pt': '$p_T^{recoW1Prod1}$ [GeV]',\n",
    "                      'top.recoW1Prod2.Pt': '$p_T^{recoW1Prod2}$ [GeV]',\n",
    "                      'top.recoW2Prod1.Pt': '$p_T^{recoW2Prod1}$ [GeV]',\n",
    "                      'top.recoW2Prod2.Pt': '$p_T^{recoW2Prod2}$ [GeV]',\n",
    "                      'Delta_Rbb': r'$\\Delta R_{b\\bar{b}}$',\n",
    "                      'top.recoW1.M': '$m_{W1}^{reco}$ [GeV]',\n",
    "                      'top.recoW2.M': '$m_{W2}^{reco}$ [GeV]',\n",
    "                      'top.fitTop1.M': '$m_t^{fit}$ [GeV]',\n",
    "                      'top.fitProb': '$P_{GoF}$',\n",
    "                      'jet.HT': '$H_T$ [GeV]',\n",
    "                      'jet.jet[5].Pt': '$p_T^{jet[5]}$ [GeV]'}\n",
    "\n",
    "# dictionary for values where most of the data lies\n",
    "range_dict_features_of_interest = {'top.recoB1.Pt': (45, 300),\n",
    "                                   'top.recoB2.Pt': (45, 200),\n",
    "                                   'top.recoW1Prod1.Pt': (45, 300),\n",
    "                                   'top.recoW1Prod2.Pt': (45, 140),\n",
    "                                   'top.recoW2Prod1.Pt': (45, 400),\n",
    "                                   'top.recoW2Prod2.Pt': (45, 200),\n",
    "                                   'Delta_Rbb': (2, 4),\n",
    "                                   'top.recoW1.M': (60, 120),\n",
    "                                   'top.recoW2.M': (60, 120),\n",
    "                                   'top.fitTop1.M': (100, 300),\n",
    "                                   'top.fitProb': (0, 1),\n",
    "                                   'jet.HT': (450, 1250),\n",
    "                                   'jet.jet[5].Pt': (45, 140)}\n",
    "\n",
    "\n",
    "# number of different bins \n",
    "BIN_NUMBER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "# workspace folder\n",
    "workspace_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and divide into signal background and mixed samples M1 and M2, needs about 4 GB to process these lines, because the \n",
    "# data is in the form of [Event1, Event2, ...] with Event = [Permutation1, Permutation2, ...] and has to be converted into lists,\n",
    "# another approach could be realized if one uses inices and only original files, but since the used memory is not too large for \n",
    "# most computers and computing clusters it's ok at this point I guess... \n",
    "\n",
    "\n",
    "CMS_data_non_flattened = np.load('Thesis_data_after_preselection/CMS_1_b_tag_after_preselection.npy', \n",
    "                                 allow_pickle=True, \n",
    "                                 encoding='latin1')\n",
    "CMS_background_estimation_non_flattened = np.load('Thesis_data_after_preselection/CMS_0_b_tag_after_preselection.npy', \n",
    "                                                  allow_pickle=True, \n",
    "                                                  encoding='latin1')\n",
    "\n",
    "TTbar_non_flattened = np.load('Thesis_data_after_preselection/TTbar_1_b_tag_after_preselection.npy', \n",
    "                              allow_pickle=True, \n",
    "                              encoding='latin1') \n",
    "\n",
    "QCD_non_flattened = np.load('Thesis_data_after_preselection/QCD_1_b_tag_after_preselection.npy', \n",
    "                            allow_pickle=True, \n",
    "                            encoding='latin1')\n",
    "QCD_background_estimation_non_flattened = np.load('Thesis_data_after_preselection/QCD_0_b_tag_after_preselection.npy', \n",
    "                                                  allow_pickle=True, \n",
    "                                                  encoding='latin1')\n",
    "\n",
    "\n",
    "# MC luminosity factor (simulated events are normed to add up to 1 fb^(-1)\n",
    "MC_lum = 35.866203\n",
    "\n",
    "# preparing lists\n",
    "CMS = []\n",
    "CMS_1_btag = []\n",
    "CMS_2_btag = []\n",
    "\n",
    "TTbar = []\n",
    "TTbar_1_btag = []\n",
    "TTbar_2_btag = []\n",
    "\n",
    "QCD = []\n",
    "QCD_1_btag = []\n",
    "QCD_2_btag = []\n",
    "\n",
    "MC_Simulation_1_btag = []\n",
    "MC_Simulation_2_btag = []\n",
    "\n",
    "# CMS\n",
    "for event in CMS_data_non_flattened:\n",
    "    for permutation in event:\n",
    "        if permutation[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "            CMS_2_btag.append(permutation)\n",
    "        else:\n",
    "            CMS_1_btag.append(permutation)\n",
    "        CMS.append(permutation)\n",
    "            \n",
    "# TTbar\n",
    "for event in TTbar_non_flattened:\n",
    "    for permutation in event:\n",
    "        if permutation[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "            MC_Simulation_2_btag.append(permutation)\n",
    "            TTbar_2_btag.append(permutation)\n",
    "        else:\n",
    "            MC_Simulation_1_btag.append(permutation)\n",
    "            TTbar_1_btag.append(permutation)\n",
    "        TTbar.append(permutation)\n",
    "\n",
    "# QCD\n",
    "for event in QCD_non_flattened:\n",
    "    for permutation in event:\n",
    "        if permutation[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "            MC_Simulation_2_btag.append(permutation)\n",
    "            QCD_2_btag.append(permutation)\n",
    "        else:\n",
    "            MC_Simulation_1_btag.append(permutation)\n",
    "            QCD_1_btag.append(permutation)\n",
    "        QCD.append(permutation)\n",
    "        \n",
    "\n",
    "# Converting lists into numpy arrays\n",
    "CMS_1_btag = np.array(CMS_1_btag)\n",
    "CMS_2_btag = np.array(CMS_2_btag)\n",
    "CMS = np.array(CMS)\n",
    "\n",
    "TTbar = np.array(TTbar)\n",
    "TTbar_1_btag = np.array(TTbar_1_btag)\n",
    "TTbar_2_btag = np.array(TTbar_2_btag)\n",
    "\n",
    "QCD = np.array(QCD)\n",
    "QCD_1_btag = np.array(QCD_1_btag)\n",
    "QCD_2_btag = np.array(QCD_2_btag)\n",
    "\n",
    "MC_Simulation_1_btag = np.array(MC_Simulation_1_btag)\n",
    "MC_Simulation_2_btag = np.array(MC_Simulation_2_btag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the input features are displayed, divided into 1 and 2 b-tag events and $t\\bar{t}$ signal and QCD background. A folder is created and the figures are saved into it, if the folder doesn't already exist. Otherwise this step is skipped. Additionally the input distributions are displayed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# navigate to workspace folder and create new folder if it not existing already\n",
    "os.chdir(workspace_folder)\n",
    "\n",
    "if 'Input distributions' in os.listdir():\n",
    "    pass\n",
    "else:\n",
    "    new_folder = os.mkdir('Input distributions')\n",
    "os.chdir('Input distributions')\n",
    "\n",
    "\n",
    "# compare number of events\n",
    "print('Sum events CMS data: ' \n",
    "      + str(sum(CMS_2_btag[:, complete_feature_dictionary['weight.combinedWeight']])\n",
    "            + sum(CMS_1_btag[:, complete_feature_dictionary['weight.combinedWeight']])))\n",
    "print('Sum events MC simulation: ' \n",
    "      + str(np.around(MC_lum * (sum(TTbar[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']]) \n",
    "                                + sum(QCD[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']])),\n",
    "                      decimals=0)))\n",
    "\n",
    "\n",
    "# comparing CMS and MC simulated events for every chosen feature\n",
    "for plot_number, feature in enumerate(features_of_interest):\n",
    "\n",
    "    fig1 = plt.figure(plot_number)\n",
    "    frame1 = fig1.add_axes((.15, .3, .8, .6))\n",
    "    \n",
    "    n_cms_1b, bins_cms_1b = \\\n",
    "        np.histogram(CMS_1_btag[:, complete_feature_dictionary[feature]],\n",
    "                     bins=BIN_NUMBER,\n",
    "                     range=range_dict_features_of_interest[feature])\n",
    "    n_cms_2b, bins_cms_2b = \\\n",
    "        np.histogram(CMS_2_btag[:, complete_feature_dictionary[feature]],\n",
    "                     bins=BIN_NUMBER,\n",
    "                     range=range_dict_features_of_interest[feature])\n",
    "    \n",
    "    n, bins, patches = \\\n",
    "        frame1.hist([TTbar_2_btag[:, complete_feature_dictionary[feature]],\n",
    "                     QCD_2_btag[:, complete_feature_dictionary[feature]],\n",
    "                     TTbar_1_btag[:, complete_feature_dictionary[feature]],\n",
    "                     QCD_1_btag[:, complete_feature_dictionary[feature]]],\n",
    "                    weights=[TTbar_2_btag[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']] \n",
    "                             * MC_lum,\n",
    "                             QCD_2_btag[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']] \n",
    "                             * MC_lum,\n",
    "                             TTbar_1_btag[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']] \n",
    "                             * MC_lum,\n",
    "                             QCD_1_btag[:, complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction']] \n",
    "                             * MC_lum],\n",
    "                    bins=BIN_NUMBER,\n",
    "                    stacked=True,\n",
    "                    alpha=1,\n",
    "                    edgecolor='k',\n",
    "                    range=(range_dict_features_of_interest[feature]),\n",
    "                    color=['lightsteelblue', 'cornflowerblue', 'seagreen', 'darkgreen'],\n",
    "                    label= ['_nolegend_', '_nolegend_', '_nolegend_', '_nolegend_'])\n",
    "\n",
    "    frame1.plot(bins_cms_1b[0:-1],\n",
    "                n_cms_2b,\n",
    "                color='k',\n",
    "                marker='o',\n",
    "                markersize=2,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='')\n",
    "    frame1.plot(bins_cms_1b[0:-1],\n",
    "                n_cms_2b + n_cms_1b,\n",
    "                color='r',\n",
    "                marker='s',\n",
    "                markersize=2,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='')\n",
    "\n",
    "    frame1.set_xticklabels([])  # Remove x-tic labels from the first frame\n",
    "\n",
    "    x_label_string = x_label_dictionary[feature]\n",
    "    y_label = np.around(((range_dict_features_of_interest[feature][1] - range_dict_features_of_interest[feature][0]) /\n",
    "                         BIN_NUMBER), decimals=2)\n",
    "    bin_shift_for_plotting = y_label / 2                                 # schifts residual plots in the\n",
    "\n",
    "    if '[GeV]' in x_label_dictionary[feature]:             # GeV values on x axis need a unit\n",
    "        y_label_string = r'Events / ' + str(y_label) + ' GeV'\n",
    "    else:\n",
    "        y_label_string = r'Events / ' + str(y_label)\n",
    "\n",
    "    plt.title(r'private work'\n",
    "              '                          '\n",
    "              r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "    frame1.set_ylabel(y_label_string, fontsize=13)\n",
    "\n",
    "    if feature == 'top.fitProb':                                          # here log scale, because > 95 % of data\n",
    "        frame1.set_yscale('log')                                          # have P_GoF < 0.1\n",
    "\n",
    "    frame1.legend({r'2 b-tag data': 'k',\n",
    "                   r'1 b-tag data': 'r',\n",
    "                   r'2 b-tag $t\\bar{t}$': 'lightsteelblue',\n",
    "                   r'2 b-tag multijet': 'cornflowerblue',\n",
    "                   r'1 b-tag $t\\bar{t}$': 'seagreen',\n",
    "                   r'1 b-tag multijet': 'darkgreen'},\n",
    "                  fontsize=9,\n",
    "                  loc='best')\n",
    "\n",
    "    frame2 = fig1.add_axes((.15, .11, .8, .175))\n",
    "\n",
    "    # carefully addressing stacked histogram\n",
    "    normalized_ttbar_complete = (n[2] - n[1] + n[0]) / sum(n[2] - n[1] + n[0])\n",
    "    normalized_qcd_complete = (n[3] - n[2] + n[1] - n[0]) / sum(n[3] - n[2] + n[1] - n[0])\n",
    "\n",
    "    normalized_1_btag = (n[3] - n[1]) / sum(n[3] - n[1])\n",
    "    normalized_2_btag = n[1] / sum(n[1])\n",
    "\n",
    "    normalized_ttbar_1_btag = (n[2] - n[1]) / sum(n[2] - n[1])\n",
    "    normalized_ttbar_2_btag = n[0] / sum(n[0])\n",
    "\n",
    "    normalized_qcd_1_tag = (n[3] - n[2]) / sum(n[3] - n[2])\n",
    "    normalized_qcd_2_tag = (n[1] - n[0]) / sum(n[1] - n[0])\n",
    "\n",
    "    normalized_ttbar_qcd_ratio_complete = normalized_ttbar_complete / normalized_qcd_complete\n",
    "    normalized_ttbar_qcd_ratio_1_btag = normalized_ttbar_1_btag / normalized_qcd_1_tag\n",
    "    normalized_ttbar_qcd_ratio_2_btag = normalized_ttbar_2_btag / normalized_qcd_2_tag\n",
    "    normalized_2_btag_1_b_tag_ratio = normalized_2_btag / normalized_1_btag\n",
    "\n",
    "    frame2.plot((bins + bin_shift_for_plotting)[0:-1],\n",
    "                normalized_ttbar_qcd_ratio_complete,\n",
    "                color='k',\n",
    "                marker='x',\n",
    "                markersize=3,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='',\n",
    "                label=r'$t\\bar{t}$ / QCD complete')\n",
    "    frame2.plot((bins + bin_shift_for_plotting)[0:-1],\n",
    "                normalized_2_btag_1_b_tag_ratio,\n",
    "                color='g',\n",
    "                marker='o',\n",
    "                markersize=3,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='',\n",
    "                label='2 b-tags / 1 b-tags')\n",
    "    frame2.plot((bins + bin_shift_for_plotting)[0:-1],\n",
    "                normalized_ttbar_qcd_ratio_1_btag,\n",
    "                color='r',\n",
    "                marker='^',\n",
    "                markersize=3,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='',\n",
    "                label=r'$t\\bar{t}$ / QCD 1 b-tag')\n",
    "    frame2.plot((bins + bin_shift_for_plotting)[0:-1],\n",
    "                normalized_ttbar_qcd_ratio_2_btag,\n",
    "                color='b',\n",
    "                marker='v',\n",
    "                markersize=3,\n",
    "                markeredgewidth=1.75,\n",
    "                linestyle='',\n",
    "                label=r'$t\\bar{t}$ / QCD 2 b-tags')\n",
    "    \n",
    "    if feature == 'top.fitProb':                                          # here log scale, because > 95 % of data\n",
    "        frame2.set_yscale('log') \n",
    "    frame2.legend(loc='right', fontsize=9)\n",
    "    frame2.set_xlabel(x_label_string, fontsize=9)\n",
    "    plt.savefig('Input_distribution_' + feature.replace('.', '_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features interesting for the CWoLa approach should have the green dots in the ratio plots around the value 1,\n",
    "while the other markers should differ from that. This would mean, that a separation in terms of signal ($t\\bar{t}$) and background (QCD) is possible, while no discrimination for the number of b-tags per example is possible. So it might be possible to leave out the features which would accodriding to the just given explanation, but there is a reason to not leave them out. Referring to the $p_T$ values - to have all 6 jets in the training process is desired, since they can be confused by the kinematic fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2 Normalization\n",
    "\n",
    "The Normalization is done on $H_T$, the hadronic activity of an event with $H_T = \\sum_{i=0}^{5}p_T^{jet[i]}$. Each value, if \n",
    "measured in $p_T$, is divided by the $H_T$ value, while the $H_T$ value itself is just divided by a factor of 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing input data\n",
    "def normalize_on_ht(data_set, dictionary, feature_list):\n",
    "    \"\"\"This function takes a data set, a dictionary describing the properties of the data and a feature list\n",
    "    and returns a normalization of values given in GeV in parts of HT.\n",
    "    :param data_set: flattened numpy array\n",
    "    :param dictionary: dictionary describing the properties of the values inside the data set\n",
    "    :param feature_list: list of features which are normalized with respect to ht value\n",
    "    :return returns the on ht normalized data set\"\"\"\n",
    "    data_set_return = np.copy(data_set)\n",
    "    for feature in feature_list:\n",
    "        if feature != dictionary['top.fitProb'] and feature != dictionary['jet.HT']:\n",
    "            data_set_return[:, dictionary[feature]] = data_set_return[:, dictionary[feature]] / data_set_return[:, dictionary['jet.HT']]\n",
    "    data_set_return[:, dictionary['jet.HT']] = data_set_return[:, dictionary['jet.HT']] / 5000\n",
    "    return data_set_return\n",
    "\n",
    "\n",
    "# Normalizing arrays\n",
    "CMS_1_btag = normalize_on_ht(CMS_1_btag, complete_feature_dictionary, training_features)\n",
    "CMS_2_btag = normalize_on_ht(CMS_2_btag, complete_feature_dictionary, training_features)\n",
    "CMS = normalize_on_ht(CMS, complete_feature_dictionary, training_features)\n",
    "\n",
    "TTbar = normalize_on_ht(TTbar, complete_feature_dictionary, training_features)\n",
    "TTbar_1_btag = normalize_on_ht(TTbar_1_btag, complete_feature_dictionary, training_features)\n",
    "TTbar_2_btag = normalize_on_ht(TTbar_2_btag, complete_feature_dictionary, training_features)\n",
    "\n",
    "QCD = normalize_on_ht(QCD, complete_feature_dictionary, training_features)\n",
    "QCD_1_btag = normalize_on_ht(QCD_1_btag, complete_feature_dictionary, training_features)\n",
    "QCD_2_btag = normalize_on_ht(QCD_2_btag, complete_feature_dictionary, training_features)\n",
    "\n",
    "MC_Simulation_1_btag = normalize_on_ht(MC_Simulation_1_btag, complete_feature_dictionary, training_features)\n",
    "MC_Simulation_2_btag = normalize_on_ht(MC_Simulation_2_btag, complete_feature_dictionary, training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Buidling the Neural Network\n",
    "\n",
    "Pease note, that this is a very basic implementation. The following code box can be replaced with a more sophisticated model, but keras gives an easy way to create a very basic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer,\n",
    "                 nodes_list=(16, 32, 64, 32, 16, 8),\n",
    "                 batch_normalization=True,\n",
    "                 activation_function='selu',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 bias_initializer='glorot_normal',\n",
    "                 batch_size=1024,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics='accuracy'):\n",
    "\n",
    "    \"\"\" creates a sequential model and writes a file with some of the training history\n",
    "    :param nodes_list list of nodes for each dense layer, has to be the size of num_dense_layers\n",
    "    :param batch_normalization if True batch normalization is used after each layer\n",
    "    :param activation_function the used activation function for all but the last layer\n",
    "    :param kernel_initializer kernel initializer, default 'he_uniform' like the one in the CWoLa paper\n",
    "    :param bias_initializer bias initializer, default 'he_uniform' like the one in the CWoLa paper\n",
    "    :param batch_size size of the used batches for training\n",
    "    :param optimizer self build optimizer or adam with default values\n",
    "    :param loss function of the model\n",
    "    :param metrics tuple of metrics\n",
    "    :return created model\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    for i in range(len(nodes_list)):\n",
    "        model.add(Dense(nodes_list[i],\n",
    "                        activation=activation_function,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        bias_initializer=bias_initializer,\n",
    "                        batch_size=batch_size))\n",
    "        if batch_normalization and i != len(nodes_list) - 1:      # Never add batch normalization before softmax layer\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# specifying parameters for optimizer\n",
    "LR = 1e-3\n",
    "EPOCHS = 2 # default = 50\n",
    "DECAY = LR / EPOCHS\n",
    "BETA_1 = 0.6\n",
    "BETA_2 = 0.85\n",
    "AMSGRAD = True\n",
    "\n",
    "# Preparing k-fold cross validation\n",
    "K_FOLDING = 3 # default = 10\n",
    "\n",
    "adam = optimizers.Adam(lr=LR, decay=DECAY, beta_1=BETA_1, beta_2=BETA_2, amsgrad=AMSGRAD)\n",
    "\n",
    "print('############################################################')\n",
    "print('Using adam optimizer with the following parameters:\\n' +\n",
    "      'learning rate:', LR, '\\n'\n",
    "      'decay:', DECAY,  '\\n'\n",
    "      'beta1:', BETA_1, '\\n'\n",
    "      'beta2:', BETA_2, '\\n'\n",
    "      'amsgrad:', AMSGRAD)\n",
    "print('############################################################', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two useful functions for scaling weights and for implementing a k fold cross validataion with \n",
    "# respect to the weighting parameter of the samples\n",
    "def scaling_weights(data, weight_position, number_normed_events=1e6):\n",
    "    \"\"\" Normalizes input data, so weights of background and signal add up to the same value\n",
    "    :param data data which has to be scaled\n",
    "    :param weight_position position of the weight parameter\n",
    "    :param number_normed_events re-weighting to 1 Million events per sample\n",
    "    :return returns reweighted sample\"\"\"\n",
    "\n",
    "    data_copy = np.copy(data)\n",
    "    scaling_s = number_normed_events / sum(data[:, weight_position])\n",
    "\n",
    "    data_copy[:, weight_position] = data[:, weight_position] * scaling_s\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "def k_fold_cross_validation(k_folding, x_s, x_bg, weight_position):\n",
    "    \"\"\"Divides the data into k_folding equal weighted parts\n",
    "     :param k_folding number of different parts\n",
    "     :param x_s signal data\n",
    "     :param x_bg background data\n",
    "     :param weight_position -1 for MC and -2 for CMS data\n",
    "     :return list of 2 times k_folding fractions\"\"\"\n",
    "    np.random.shuffle(x_s)\n",
    "    np.random.shuffle(x_bg)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    j = 0\n",
    "    k = 0\n",
    "\n",
    "    j_index = 0\n",
    "    k_index = 0\n",
    "\n",
    "    x_s_sum = sum(x_s[:, weight_position])\n",
    "    x_bg_sum = sum(x_bg[:, weight_position])\n",
    "\n",
    "    for k_times in range(1, k_folding + 1):\n",
    "        x_sample = []\n",
    "        y_sample = []\n",
    "        while j < k_times * 1 / k_folding * x_s_sum:\n",
    "            x_sample.append(x_s[j_index, :])\n",
    "            y_sample.append(1)\n",
    "            j += x_s[j_index, weight_position]\n",
    "            j_index += 1\n",
    "        while k < k_times * 1 / k_folding * x_bg_sum:\n",
    "            x_sample.append(x_bg[k_index, :])\n",
    "            y_sample.append(0)\n",
    "            k += x_bg[k_index, weight_position]\n",
    "            k_index += 1\n",
    "\n",
    "        randomize = np.arange(len(y_sample))\n",
    "        np.random.shuffle(randomize)\n",
    "        x_sample = np.array(x_sample)[randomize, :]\n",
    "        y_sample = np.array(y_sample)[randomize]\n",
    "\n",
    "        result.append([x_sample, y_sample])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tests on MC simulated events\n",
    "\n",
    "Before using the CWoLa approach on data, one has to verify if the Neural Network configuration along with \n",
    "the MC simulated samples to get an idea how good the classification might be. The code for the training is not optimzed\n",
    "for runtime or for using gpu, but for good readability and to understand whats going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1  MC Simulation $t\\bar{t}$ vs. QCD\n",
    "\n",
    "First the training and evulation is done on MC simualted events. This will be the benchmark on which the CWoLa training will be measured in terms of AUC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reweighting samples to match equal weighting\n",
    "x_signal_normalized_reweighted = \\\n",
    "    scaling_weights(TTbar,\n",
    "                    complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "x_background_normalized_reweighted = \\\n",
    "    scaling_weights(QCD,\n",
    "                    complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "\n",
    "# define model output location over date string\n",
    "date_string = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "os.chdir(workspace_folder)\n",
    "os.mkdir('MC_Model_' + date_string)\n",
    "os.chdir('MC_Model_' + date_string)\n",
    "os.mkdir('Model_history')\n",
    "\n",
    "# Take a self written k-folding algorithm, because\n",
    "tenfold_split_data = k_fold_cross_validation(K_FOLDING,\n",
    "                                             x_signal_normalized_reweighted,\n",
    "                                             x_background_normalized_reweighted,\n",
    "                                             complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'])\n",
    "\n",
    "\n",
    "for i in range(K_FOLDING):\n",
    "    print('Run k = ', i, '\\n')\n",
    "    X_tr = []\n",
    "    y_tr = []\n",
    "    for k_fold in range(K_FOLDING):\n",
    "        if k_fold != i:\n",
    "            for event, label in zip(tenfold_split_data[k_fold][0], tenfold_split_data[k_fold][1]):\n",
    "                X_tr.append(event)\n",
    "                y_tr.append(label)\n",
    "\n",
    "    X_tr = np.array(X_tr)\n",
    "    y_tr = np.array(y_tr)\n",
    "\n",
    "    X_val = np.array(tenfold_split_data[i][0])\n",
    "    y_val = np.array(tenfold_split_data[i][1])\n",
    "\n",
    "    file_path_val_loss = 'Run_' + str(i) + '_weights_best_val_loss.h5'\n",
    "    file_path_val_accuracy = 'Run_' + str(i) + '_weights_best_val_accuracy.h5'\n",
    "    \n",
    "    classifier = create_model(adam,\n",
    "                          nodes_list=(16, 32, 64, 32, 16, 8),\n",
    "                          batch_normalization=True,\n",
    "                          activation_function='selu',\n",
    "                          kernel_initializer='glorot_normal',\n",
    "                          bias_initializer='glorot_normal',\n",
    "                          batch_size=1024,\n",
    "                          loss='sparse_categorical_crossentropy',\n",
    "                          metrics='accuracy')\n",
    "    \n",
    "    # checking for lowest val loss and accuracy\n",
    "    checkpoint_val_loss = \\\n",
    "        ModelCheckpoint(file_path_val_loss,\n",
    "                        monitor='val_loss',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode='min')\n",
    "\n",
    "    checkpoint_val_accuracy = \\\n",
    "        ModelCheckpoint(file_path_val_accuracy,\n",
    "                        monitor='val_accuracy',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode='max')\n",
    "\n",
    "    callbacks_list = [checkpoint_val_loss, checkpoint_val_accuracy]\n",
    "    model_history = classifier.fit(X_tr[:, np.array([complete_feature_dictionary[f] \n",
    "                                                     for f in training_features])],\n",
    "                                   y_tr,\n",
    "                                   sample_weight=X_tr[:, complete_feature_dictionary[\n",
    "                                       'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                                   epochs=EPOCHS,\n",
    "                                   verbose=2,\n",
    "                                   validation_data=(X_val[:, np.array([complete_feature_dictionary[f] \n",
    "                                                                       for f in training_features])],\n",
    "                                                    y_val,\n",
    "                                                    X_val[:, complete_feature_dictionary[\n",
    "                                                        'combinedWeight_and_trigger_efficiency_correction']]),\n",
    "                                   callbacks=callbacks_list,\n",
    "                                   shuffle=True)\n",
    "\n",
    "    model_json = classifier.to_json()\n",
    "    with open('model.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    classifier.save_weights('after_last_epoch.h5')\n",
    "    \n",
    "    del classifier\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(i)\n",
    "    plt.plot(model_history.history['accuracy'])\n",
    "    plt.plot(model_history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.savefig('Model_history/Model accuracy_' + str(i))\n",
    "    plt.close(i)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(i + K_FOLDING)\n",
    "    plt.plot(model_history.history['loss'])\n",
    "    plt.plot(model_history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.savefig('Model_history/Model loss_' + str(i))\n",
    "    plt.close(i + K_FOLDING)\n",
    "    \n",
    "\n",
    "# sometimes there are issues with the garbage collector \n",
    "del x_signal_normalized_reweighted\n",
    "del x_background_normalized_reweighted\n",
    "del tenfold_split_data\n",
    "del classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC value\n",
    "Now The AUC value with the standard deviation derived from the k-fold cross validation outcomes is calculated.\n",
    "    In this case, the AUC value is not calculated for a validation set, but for the whole data set. This is in fact not \"good practice\", but we later want to compare this result to the CWoLa trained classifier where only data is used t for the training process. Therefore one can use the whole data set to compare results, especially regarding unbalanced weights of the MC simulated QCD events, which might mess up the evaulation greatly if there aren't many other events to balance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([TTbar, QCD])\n",
    "y = np.concatenate([np.ones(len(TTbar)), np.zeros(len(QCD))])\n",
    "\n",
    "tprs = []\n",
    "tprs_btag = []\n",
    "aucs = []\n",
    "aucs_btag = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_fpr_btag = np.linspace(0, 1, 100)\n",
    "\n",
    "# load json and create model\n",
    "os.chdir(workspace_folder)\n",
    "os.chdir('MC_Model_' + date_string)\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into model\n",
    "for i in range(K_FOLDING):\n",
    "    loaded_model.load_weights('Run_' + str(i) + '_weights_best_val_loss.h5')\n",
    "    print('Loaded model ' + str(i) + ' from disk')\n",
    "\n",
    "    predictions = loaded_model.predict(X[:, np.array([complete_feature_dictionary[f] \n",
    "                                                      for f in training_features])])\n",
    "    fpr, tpr, thresholds = roc_curve(y,\n",
    "                                     predictions[:, 1],\n",
    "                                     sample_weight=X[:, complete_feature_dictionary[\n",
    "                                         'combinedWeight_and_trigger_efficiency_correction']])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc_val = auc(mean_fpr, mean_tpr)\n",
    "std_auc_val = np.std(aucs_btag)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'MC $t\\bar{t}$ vs. QCD  (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_val, std_auc_val),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "\n",
    "std_tpr_val = np.std(tprs, axis=0)\n",
    "tprs_upper_val = np.minimum(mean_tpr + std_tpr_val, 1)\n",
    "tprs_lower_val = np.maximum(mean_tpr - std_tpr_val, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower_val, tprs_upper_val, color='grey', alpha=.5,\n",
    "                 label='')\n",
    "\n",
    "\n",
    "plt.xlim([0, 1.0])\n",
    "plt.ylim([0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10-fold cross validation ROC-Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_Curve')\n",
    "\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2  MC Simulation $t\\bar{t}$ vs. QCD CWoLa\n",
    "\n",
    "The data is now separated according to the next figure. The estimated signal fractions are of 10.3% for 1 b-tag samples and 26.9% for 2 b-tag samples. For pairs of fractions $f_1$ and $f_2$ $\\in$ [0.0, 0.40] in steps of 0.25 a training is done and the AUC value is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/MC_CWoLa_separation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data for mixed samples using different fractions\n",
    "def creating_training_data(signal_data,\n",
    "                           background_data,\n",
    "                           f1,\n",
    "                           f2,\n",
    "                           number_of_events,\n",
    "                           lum=MC_lum):\n",
    "    \"\"\" Takes signal and background data and creates training, test and validation samples. The ratio between training\n",
    "    validation and signal samples is always 8:1:1.\n",
    "    :param signal_data data with signal events\n",
    "    :param background_data data with background events\n",
    "    :param f1 signal fraction of training signal sample, has to be greater than f2\n",
    "    :param f2 signal fraction of training background sample\n",
    "    :param number_of_events number of events per returned sample\n",
    "    :param lum luminosity\n",
    "    :return returns training data x,y with fractions\"\"\"\n",
    "\n",
    "    shuffle_signal = np.arange(len(signal_data))\n",
    "    shuffle_background = np.arange(len(background_data))\n",
    "\n",
    "    np.random.shuffle(shuffle_signal)\n",
    "    np.random.shuffle(shuffle_background)\n",
    "\n",
    "    signal_data = signal_data[shuffle_signal]\n",
    "    background_data = background_data[shuffle_background]\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "    signal_index = 0\n",
    "    background_index = 0\n",
    "\n",
    "    # Sample 1\n",
    "    sample_one_counter = 0\n",
    "\n",
    "    while sample_one_counter < f1 * number_of_events:\n",
    "        train_x.append(signal_data[signal_index, :])\n",
    "        train_y.append(1)\n",
    "        signal_index += 1\n",
    "        sample_one_counter += signal_data[signal_index, -1] * lum\n",
    "\n",
    "    while sample_one_counter < number_of_events:\n",
    "        train_x.append(background_data[background_index, :])\n",
    "        train_y.append(1)\n",
    "        background_index += 1\n",
    "        sample_one_counter += background_data[background_index, -1] * lum\n",
    "\n",
    "    # Sample 2\n",
    "    sample_two_counter = 0\n",
    "\n",
    "    while sample_two_counter < f2 * number_of_events:\n",
    "        train_x.append(signal_data[signal_index, :])\n",
    "        train_y.append(0)\n",
    "        signal_index += 1\n",
    "        sample_two_counter += signal_data[signal_index, -1] * lum\n",
    "\n",
    "    while sample_two_counter < number_of_events:\n",
    "        train_x.append(background_data[background_index, :])\n",
    "        train_y.append(0)\n",
    "        background_index += 1\n",
    "        sample_two_counter += background_data[background_index, -1] * lum\n",
    "\n",
    "    # shuffle again. just to make sure there will be no order\n",
    "    shuffle_array_training = np.arange(np.size(train_x, axis=0))\n",
    "    np.random.shuffle(shuffle_array_training)\n",
    "\n",
    "    shuffled_array_training_x = np.array(train_x)[shuffle_array_training]\n",
    "    shuffled_array_training_y = np.array(train_y)[shuffle_array_training]\n",
    "\n",
    "    # print('Weight sum of the training data = ', sum(np.array(shuffled_array_training_x)[:, -1]) * lum)\n",
    "\n",
    "    return shuffled_array_training_x, shuffled_array_training_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting AUC 2D calculation\n",
    "os.chdir(workspace_folder)\n",
    "\n",
    "if 'MC_2D_f1_f2_combinations' in os.listdir():\n",
    "    pass\n",
    "else:\n",
    "    new_folder = os.mkdir('MC_2D_f1_f2_combinations')\n",
    "os.chdir('MC_2D_f1_f2_combinations')\n",
    "\n",
    "x_signal_normalized_reweighted = \\\n",
    "    scaling_weights(MC_Simulation_2_btag,\n",
    "                    complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "x_background_normalized_reweighted = \\\n",
    "    scaling_weights(MC_Simulation_1_btag,\n",
    "                    complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "x_val = np.concatenate([scaling_weights(TTbar,\n",
    "                                        complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                                        number_normed_events=1e6), \n",
    "                        scaling_weights(QCD,\n",
    "                                        complete_feature_dictionary['combinedWeight_and_trigger_efficiency_correction'],\n",
    "                                        number_normed_events=1e6)])\n",
    "y_val = np.concatenate([np.ones(len(TTbar)), np.zeros(len(QCD))])\n",
    "\n",
    "\n",
    "N = 400000\n",
    "f = [0.4, 0.375, 0.35, 0.325, 0.3, 0.275, 0.25, 0.225, 0.2, 0.175, 0.15, 0.125, 0.1, 0.075, 0.05, 0.025, 0]\n",
    "\n",
    "\n",
    "auc_array = np.zeros((len(f), len(f)))\n",
    "for i, f1 in enumerate(f):\n",
    "    for k, f2 in enumerate(f):\n",
    "        if f2 > f1:\n",
    "            continue\n",
    "        else:\n",
    "            print('f1 = ' + str(f1) + ', f2 = ' + str(f2))\n",
    "\n",
    "            X_tr, y_tr = creating_training_data(x_signal_normalized_reweighted,\n",
    "                                                x_background_normalized_reweighted,\n",
    "                                                f1,\n",
    "                                                f2,\n",
    "                                                N,\n",
    "                                                MC_lum)\n",
    "\n",
    "            classifier = create_model(adam,\n",
    "                                      nodes_list=(16, 32, 64, 32, 16, 8),\n",
    "                                      batch_normalization=True,\n",
    "                                      activation_function='selu',\n",
    "                                      kernel_initializer='glorot_normal',\n",
    "                                      bias_initializer='glorot_normal',\n",
    "                                      batch_size=1024,\n",
    "                                      loss='sparse_categorical_crossentropy',\n",
    "                                      metrics='accuracy')\n",
    "\n",
    "            filepath_val_loss = 'Run_' + str(i) + '_' + str(k) + '_weights_best_val_loss.h5'\n",
    "            # filepath_val_accuracy = 'Run_' + str(i) + '_' + str(k) + '_weights_best_val_accuracy.h5'\n",
    "\n",
    "            checkpoint_val_loss = \\\n",
    "                ModelCheckpoint(filepath_val_loss, monitor='val_loss', verbose=1, save_weights_only=True,\n",
    "                                save_best_only=True, mode='min')\n",
    "\n",
    "            model_history = classifier.fit(X_tr[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])],\n",
    "                                           y_tr,\n",
    "                                           sample_weight=X_tr[:, complete_feature_dictionary[\n",
    "                                                                 'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                                           epochs=2,\n",
    "                                           verbose=2,\n",
    "                                           validation_data=(x_val[:, np.array([complete_feature_dictionary[f] \n",
    "                                                                              for f in training_features])],\n",
    "                                                            y_val,\n",
    "                                                            x_val[:, complete_feature_dictionary[\n",
    "                                                                'combinedWeight_and_trigger_efficiency_correction']]),\n",
    "                                           callbacks=[checkpoint_val_loss],\n",
    "                                           shuffle=True)\n",
    "\n",
    "            # ROC Curves\n",
    "            classifier.load_weights('Run_' + str(i) + '_' + str(k) + '_weights_best_val_loss.h5')\n",
    "            predictions = classifier.predict(x_val[:, np.array([complete_feature_dictionary[f] \n",
    "                                                                       for f in training_features])])\n",
    "            fpr, tpr, thresholds = roc_curve(y_val,\n",
    "                                             predictions[:, 1],\n",
    "                                             sample_weight=x_val[:, complete_feature_dictionary[\n",
    "                                                                    'combinedWeight_and_trigger_efficiency_correction']])\n",
    "\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            if roc_auc > 0.5:\n",
    "                auc_array[i, k] = roc_auc\n",
    "                auc_array[k, i] = roc_auc\n",
    "            else:\n",
    "                auc_array[i, k] = 1 - roc_auc\n",
    "                auc_array[k, i] = 1 - roc_auc\n",
    "\n",
    "            backend.clear_session()\n",
    "            del classifier\n",
    "\n",
    "print(auc_array)\n",
    "np.save('auc_array_N_' + str(N), np.array(auc_array))\n",
    "\n",
    "x, y = f, f\n",
    "\n",
    "label_1vs2_btag = 'AUC value\\n1 vs. 2 b-tag'\n",
    "xx, yy = np.meshgrid(f, f)\n",
    "h = plt.pcolormesh(x, y, auc_array)\n",
    "plt.plot(0.103, 0.269, 'xr', markersize=12, mew=4)\n",
    "plt.xlabel('signal fraction $f_1$', fontsize=13)\n",
    "plt.ylabel('signal fraction $f_2$', fontsize=13)\n",
    "plt.legend({label_1vs2_btag: 'rx'}, fontsize=13)\n",
    "plt.colorbar()\n",
    "plt.savefig('AUC_2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CWoLa on data\n",
    "\n",
    "## 5.1 Training and evaluation\n",
    "\n",
    "Now the time has come to use this approach on data and evaluate on MC simualted events and data simultaneously. \n",
    "The modified procedure is shown in teh following figure.\n",
    "\n",
    "![title](img/CMS_data_CWoLa_separation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for training\n",
    "x_signal_normed_reweighted = \\\n",
    "    scaling_weights(CMS_2_btag,\n",
    "                    complete_feature_dictionary['weight.combinedWeight'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "x_background_normed_reweighted = \\\n",
    "    scaling_weights(CMS_1_btag,\n",
    "                    complete_feature_dictionary['weight.combinedWeight'],\n",
    "                    number_normed_events=1e6)\n",
    "\n",
    "\n",
    "# define model output location over date string\n",
    "try:\n",
    "    os.mkdir('CMS_Model_' + date_string)\n",
    "except:\n",
    "    pass\n",
    "os.chdir('CMS_Model_' + date_string)\n",
    "os.mkdir('Model_history')\n",
    "\n",
    "\n",
    "tenfold_split_data = k_fold_cross_validation(K_FOLDING,\n",
    "                                             x_signal_normed_reweighted,\n",
    "                                             x_background_normed_reweighted,\n",
    "                                             complete_feature_dictionary['weight.combinedWeight'])\n",
    "\n",
    "\n",
    "for i in range(K_FOLDING):\n",
    "    print('Run k = ', i, '\\n')\n",
    "    classifier = create_model(adam,\n",
    "                              nodes_list=(16, 32, 64, 32, 16, 8),\n",
    "                              batch_normalization=True,\n",
    "                              activation_function='selu',\n",
    "                              kernel_initializer='glorot_normal',\n",
    "                              bias_initializer='glorot_normal',\n",
    "                              batch_size=1024,\n",
    "                              loss='sparse_categorical_crossentropy',\n",
    "                              metrics='accuracy')\n",
    "    X_tr = []\n",
    "    y_tr = []\n",
    "    for k_fold in range(K_FOLDING):\n",
    "        if k_fold != i:\n",
    "            for event, label in zip(tenfold_split_data[k_fold][0], tenfold_split_data[k_fold][1]):\n",
    "                X_tr.append(event)\n",
    "                y_tr.append(label)\n",
    "\n",
    "    X_tr = np.array(X_tr)\n",
    "    y_tr = np.array(y_tr)\n",
    "\n",
    "    X_val = np.array(tenfold_split_data[i][0])\n",
    "    y_val = np.array(tenfold_split_data[i][1])\n",
    "\n",
    "    file_path_val_loss = 'Run_' + str(i) + '_weights_best_val_loss.h5'\n",
    "    file_path_val_accuracy = 'Run_' + str(i) + '_weights_best_val_accuracy.h5'\n",
    "\n",
    "    checkpoint_val_loss = \\\n",
    "        ModelCheckpoint(file_path_val_loss,\n",
    "                        monitor='val_loss',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        save_weights_only=True,\n",
    "                        mode='min')\n",
    "\n",
    "    checkpoint_val_accuracy = \\\n",
    "        ModelCheckpoint(file_path_val_accuracy,\n",
    "                        monitor='val_accuracy',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        save_weights_only=True,\n",
    "                        mode='max')\n",
    "\n",
    "    callbacks_list = [checkpoint_val_loss, checkpoint_val_accuracy]\n",
    "    model_history = classifier.fit(X_tr[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])],\n",
    "                                   y_tr,\n",
    "                                   sample_weight=X_tr[:, complete_feature_dictionary['weight.combinedWeight']],\n",
    "                                   epochs=EPOCHS,\n",
    "                                   verbose=2,\n",
    "                                   validation_data=(X_val[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])],\n",
    "                                                    y_val,\n",
    "                                                    X_val[:, complete_feature_dictionary['weight.combinedWeight']]),\n",
    "                                   callbacks=callbacks_list,\n",
    "                                   shuffle=True)\n",
    "\n",
    "    model_json = classifier.to_json()\n",
    "    with open('model.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    classifier.save_weights('after_last_epoch.h5')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(i)\n",
    "    plt.plot(model_history.history['accuracy'])\n",
    "    plt.plot(model_history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.savefig('Model_history/Model accuracy_' + str(i))\n",
    "    plt.close(i)\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(i + K_FOLDING)\n",
    "    plt.plot(model_history.history['loss'])\n",
    "    plt.plot(model_history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.savefig('Model_history/Model loss_' + str(i))\n",
    "    plt.close(i + k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC value \n",
    "\n",
    "The AUC value and ROC curve can now be evaluated on 1/2 b-tag(s) CMS data and on $t\\bar{t}$ and QCD MC simulated events.\n",
    "If the AUC value on $t\\bar{t}$ and QCD MC simulated events is nearly as good as when trained only on them (see ch. 4.1) then a\n",
    "good classification is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_btag = np.concatenate([CMS_2_btag, CMS_1_btag])\n",
    "y_btag = np.concatenate([np.ones(len(CMS_2_btag)), np.zeros(len(CMS_1_btag))])\n",
    "\n",
    "X_true = np.concatenate([TTbar, QCD])\n",
    "y_true = np.concatenate([np.ones(len(TTbar)), np.zeros(len(QCD))])\n",
    "\n",
    "# define lists for tpr, fpr, auc\n",
    "tprs = []\n",
    "tprs_evaluate_on_MC = []\n",
    "aucs = []\n",
    "aucs_evaluate_on_MC = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_fpr_evaluate_on_MC = np.linspace(0, 1, 100)\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "for i in range(K_FOLDING):\n",
    "    loaded_model.load_weights('Run_' + str(i) + '_weights_best_val_loss.h5')\n",
    "    print('Loaded model ' + str(i) + ' from disk')\n",
    "    # loaded_model.load_weights('after_last_epoch.h5')\n",
    "\n",
    "    predictions = loaded_model.predict(X_btag[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])])\n",
    "    fpr, tpr, thresholds = roc_curve(y_btag,\n",
    "                                     predictions[:, 1],\n",
    "                                     sample_weight=X_btag[:, complete_feature_dictionary['weight.combinedWeight']])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "    predictions_evaluate_on_MC = loaded_model.predict(X_true[:, np.array([complete_feature_dictionary[f] \n",
    "                                                             for f in training_features])])\n",
    "    fpr_evaluate_on_MC, tpr_evaluate_on_MC, thresholds_evaluate_on_MC\\\n",
    "        = roc_curve(y_true,\n",
    "                    predictions_evaluate_on_MC[:, 1],\n",
    "                    sample_weight=MC_lum * X_true[:, complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']])\n",
    "    tprs_evaluate_on_MC.append(np.interp(mean_fpr_evaluate_on_MC, fpr_evaluate_on_MC, tpr_evaluate_on_MC))\n",
    "    tprs_evaluate_on_MC[-1][0] = 0.0\n",
    "    roc_auc_val = auc(fpr_evaluate_on_MC, tpr_evaluate_on_MC)\n",
    "    aucs_evaluate_on_MC.append(roc_auc_val)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "\n",
    "# Training data\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'CMS Val 1/2 b-tag (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.5,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "\n",
    "# Evaluation data\n",
    "mean_tpr_evaluate_on_MC = np.mean(tprs_evaluate_on_MC, axis=0)\n",
    "mean_tpr_evaluate_on_MC[-1] = 1.0\n",
    "mean_auc_val = auc(mean_fpr_evaluate_on_MC, mean_tpr_evaluate_on_MC)\n",
    "std_auc_val = np.std(aucs_evaluate_on_MC)\n",
    "plt.plot(mean_fpr_evaluate_on_MC, mean_tpr_evaluate_on_MC, color='g',\n",
    "         label=r'MC Test S/BG (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_val, std_auc_val),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "\n",
    "std_tpr_val = np.std(tprs_evaluate_on_MC, axis=0)\n",
    "tprs_upper_val = np.minimum(mean_tpr_evaluate_on_MC + std_tpr_val, 1)\n",
    "tprs_lower_val = np.maximum(mean_tpr_evaluate_on_MC - std_tpr_val, 0)\n",
    "plt.fill_between(mean_fpr_evaluate_on_MC, tprs_lower_val, tprs_upper_val, color='grey', alpha=.5,\n",
    "                 label='')\n",
    "\n",
    "\n",
    "plt.xlim([0, 1.0])\n",
    "plt.ylim([0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10-fold cross validation ROC-Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing input data\n",
    "def normalize_on_ht(data_set, dictionary, feature_list):\n",
    "    \"\"\"This function takes a data set, a dictionary describing the properties of the data and a feature list\n",
    "    and returns a normalization of values given in GeV in parts of HT.\n",
    "    :param data_set: flattened numpy array\n",
    "    :param dictionary: dictionary describing the properties of the values inside the data set\n",
    "    :param feature_list: list of features which are normalized with respect to ht value\n",
    "    :return returns the on ht normalized data set\"\"\"\n",
    "    data_set_return = np.copy(data_set)\n",
    "    for feature in feature_list:\n",
    "        if feature != dictionary['top.fitProb'] and feature != dictionary['jet.HT']:\n",
    "            data_set_return[:, dictionary[feature]] = data_set_return[:, dictionary[feature]] / data_set_return[:, dictionary['jet.HT']]\n",
    "    data_set_return[:, dictionary['jet.HT']] = data_set_return[:, dictionary['jet.HT']] / 5000\n",
    "    return np.array(data_set_return)\n",
    "\n",
    "\n",
    "def apply_model(input_data,\n",
    "                features,\n",
    "                model,\n",
    "                save_name):\n",
    "    \"\"\"This function takes  non-flattened input data, a list of features, a trained model and a save name and\n",
    "    returns a tuple: a scoring list with the same length as the input data and the data set itself with then only one\n",
    "    permutation per event remaining.\n",
    "    example: input data = = [event, event, event,... ] and  event = [permutation, permutation, ...]\n",
    "    output data = ([score_event_1, score_event_2, ...], [best_permutation_event_1, best_permutation_event_2, ...]\n",
    "    :param input_data: non-flattened numpy_array\n",
    "    :param features: list of features for the model\n",
    "    :param model: trained model,\n",
    "    :param save_name: name of file which the tuple will be saved\n",
    "    :return (list of scores, list of events)\n",
    "    \"\"\"\n",
    "    output_event_list = []\n",
    "    output_score_list = []\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    for num, event in enumerate(input_data):\n",
    "        normed_event = normalize_on_ht(event,\n",
    "                                       complete_feature_dictionary,\n",
    "                                       training_features)\n",
    "        permutation_number = -1\n",
    "        max_prediction = -1\n",
    "        predictions = model.predict(normed_event[:, features])\n",
    "        for k, perm in enumerate(normed_event):\n",
    "            # prediction = model.predict(np.expand_dims(perm[features], axis=0))[0][1]\n",
    "            # prediction = model.predict(perm[features])[1]\n",
    "            if predictions[k][1] > max_prediction:\n",
    "                max_prediction = predictions[k][1]\n",
    "                permutation_number = k\n",
    "        output_event_list.append(event[permutation_number])\n",
    "        output_score_list.append(max_prediction)\n",
    "        \n",
    "        if num % 100000 == 0:\n",
    "            one_thousand_elapsed = time.time() - start\n",
    "            print(str(num) + ' of ' + str(len(input_data)) + \n",
    "                  ' done of ' + \n",
    "                  save_name + \n",
    "                  ', elapsed time: ' \n",
    "                  + hms_string(one_thousand_elapsed))\n",
    "    np.save(save_name + '_predictions', np.array([output_score_list, output_event_list]))\n",
    "\n",
    "\n",
    "    \n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights('Run_' + str(randrange(K_FOLDING)) + '_weights_best_val_loss.h5')\n",
    "\n",
    "try: \n",
    "    os.mkdir('Applied Model outputs')\n",
    "except:\n",
    "    pass\n",
    "os.chdir('Applied Model outputs')\n",
    "\n",
    "# applying on data sets\n",
    "\n",
    "apply_model(TTbar_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'ttbar_172_5')\n",
    "apply_model(QCD_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'qcd')\n",
    "apply_model(CMS_data_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'cms_data')\n",
    "apply_model(CMS_background_estimation_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'cms_bkg_estimation')\n",
    "apply_model(QCD_background_estimation_non_flattened,\n",
    "            [complete_feature_dictionary[f] for f in training_features],\n",
    "            loaded_model,\n",
    "            'qcd_bkg_estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Output distributions\n",
    "\n",
    "Now the output distributions and the results in numbers will be displayed. \n",
    "In this part the output distributions of MC ttbar and qcd samples are shown and the selection efficiency \n",
    "in contrast to cms data is calculated <br>\n",
    "1) output distributions for ttbar correct + $t\\bar{t}$ wrong + qcd <br>\n",
    "2) output distributions for ttbar correct + $t\\bar{t}$ wrong <br>\n",
    "3) comparing 1 and 2 btag output distributions in various plots <br>\n",
    "4) selection efficiency numbers <br>\n",
    "5) W mass distribution and $m_t^{fit}$ distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(workspace_folder)\n",
    "# os.chdir('CMS_Model_' + date_string)\n",
    "os.chdir('E:\\Jupyter Notebooks Workspace\\CMS_Model_2020_11_07_15_42_10')\n",
    "\n",
    "# set paths for output distributions\n",
    "data_1725_output_distribution = np.load('Applied Model outputs/ttbar_172_5_predictions.npy', allow_pickle=True)\n",
    "qcd_output_distribution = np.load('Applied Model outputs/qcd_predictions.npy', allow_pickle=True)\n",
    "cms_output_distribution = np.load('Applied Model outputs/cms_data_predictions.npy', allow_pickle=True)\n",
    "\n",
    "os.mkdir('output_distributions')\n",
    "os.chdir('output_distributions')\n",
    "\n",
    "current_folder = os.getcwd()                                            # needed for later directory changing\n",
    "\n",
    "correct_permutations_ttbar_scores = []\n",
    "wrong_permutations_ttbar_scores = []\n",
    "correct_permutations_ttbar_events = []\n",
    "wrong_permutations_ttbar_events = []\n",
    "\n",
    "two_btag_ttbar_scores = []\n",
    "one_btag_ttbar_scores = []\n",
    "two_btag_ttbar_events = []\n",
    "one_btag_ttbar_events = []\n",
    "\n",
    "two_btag_qcd_scores = []\n",
    "one_btag_qcd_scores = []\n",
    "two_btag_qcd_events = []\n",
    "one_btag_qcd_events = []\n",
    "\n",
    "# dividing by number of btags\n",
    "for qcd_score, qcd_event in zip(qcd_output_distribution[0], qcd_output_distribution[1]):\n",
    "    if qcd_event[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "        two_btag_qcd_scores.append(qcd_score)\n",
    "        two_btag_qcd_events.append(qcd_event)\n",
    "    else:\n",
    "        one_btag_qcd_scores.append(qcd_score)\n",
    "        one_btag_qcd_events.append(qcd_event)\n",
    "\n",
    "for ttbar_score, ttbar_event in zip(data_1725_output_distribution[0], data_1725_output_distribution[1]):\n",
    "    if ttbar_event[complete_feature_dictionary['n_bjets']] >= 2:\n",
    "        two_btag_ttbar_scores.append(ttbar_score)\n",
    "        two_btag_ttbar_events.append(ttbar_event)\n",
    "    else:\n",
    "        one_btag_ttbar_scores.append(ttbar_score)\n",
    "        one_btag_ttbar_events.append(ttbar_event)\n",
    "\n",
    "# dividing by combination type\n",
    "for ttbar_score, ttbar_event in zip(data_1725_output_distribution[0], data_1725_output_distribution[1]):\n",
    "    if ttbar_event[complete_feature_dictionary['top.combinationType']] == 1:\n",
    "        correct_permutations_ttbar_scores.append(ttbar_score)\n",
    "        correct_permutations_ttbar_events.append(ttbar_event)\n",
    "    else:\n",
    "        wrong_permutations_ttbar_scores.append(ttbar_score)\n",
    "        wrong_permutations_ttbar_events.append(ttbar_event)\n",
    "\n",
    "# defining plot range\n",
    "plot_range = (0.2, 0.9)\n",
    "num_bins = 100\n",
    "lum = MC_lum\n",
    "\n",
    "# defining labels\n",
    "label_ttbar_correct = r'$t\\bar{t}$ correct'\n",
    "label_ttbar_wrong = r'$t\\bar{t}$ wrong'\n",
    "label_ttbar_one_btag = r'1 b-tag $t\\bar{t}$'\n",
    "label_ttbar_two_btag = r'2 b-tag $t\\bar{t}$'\n",
    "label_qcd = r'Multijet'\n",
    "\n",
    "# ttbar correct + wrong\n",
    "plt.figure()\n",
    "plt.hist([qcd_output_distribution[0],\n",
    "          correct_permutations_ttbar_scores,\n",
    "          wrong_permutations_ttbar_scores],\n",
    "         weights=[[qcd[complete_feature_dictionary[\n",
    "             'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for qcd in qcd_output_distribution[1]],\n",
    "                  [correct[complete_feature_dictionary[\n",
    "                      'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for correct in correct_permutations_ttbar_events],\n",
    "                  [wrong[complete_feature_dictionary[\n",
    "                      'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for wrong in wrong_permutations_ttbar_events]],\n",
    "         bins=num_bins,\n",
    "         stacked=True,\n",
    "         alpha=1,\n",
    "         range=plot_range,\n",
    "         edgecolor='k',\n",
    "         color=['grey', '#CC0000', '#FF6666'])\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'Events \\ 0.01', fontsize=13)\n",
    "plt.legend({label_qcd: 'grey', label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ttbar correct + wrong\n",
    "plt.figure()\n",
    "plt.hist([correct_permutations_ttbar_scores, wrong_permutations_ttbar_scores],\n",
    "         weights=[[correct[complete_feature_dictionary[\n",
    "             'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for correct in correct_permutations_ttbar_events],\n",
    "                  [wrong[complete_feature_dictionary[\n",
    "                      'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                   * lum for wrong in wrong_permutations_ttbar_events]],\n",
    "         bins=num_bins,\n",
    "         stacked=True,\n",
    "         alpha=1,\n",
    "         range=plot_range,\n",
    "         edgecolor='k',\n",
    "         color=['#CC0000', '#FF6666'])\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'Events \\ 0.01', fontsize=13)\n",
    "plt.legend({label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ttbar 1 + 2 btag\n",
    "plt.figure()\n",
    "plt.hist(np.array(one_btag_ttbar_scores),\n",
    "         weights=lum * np.array(one_btag_ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                              'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         density=True,\n",
    "         histtype='step',\n",
    "         range=plot_range,\n",
    "         linewidth=2,\n",
    "         edgecolor='seagreen',\n",
    "         color='seagreen')\n",
    "\n",
    "plt.hist(np.array(two_btag_ttbar_scores),\n",
    "         weights=lum * np.array(two_btag_ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                              'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         density=True,\n",
    "         histtype='step',\n",
    "         range=plot_range,\n",
    "         linewidth=2,\n",
    "         edgecolor='lightsteelblue',\n",
    "         color='lightsteelblue')\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'normalized', fontsize=13)\n",
    "plt.legend({r'1 b-tag $t\\bar{t}$': 'seagreen',\n",
    "            r'2 b-tag $t\\bar{t}$': 'lightsteelblue'}, fontsize=13)\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations_btag_ttbar')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# qcd 1 + 2 btag\n",
    "plt.figure()\n",
    "plt.hist(np.array(one_btag_qcd_scores),\n",
    "         weights=lum * np.array(one_btag_qcd_events)[:, complete_feature_dictionary[\n",
    "                                                            'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         histtype='step',\n",
    "         density=True,\n",
    "         range=plot_range,\n",
    "         edgecolor='darkgreen',\n",
    "         linewidth=2,\n",
    "         color='darkgreen')\n",
    "\n",
    "plt.hist(np.array(two_btag_qcd_scores),\n",
    "         weights=lum * np.array(two_btag_qcd_events)[:, complete_feature_dictionary[\n",
    "                                                            'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         histtype='step',\n",
    "         density=True,\n",
    "         range=plot_range,\n",
    "         edgecolor='cornflowerblue',\n",
    "         linewidth=2,\n",
    "         color='cornflowerblue')\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'normalized', fontsize=13)\n",
    "plt.legend({r'1 b-tag multijet': 'darkgreen',\n",
    "            r'2 b-tag multijet': 'cornflowerblue'}, fontsize=13, loc='best')\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations_btag_qcd')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ttbar + qcd 2 btag\n",
    "plt.figure()\n",
    "plt.hist(np.array(two_btag_qcd_scores),\n",
    "         weights=lum * np.array(two_btag_qcd_events)[:, complete_feature_dictionary[\n",
    "                                                            'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         histtype='step',\n",
    "         density=True,\n",
    "         range=plot_range,\n",
    "         edgecolor='cornflowerblue',\n",
    "         linewidth=2,\n",
    "         color='cornflowerblue')\n",
    "\n",
    "plt.hist(np.array(two_btag_ttbar_scores),\n",
    "         weights=lum * np.array(two_btag_ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                              'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         density=True,\n",
    "         histtype='step',\n",
    "         range=plot_range,\n",
    "         linewidth=2,\n",
    "         edgecolor='lightsteelblue',\n",
    "         color='lightsteelblue')\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'normalized', fontsize=13)\n",
    "plt.legend({r'2 b-tag multijet': 'cornflowerblue',\n",
    "            r'2 b-tag $t\\bar{t}$': 'lightsteelblue'}, fontsize=13)\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations_2_btag_qcd_vs_ttbar')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ttbar + qcd 1 btag\n",
    "plt.figure()\n",
    "plt.hist(np.array(one_btag_qcd_scores),\n",
    "         weights=lum * np.array(one_btag_qcd_events)[:, complete_feature_dictionary[\n",
    "                                                            'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         histtype='step',\n",
    "         density=True,\n",
    "         range=plot_range,\n",
    "         edgecolor='darkgreen',\n",
    "         linewidth=2,\n",
    "         color='darkgreen')\n",
    "\n",
    "plt.hist(np.array(one_btag_ttbar_scores),\n",
    "         weights=lum * np.array(one_btag_ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                              'combinedWeight_and_trigger_efficiency_correction']],\n",
    "         bins=num_bins,\n",
    "         alpha=1,\n",
    "         density=True,\n",
    "         histtype='step',\n",
    "         range=plot_range,\n",
    "         linewidth=2,\n",
    "         edgecolor='seagreen',\n",
    "         color='seagreen')\n",
    "\n",
    "\n",
    "plt.xlabel('NN output', fontsize=13)\n",
    "plt.ylabel(r'normalized', fontsize=13)\n",
    "plt.legend({r'1 b-tag multijet': 'darkgreen',\n",
    "            r'1 b-tag $t\\bar{t}$': 'seagreen'}, fontsize=13, loc='best')\n",
    "plt.title(r'simulation $\\cdot$ private work'\n",
    "          '                  '\n",
    "          r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "plt.savefig('output_distribution_permutations_1_btag_qcd_vs_ttbar')\n",
    "plt.close()\n",
    "\n",
    "# calculating selection efficiencies\n",
    "suggested_threshold_values = output_threshold_values = [round(num, 3) for num in [0.6 + 0.01 * i for i in range(15)]]\n",
    "remaining_cms_data = np.zeros(len(suggested_threshold_values))\n",
    "remaining_ttbar_data = np.zeros(len(suggested_threshold_values))\n",
    "\n",
    "for position, threshold in enumerate(suggested_threshold_values):\n",
    "    m_t_fit_list_correct = []\n",
    "    m_W_reco_list_correct = []\n",
    "    event_weights_correct = []\n",
    "\n",
    "    m_t_fit_list_wrong = []\n",
    "    m_W_reco_list_wrong = []\n",
    "    event_weights_wrong = []\n",
    "\n",
    "    for ttbar_score, ttbar_event in zip(data_1725_output_distribution[0], data_1725_output_distribution[1]):\n",
    "        if ttbar_score >= threshold and ttbar_event[complete_feature_dictionary['n_bjets']] >= 1:\n",
    "            remaining_ttbar_data[position] += lum * ttbar_event[complete_feature_dictionary[\n",
    "                'combinedWeight_and_trigger_efficiency_correction']]\n",
    "            if ttbar_event[complete_feature_dictionary['top.combinationType']] == 1:\n",
    "                m_t_fit_list_correct.append(ttbar_event[complete_feature_dictionary['top.fitTop1.M']])\n",
    "                event_weights_correct.append(lum * ttbar_event[complete_feature_dictionary[\n",
    "                    'combinedWeight_and_trigger_efficiency_correction']])\n",
    "                m_W_reco_list_correct.append(\n",
    "                    0.5 * ttbar_event[complete_feature_dictionary['top.recoW1.M']] +\n",
    "                    0.5 * ttbar_event[complete_feature_dictionary['top.recoW1.M']])\n",
    "            else:\n",
    "                m_t_fit_list_wrong.append(ttbar_event[complete_feature_dictionary['top.fitTop1.M']])\n",
    "                event_weights_wrong.append(lum * ttbar_event[complete_feature_dictionary[\n",
    "                    'combinedWeight_and_trigger_efficiency_correction']])\n",
    "                m_W_reco_list_wrong.append(0.5 * ttbar_event[\n",
    "                    complete_feature_dictionary['top.recoW1.M']] +\n",
    "                                            0.5 * ttbar_event[\n",
    "                    complete_feature_dictionary['top.recoW2.M']])\n",
    "\n",
    "    for cms_score, cms_event in zip(cms_output_distribution[0], cms_output_distribution[1]):\n",
    "        if cms_score >= threshold and cms_event[complete_feature_dictionary['n_bjets']] >= 1:\n",
    "            remaining_cms_data[position] += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist([m_t_fit_list_correct, m_t_fit_list_wrong],\n",
    "             weights=[event_weights_correct, event_weights_wrong],\n",
    "             bins=75,\n",
    "             stacked=True,\n",
    "             alpha=1,\n",
    "             range=(100, 250),\n",
    "             edgecolor='k',\n",
    "             color=['#CC0000', '#FF6666'])\n",
    "\n",
    "    plt.xlabel('$m_t^{fit}$ [GeV]', fontsize=13)\n",
    "    plt.ylabel(r'Events \\ 2 GeV', fontsize=13)\n",
    "    plt.legend({label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "    plt.title(r'simulation $\\cdot$ private work'\n",
    "              '                  '\n",
    "              r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "    plt.savefig('m_t_fit_' + str(threshold).replace('.', '_'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist([m_W_reco_list_correct, m_W_reco_list_wrong],\n",
    "             weights=[event_weights_correct, event_weights_wrong],\n",
    "             bins=60,\n",
    "             stacked=True,\n",
    "             alpha=1,\n",
    "             range=(40, 160),\n",
    "             edgecolor='k',\n",
    "             color=['#CC0000', '#FF6666'])\n",
    "\n",
    "    plt.xlabel('$m_W^{reco}$ [GeV]', fontsize=13)\n",
    "    plt.ylabel(r'Events \\ 2 GeV', fontsize=13)\n",
    "    plt.legend({label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "    plt.title(r'simulation $\\cdot$ private work'\n",
    "              '                  '\n",
    "              r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "    plt.savefig('m_W_reco' + str(threshold).replace('.', '_'))\n",
    "    plt.close()\n",
    "\n",
    "for threshold, r_ttbar, r_cms in zip(suggested_threshold_values, remaining_ttbar_data, remaining_cms_data):\n",
    "    print('Threshold value: ',\n",
    "          threshold,\n",
    "          'remaining ttbar: ', np.around(r_ttbar, decimals=1),\n",
    "          'remaining cms: ', np.around(r_cms, decimals=1),\n",
    "          ': selection efficiency: ', np.around(100 * r_ttbar/r_cms, decimals=1), r'%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Background estimation and feature distributions correct/wrong and 1/2 btag(s)\n",
    "\n",
    "1) A background closure test between direct simulation and prediction from MC simulated samples and compares this\n",
    "   to prediction from cms data <br>\n",
    "2) Compare cms data and ttbar MC imulated samples. The difference between these data is \n",
    "   calculated and the background estimation from CMS data is scaled with a constant factor. <br>\n",
    "   --> CMS  = ttbar + scale_factor * background_estimation_CMS<br>\n",
    "3) The output distributions of ttbar data, divided into<br>\n",
    "   a) correct/wrong permutations <br>\n",
    "   b) 1/2 btag permutations <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(workspace_folder)\n",
    "os.chdir('E:\\Jupyter Notebooks Workspace\\CMS_Model_2020_11_07_15_42_10')\n",
    "suggested_threshold_values = output_threshold_values = [round(num, 3) for num in [0.6 + 0.01 * i for i in range(15)]]\n",
    "\n",
    "# load data & predictions\n",
    "qcd_zero = np.load('Applied Model outputs/qcd_bkg_estimation_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "qcd = np.load('Applied Model outputs/qcd_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "cms_zero = np.load('Applied Model outputs/cms_bkg_estimation_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "cms = np.load('Applied Model outputs/cms_data_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "ttbar = np.load('Applied Model outputs/ttbar_172_5_predictions.npy', allow_pickle=True, encoding='latin1')\n",
    "\n",
    "# getting dictionries from cwola files\n",
    "range_bins_list = range_dict_features_of_interest \n",
    "variable_list = features_of_interest\n",
    "x_label_list = x_label_dictionary\n",
    "\n",
    "# creating parent folder for file\n",
    "try:\n",
    "    os.mkdir('background_prediction_and_output_distributions')\n",
    "except:\n",
    "    pass\n",
    "os.chdir('background_prediction_and_output_distributions')\n",
    "background_prediction_and_output_distributions_folder = os.getcwd()\n",
    "\n",
    "# creating closure folder\n",
    "try:\n",
    "    os.mkdir('closure_tests')\n",
    "except:\n",
    "    pass\n",
    "os.chdir('closure_tests')\n",
    "closure_tests_folder = os.getcwd()\n",
    "\n",
    "# looping over threshold values and fill lists with remaining events\n",
    "for threshold_value in output_threshold_values:\n",
    "    qcd_zero_events = []\n",
    "    qcd_events = []\n",
    "    cms_zero_events = []\n",
    "    cms_events = []\n",
    "    ttbar_events = []\n",
    "\n",
    "    # creating folder with threshold value in name\n",
    "    try:\n",
    "        os.mkdir(str(threshold_value).replace('.', '_'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(str(threshold_value).replace('.', '_'))\n",
    "\n",
    "    for qcd_zero_score, qcd_zero_e in zip(qcd_zero[0], qcd_zero[1]):\n",
    "        if qcd_zero_score >= threshold_value:\n",
    "            qcd_zero_events.append(qcd_zero_e)\n",
    "\n",
    "    for qcd_score, qcd_e in zip(qcd[0], qcd[1]):\n",
    "        if qcd_score >= threshold_value:\n",
    "            qcd_events.append(qcd_e)\n",
    "\n",
    "    for cms_zero_score, cms_zero_e in zip(cms_zero[0], cms_zero[1]):\n",
    "        if cms_zero_score >= threshold_value:\n",
    "            cms_zero_events.append(cms_zero_e)\n",
    "\n",
    "    for cms_score, cms_e in zip(cms[0], cms[1]):\n",
    "        if cms_score >= threshold_value:\n",
    "            cms_events.append(cms_e)\n",
    "\n",
    "    for ttbar_score, ttbar_e in zip(ttbar[0], ttbar[1]):\n",
    "        if ttbar_score >= threshold_value:\n",
    "            ttbar_events.append(ttbar_e)\n",
    "\n",
    "    # looping over each variable of interest\n",
    "    for variable, x_label in zip(variable_list, x_label_list):\n",
    "        bins = 40\n",
    "        range_bins = range_bins_list[variable]\n",
    "        sum_events_zero_btags = 0\n",
    "        sum_events_1_btags = 0\n",
    "        sum_events_cms_qcd_estimation = 0\n",
    "\n",
    "        for event in qcd_events:\n",
    "            if event[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                pass\n",
    "            if event[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                pass\n",
    "            else:\n",
    "                sum_events_1_btags += event[complete_feature_dictionary[\n",
    "                    'combinedWeight_and_trigger_efficiency_correction']]\n",
    "\n",
    "        for event in qcd_zero_events:\n",
    "            if event[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                pass\n",
    "            if event[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                pass\n",
    "            else:\n",
    "                sum_events_zero_btags += event[complete_feature_dictionary[\n",
    "                    'combinedWeight_and_trigger_efficiency_correction']]\n",
    "\n",
    "        for event in cms_zero_events:\n",
    "            if event[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                pass\n",
    "            if event[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                pass\n",
    "            else:\n",
    "                sum_events_cms_qcd_estimation += event[complete_feature_dictionary[\n",
    "                    'weight.combinedWeight']]\n",
    "\n",
    "        if x_label == 'top.recoW1.M':                                 # calculating mean value of both W bosons\n",
    "            y_label = (range_bins[1] - range_bins[0]) / bins\n",
    "            bin_shift_for_plotting = y_label / 2\n",
    "\n",
    "            # getting weights for error bars qcd 2 btags\n",
    "            error_bars_marker = np.zeros(bins)\n",
    "            for entry in qcd_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW1.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_1_btags) ** 2\n",
    "\n",
    "            for entry in qcd_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW2.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_1_btags) ** 2\n",
    "\n",
    "            # getting weights for error bars qcd zero btags\n",
    "            error_bars_marker2 = np.zeros(bins)\n",
    "            for entry in qcd_zero_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW1.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker2[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_zero_btags) ** 2\n",
    "\n",
    "            for entry in qcd_zero_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW2.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker2[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_zero_btags) ** 2\n",
    "\n",
    "            # error bars marker cms_background estimation\n",
    "            error_bars_marker3 = np.zeros(bins)\n",
    "            for entry in cms_zero_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW1.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW1.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker3[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'weight.combinedWeight']] / sum_events_cms_qcd_estimation) ** 2\n",
    "\n",
    "            for entry in cms_zero_events:\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary['top.recoW2.M']] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary['top.recoW2.M']] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker3[bin_number] += (0.5 * entry[complete_feature_dictionary[\n",
    "                        'weight.combinedWeight']] / sum_events_cms_qcd_estimation) ** 2\n",
    "\n",
    "            # norming histograms to let them sum up to 1\n",
    "            qcd_allcut_1_btags_copy = np.copy(qcd_events)\n",
    "            qcd_allcut_zero_btags_copy = np.copy(qcd_zero_events)\n",
    "            cms_qcd_est_copy = np.copy(cms_zero_events)\n",
    "\n",
    "            qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] = \\\n",
    "                qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']]))\n",
    "\n",
    "            qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] = \\\n",
    "                qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']]))\n",
    "\n",
    "            cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'weight.combinedWeight']] = \\\n",
    "                cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'weight.combinedWeight']]))\n",
    "\n",
    "            n_qcd_1_btags, bins_qcd_1_btags, patches_qcd_1_btags = \\\n",
    "                plt.hist([0.5 * qcd_allcut_1_btags_copy[:, complete_feature_dictionary['top.recoW1.M']] +\n",
    "                          0.5 * qcd_allcut_1_btags_copy[:, complete_feature_dictionary['top.recoW2.M']]],\n",
    "                         weights=qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                                                'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            n_cms_qcd_est, bins_cms_qcd_est, patches_cms_qcd_est = \\\n",
    "                plt.hist([0.5 * cms_qcd_est_copy[:, complete_feature_dictionary['top.recoW1.M']] +\n",
    "                          0.5 * cms_qcd_est_copy[:, complete_feature_dictionary['top.recoW2.M']]],\n",
    "                         weights=cms_qcd_est_copy[:, complete_feature_dictionary['weight.combinedWeight']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            n_qcd_zero_btags, bins_qcd_zero, patches_qcd_zero_btags = \\\n",
    "                plt.hist(np.array(qcd_zero_events)[:, complete_feature_dictionary[variable]],\n",
    "                         weights=np.array(qcd_zero_events)[:, complete_feature_dictionary[\n",
    "                                                                  'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         density=True,\n",
    "                         color='grey')\n",
    "\n",
    "            plt.errorbar((bins_cms_qcd_est + bin_shift_for_plotting)[0:-1],\n",
    "                         n_cms_qcd_est,\n",
    "                         yerr=np.sqrt(error_bars_marker3) / sum_events_cms_qcd_estimation,\n",
    "                         color='b',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_qcd_1_btags + bin_shift_for_plotting)[0:-1],\n",
    "                         n_qcd_1_btags,\n",
    "                         yerr=np.sqrt(error_bars_marker),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_qcd_1_btags + bin_shift_for_plotting)[0:-1],\n",
    "                         n_qcd_zero_btags,\n",
    "                         yerr=np.sqrt(error_bars_marker2),\n",
    "                         color='grey',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.plot([], [], 'grey', label='prediction')\n",
    "            plt.plot([], [], 'k', label='direct simulation')\n",
    "            plt.plot([], [], 'blue', label='prediction (data)')\n",
    "            plt.ylim(bottom=0)\n",
    "\n",
    "            plt.legend(loc='best', fontsize=13)\n",
    "            plt.title(r'private work'\n",
    "                      '                                        '\n",
    "                      r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "\n",
    "            plt.ylabel(r'normalized', fontsize=13)\n",
    "            plt.xlabel(x_label_list[x_label], fontsize=13)\n",
    "\n",
    "            plt.savefig(\n",
    "                'QCD_estimation_closure_' + str(variable).replace('.', '_') + '_' +\n",
    "                str(threshold_value).replace('.', '_'))\n",
    "            plt.close()\n",
    "        elif x_label == 'top.recoW2.M':\n",
    "            continue\n",
    "        else:\n",
    "            y_label = (range_bins[1] - range_bins[0]) / bins\n",
    "            bin_shift_for_plotting = y_label / 2\n",
    "\n",
    "            # getting weights for error bars qcd 2 btags\n",
    "            error_bars_marker = np.zeros(bins)\n",
    "            for entry in qcd_events:\n",
    "                if entry[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary[variable]] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker[bin_number] += (entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_1_btags) ** 2\n",
    "\n",
    "            # getting weights for error bars qcd zero btags\n",
    "            error_bars_marker2 = np.zeros(bins)\n",
    "            for entry in qcd_zero_events:\n",
    "                if entry[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary[variable]] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker2[bin_number] += (entry[complete_feature_dictionary[\n",
    "                        'combinedWeight_and_trigger_efficiency_correction']] / sum_events_zero_btags) ** 2\n",
    "\n",
    "            error_bars_marker3 = np.zeros(bins)\n",
    "            for entry in cms_zero_events:\n",
    "                if entry[complete_feature_dictionary[variable]] < range_bins[0]:\n",
    "                    pass\n",
    "                if entry[complete_feature_dictionary[variable]] > range_bins[1]:\n",
    "                    pass\n",
    "                else:\n",
    "                    res = entry[complete_feature_dictionary[variable]] - range_bins[0]\n",
    "                    bin_number = int(res / y_label)\n",
    "                    error_bars_marker3[bin_number] += (entry[complete_feature_dictionary[\n",
    "                        'weight.combinedWeight']] / sum_events_cms_qcd_estimation) ** 2\n",
    "\n",
    "            # norming histograms to let them sum up to 1\n",
    "            qcd_allcut_1_btags_copy = np.copy(qcd_events)\n",
    "            qcd_allcut_zero_btags_copy = np.copy(qcd_zero_events)\n",
    "            cms_qcd_est_copy = np.copy(cms_zero_events)\n",
    "\n",
    "            qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] = \\\n",
    "                qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']]))\n",
    "\n",
    "            qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] = \\\n",
    "                qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']]))\n",
    "\n",
    "            cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'weight.combinedWeight']] = \\\n",
    "                cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] \\\n",
    "                / (sum(cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                           'weight.combinedWeight']]))\n",
    "\n",
    "            n_qcd_1_btags, bins_qcd_1_btags, patches_qcd_1_btags = \\\n",
    "                plt.hist(qcd_allcut_1_btags_copy[:, complete_feature_dictionary[variable]],\n",
    "                         weights=qcd_allcut_1_btags_copy[:, complete_feature_dictionary[\n",
    "                                                                'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            n_cms_qcd_est, bins_cms_qcd_est, patches_cms_qcd_est = \\\n",
    "                plt.hist(cms_qcd_est_copy[:, complete_feature_dictionary[variable]],\n",
    "                         weights=cms_qcd_est_copy[:, complete_feature_dictionary[\n",
    "                                                         'weight.combinedWeight']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            n_qcd_zero_btags, bins_qcd_zero_btags, patches_qcd_zero_btags = \\\n",
    "                plt.hist(qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[variable]],\n",
    "                         weights=qcd_allcut_zero_btags_copy[:, complete_feature_dictionary[\n",
    "                                                                   'combinedWeight_and_trigger_efficiency_correction']],\n",
    "                         range=range_bins,\n",
    "                         bins=bins,\n",
    "                         color='grey')\n",
    "\n",
    "            plt.errorbar((bins_cms_qcd_est + bin_shift_for_plotting)[0:-1],\n",
    "                         n_cms_qcd_est,\n",
    "                         yerr=np.sqrt(error_bars_marker3) / sum_events_cms_qcd_estimation,\n",
    "                         color='b',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_qcd_1_btags + bin_shift_for_plotting)[0:-1],\n",
    "                         n_qcd_zero_btags,\n",
    "                         yerr=np.sqrt(error_bars_marker2),\n",
    "                         color='grey',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_qcd_1_btags + bin_shift_for_plotting)[0:-1],\n",
    "                         n_qcd_1_btags,\n",
    "                         yerr=np.sqrt(error_bars_marker),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=3,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.plot([], [], 'grey', label='prediction')\n",
    "            plt.plot([], [], 'k', label='direct simulation')\n",
    "            plt.plot([], [], 'blue', label='prediction (data)')\n",
    "            plt.ylim(bottom=0)\n",
    "\n",
    "            plt.legend(loc='best', fontsize=13)\n",
    "            plt.title(r'private work'\n",
    "                      '                                        '\n",
    "                      r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "\n",
    "            plt.ylabel(r'normalized', fontsize=13)\n",
    "            plt.xlabel(x_label_list[x_label], fontsize=13)\n",
    "\n",
    "            plt.savefig(\n",
    "                'QCD_estimation_closure_' + str(variable).replace('.', '_') + '_' +\n",
    "                str(threshold_value).replace('.', '_'))\n",
    "            plt.close()\n",
    "\n",
    "    # create folder for background estiamtion\n",
    "    os.chdir(background_prediction_and_output_distributions_folder)\n",
    "    try:\n",
    "        os.mkdir('background_estimation')\n",
    "    except: pass\n",
    "    os.chdir('background_estimation')\n",
    "    try: \n",
    "        os.mkdir(str(threshold_value).replace('.', '_'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(str(threshold_value).replace('.', '_'))\n",
    "\n",
    "    for variable, x_label in zip(variable_list, x_label_list):\n",
    "        bins = 80\n",
    "        range_bins = range_bins_list[variable]\n",
    "        y_label = np.around(((range_bins[1] - range_bins[0]) / bins), decimals=2)\n",
    "        bin_shift_for_plotting = y_label / 2\n",
    "\n",
    "        if '[GeV]' in x_label:\n",
    "            y_label_string = r'events / ' + str(y_label) + ' GeV'\n",
    "        else:\n",
    "            y_label_string = r'events / ' + str(y_label)\n",
    "\n",
    "        n_cms, bins_cms, patches_cms = plt.hist(np.array(cms_events)[\n",
    "                                                :, complete_feature_dictionary[variable]],\n",
    "                                                weights=np.array(cms_events)[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                                   'weight.combinedWeight']],\n",
    "                                                range=range_bins,\n",
    "                                                bins=bins,\n",
    "                                                color='k')\n",
    "        n_ttbar, bins_ttbar, patches_ttbar = plt.hist(np.array(ttbar_events)[\n",
    "                                                      :, complete_feature_dictionary[variable]],\n",
    "                                                      weights=np.array(ttbar_events)[\n",
    "                                                              :, complete_feature_dictionary[\n",
    "                                                                     'combinedWeight_and_trigger_efficiency_correction']\n",
    "                                                              ] * MC_lum,\n",
    "                                                      range=range_bins,\n",
    "                                                      bins=bins,\n",
    "                                                      color='r')\n",
    "\n",
    "        n_cms_qcd_est, bins_cms_qcd_est, patches_cms_qcd_est = \\\n",
    "            plt.hist(np.array(cms_zero_events)[:, complete_feature_dictionary[variable]],\n",
    "                     weights=np.array(cms_zero_events)[:, complete_feature_dictionary[\n",
    "                                                              'weight.combinedWeight']],\n",
    "                     range=range_bins,\n",
    "                     bins=bins,\n",
    "                     color='grey')\n",
    "\n",
    "        n_res = n_cms - n_ttbar\n",
    "        scaling_factor = (sum(np.array(cms_events)[:, complete_feature_dictionary[\n",
    "                                                          'weight.combinedWeight']])\n",
    "                          - (sum(np.array(ttbar_events)[:, complete_feature_dictionary[\n",
    "                                                             'combinedWeight_and_trigger_efficiency_correction']]) *\n",
    "                             MC_lum)) / sum(np.array(cms_zero_events)[:,\n",
    "                                                          complete_feature_dictionary[\n",
    "                                                              'weight.combinedWeight']])\n",
    "\n",
    "        plt.close()\n",
    "        fig1 = plt.figure(1)\n",
    "\n",
    "        frame1 = fig1.add_axes((.15, .3, .8, .6))\n",
    "\n",
    "        frame1.set_title('Remaining events after subtracting' + r'$t\\bar{t}$' + ' from CMS data')\n",
    "        if variable == 'top.fitTop1.M + ANSTATT AUSKOMMENTIERT':\n",
    "            # blind 140 - 210 GeV for mass\n",
    "            plt.errorbar((bins_cms + bin_shift_for_plotting)[0:8],\n",
    "                         n_cms[0:8],\n",
    "                         yerr=np.sqrt(n_cms[0:8]),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=1.75,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "            plt.errorbar((bins_cms + bin_shift_for_plotting)[22:-1],\n",
    "                         n_cms[22:],\n",
    "                         yerr=np.sqrt(n_cms[22:]),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=1.75,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "        else:\n",
    "            plt.errorbar((bins_cms + bin_shift_for_plotting)[0:-1],\n",
    "                         n_cms,\n",
    "                         yerr=np.sqrt(n_cms),\n",
    "                         color='k',\n",
    "                         marker='o',\n",
    "                         markersize=1.75,\n",
    "                         markeredgewidth=1.75,\n",
    "                         linestyle='')\n",
    "\n",
    "        n_combined, bins_combined, patches_combined = \\\n",
    "            plt.hist([np.array(cms_zero_events)[:, complete_feature_dictionary[variable]],\n",
    "                      np.array(ttbar_events)[:, complete_feature_dictionary[variable]]],\n",
    "                     weights=[np.array(cms_zero_events)[:, complete_feature_dictionary[\n",
    "                                                               'weight.combinedWeight']] * scaling_factor,\n",
    "                              np.array(ttbar_events)[:, complete_feature_dictionary[\n",
    "                                           'combinedWeight_and_trigger_efficiency_correction']] * MC_lum],\n",
    "                     stacked=True,\n",
    "                     range=range_bins,\n",
    "                     bins=bins,\n",
    "                     color=['grey', 'r'],\n",
    "                     edgecolor='k')\n",
    "\n",
    "        n_combined_total = n_combined[1]\n",
    "        factor_between_cms_background_prediction_plus_ttbar_mc_and_cms_data = n_cms / n_combined_total\n",
    "\n",
    "        frame1.plot([], [], 'k.', label='CMS data:' + '\\nN = ' + str(int(sum(n_cms))) + ' events')\n",
    "        frame1.plot([], [], 'r', label=r'$t\\bar{t}$ ' + 'MC:\\n' + 'N = ' + str(int(np.around(sum(n_ttbar), decimals=1)))\n",
    "                                       + ' events')\n",
    "        frame1.plot([], [], 'grey', label='scaled QCD Estimation \\nfrom 0 btag CMS data')\n",
    "\n",
    "        frame1.legend(loc='best')\n",
    "\n",
    "        plt.title(r'CMS data and MC $\\cdot$ private work'\n",
    "                  '                                                 '\n",
    "                  r'35.9 $fb^{-1}$ (13 TeV)', fontsize=9)\n",
    "\n",
    "        frame1.set_ylabel(y_label_string)\n",
    "        frame2 = fig1.add_axes((.15, .075, .8, .125))\n",
    "\n",
    "        # blind 140-210 GeV remove COMMENTED for blinding\n",
    "\n",
    "        frame2.errorbar(bins_cms[0:-1] + bin_shift_for_plotting,\n",
    "                        factor_between_cms_background_prediction_plus_ttbar_mc_and_cms_data,\n",
    "                        yerr=np.sqrt(n_cms) / n_combined_total,\n",
    "                        color='k',\n",
    "                        marker='o',\n",
    "                        markersize=1.75,\n",
    "                        markeredgewidth=1.75,\n",
    "                        linestyle='')\n",
    "\n",
    "        frame2.set_ylim([0, 2])\n",
    "        frame2.set_xlim([range_bins[0], range_bins[1]])\n",
    "        frame2.set_xticklabels([])  # Remove x-tic labels from the first frame\n",
    "        frame2.set_yticks([0, 1, 2])\n",
    "        frame2.set_ylabel(r'data / MC')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.savefig('QCD_background_prediction_' +\n",
    "                    str(variable).replace('.', '_') + '_' + str(threshold_value).replace('.', '_'))\n",
    "        plt.close()\n",
    "\n",
    "    # create folder for output distributions\n",
    "    os.chdir(background_prediction_and_output_distributions_folder)\n",
    "    try:\n",
    "        os.mkdir('output_distributions')\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir('output_distributions')\n",
    "    try:\n",
    "        os.mkdir(str(threshold_value).replace('.', '_'))\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(str(threshold_value).replace('.', '_'))\n",
    "\n",
    "    correct_permutations_ttbar = []\n",
    "    wrong_permutations_ttbar = []\n",
    "\n",
    "    one_btag_ttbar = []\n",
    "    two_btag_ttbar = []\n",
    "\n",
    "    one_btag_correct_permutation_ttbar = []\n",
    "    two_btag_correct_permutation_ttbar = []\n",
    "\n",
    "    one_btag_wrong_permutation_ttbar = []\n",
    "    two_btag_wrong_permutation_ttbar = []\n",
    "\n",
    "    for ttbar_event in ttbar_events:\n",
    "        if ttbar_event[complete_feature_dictionary['top.combinationType']] == 1:\n",
    "            correct_permutations_ttbar.append(ttbar_event)\n",
    "            if ttbar_event[complete_feature_dictionary['n_bjets']] == 2:\n",
    "                two_btag_correct_permutation_ttbar.append(ttbar_event)\n",
    "            else:\n",
    "                one_btag_correct_permutation_ttbar.append(ttbar_event)\n",
    "        else:\n",
    "            wrong_permutations_ttbar.append(ttbar_event)\n",
    "            if ttbar_event[complete_feature_dictionary['n_bjets']] == 2:\n",
    "                two_btag_wrong_permutation_ttbar.append(ttbar_event)\n",
    "            else:\n",
    "                one_btag_wrong_permutation_ttbar.append(ttbar_event)\n",
    "\n",
    "    correct_permutations_ttbar = np.array(correct_permutations_ttbar)\n",
    "    wrong_permutations_ttbar = np.array(wrong_permutations_ttbar)\n",
    "\n",
    "    one_btag_ttbar = np.array(one_btag_ttbar)\n",
    "    two_btag_ttbar = np.array(two_btag_ttbar)\n",
    "\n",
    "    one_btag_correct_permutation_ttbar = np.array(one_btag_correct_permutation_ttbar)\n",
    "    two_btag_correct_permutation_ttbar = np.array(two_btag_correct_permutation_ttbar)\n",
    "\n",
    "    one_btag_wrong_permutation_ttbar = np.array(one_btag_wrong_permutation_ttbar)\n",
    "    two_btag_wrong_permutation_ttbar = np.array(two_btag_wrong_permutation_ttbar)\n",
    "\n",
    "    label_ttbar_correct = r'$t\\bar{t}$ correct'\n",
    "    label_ttbar_wrong = r'$t\\bar{t}$ wrong'\n",
    "\n",
    "    for variable, x_label in zip(variable_list, x_label_list):\n",
    "        range_bins = range_bins_list[variable]\n",
    "\n",
    "        # correct / wrong permutations\n",
    "        n_D_perm, n_D_perm_bins, n_D_perm_patches = plt.hist([correct_permutations_ttbar[\n",
    "                                                              :, complete_feature_dictionary[variable]],\n",
    "                                                              wrong_permutations_ttbar[\n",
    "                                                              :, complete_feature_dictionary[variable]]],\n",
    "                                                             weights=[correct_permutations_ttbar[:, -1] *\n",
    "                                                                      MC_lum,\n",
    "                                                                      wrong_permutations_ttbar[:, complete_feature_dictionary[\n",
    "                                                                          'combinedWeight_and_trigger_efficiency_correction']] \n",
    "                                                                      * MC_lum],\n",
    "                                                             bins=BIN_NUMBER,\n",
    "                                                             stacked=True,\n",
    "                                                             alpha=1,\n",
    "                                                             edgecolor='k',\n",
    "                                                             range=range_bins,\n",
    "                                                             color=['#CC0000', '#FF6666'])\n",
    "\n",
    "        x_label_string = x_label\n",
    "        y_label = np.around(((range_bins[1] - range_bins[0]) / BIN_NUMBER), decimals=2)\n",
    "        if '[GeV]' in x_label:\n",
    "            y_label_string = r'Events / ' + str(y_label) + ' GeV'\n",
    "        else:\n",
    "            y_label_string = r'Events / ' + str(y_label)\n",
    "\n",
    "        plt.title(r'simulation $\\cdot$ private work'\n",
    "                  '                  '\n",
    "                  r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "        plt.xlabel(x_label_string, fontsize=13)\n",
    "        plt.ylabel(y_label_string, fontsize=13)\n",
    "        plt.legend({label_ttbar_correct: '#CC0000', label_ttbar_wrong: '#FF6666'}, fontsize=13)\n",
    "        plt.savefig('Permutation_my_cut_' + str(variable).replace('.', '_') + str(threshold_value).replace('.', '_'))\n",
    "        plt.close()\n",
    "\n",
    "        # 1/2 btag + correct/wrong\n",
    "        n_D_, n_D_bins, n_D_patches = plt.hist([two_btag_correct_permutation_ttbar[\n",
    "                                                :, complete_feature_dictionary[variable]],\n",
    "                                                two_btag_wrong_permutation_ttbar[\n",
    "                                                :, complete_feature_dictionary[variable]],\n",
    "                                                one_btag_correct_permutation_ttbar[\n",
    "                                                :, complete_feature_dictionary[variable]],\n",
    "                                                one_btag_wrong_permutation_ttbar[\n",
    "                                                :, complete_feature_dictionary[variable]]],\n",
    "                                               weights=[two_btag_correct_permutation_ttbar[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                               'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                                                        * MC_lum,\n",
    "                                                        two_btag_wrong_permutation_ttbar[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                               'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                                                        * MC_lum,\n",
    "                                                        one_btag_correct_permutation_ttbar[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                               'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                                                        * MC_lum,\n",
    "                                                        one_btag_wrong_permutation_ttbar[\n",
    "                                                        :, complete_feature_dictionary[\n",
    "                                                               'combinedWeight_and_trigger_efficiency_correction']]\n",
    "                                                        * MC_lum,],\n",
    "                                               bins=BIN_NUMBER,\n",
    "                                               stacked=True,\n",
    "                                               alpha=1,\n",
    "                                               edgecolor='k',\n",
    "                                               range=range_bins,\n",
    "                                               color=['green', 'lime', 'blue', 'royalblue'])\n",
    "\n",
    "        x_label_string = x_label\n",
    "        y_label = np.around(((range_bins[1] - range_bins[0]) / BIN_NUMBER), decimals=2)\n",
    "        if '[GeV]' in x_label:\n",
    "            y_label_string = r'Events / ' + str(y_label) + ' GeV'\n",
    "        else:\n",
    "            y_label_string = r'Events / ' + str(y_label)\n",
    "\n",
    "        plt.title(r'simulation $\\cdot$ private work'\n",
    "                  '                  '\n",
    "                  r'35.9 $fb^{-1}$ (13 TeV)', fontsize=13)\n",
    "        plt.xlabel(x_label_string, fontsize=13)\n",
    "        plt.ylabel(y_label_string, fontsize=13)\n",
    "        if variable == 'top.fitProb':\n",
    "            plt.yscale('log')\n",
    "        plt.legend({'2 b-tag ' + label_ttbar_correct: 'green',\n",
    "                    '2 b-tag ' + label_ttbar_wrong: 'lime',\n",
    "                    '1 b-tag ' + label_ttbar_correct: 'blue',\n",
    "                    '1 b-tag ' + label_ttbar_wrong: 'royalblue'\n",
    "                    }, fontsize=13)\n",
    "        plt.savefig('1vs2btag_my_cut_' + str(variable).replace('.', '_') + str(threshold_value).replace('.', '_'))\n",
    "        plt.close()\n",
    "\n",
    "    os.chdir(closure_tests_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow_keras)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
